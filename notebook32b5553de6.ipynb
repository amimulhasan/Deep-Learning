{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9825091,"sourceType":"datasetVersion","datasetId":6025001}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:20.193603Z","iopub.execute_input":"2026-01-01T07:27:20.193954Z","iopub.status.idle":"2026-01-01T07:27:22.613375Z","shell.execute_reply.started":"2026-01-01T07:27:20.193925Z","shell.execute_reply":"2026-01-01T07:27:22.612748Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install torch torchvision albumentations==1.1.0 opencv-python numpy scikit-learn tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:22.614566Z","iopub.execute_input":"2026-01-01T07:27:22.614903Z","iopub.status.idle":"2026-01-01T07:27:27.467429Z","shell.execute_reply.started":"2026-01-01T07:27:22.614881Z","shell.execute_reply":"2026-01-01T07:27:27.466710Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\nCollecting albumentations==1.1.0\n  Downloading albumentations-1.1.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from albumentations==1.1.0) (1.15.3)\nRequirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.1.0) (0.25.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations==1.1.0) (6.0.3)\nCollecting qudida>=0.0.4 (from albumentations==1.1.0)\n  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.1.0) (4.12.0.88)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (2025.10.16)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (0.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nDownloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\nInstalling collected packages: qudida, albumentations\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 2.0.8\n    Uninstalling albumentations-2.0.8:\n      Successfully uninstalled albumentations-2.0.8\nSuccessfully installed albumentations-1.1.0 qudida-0.0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom sklearn.model_selection import train_test_split\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:27.468563Z","iopub.execute_input":"2026-01-01T07:27:27.468778Z","iopub.status.idle":"2026-01-01T07:27:35.576770Z","shell.execute_reply.started":"2026-01-01T07:27:27.468752Z","shell.execute_reply":"2026-01-01T07:27:35.576207Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"IMAGE_DIR = \"/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/image\"\nMASK_DIR  = \"/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/mask\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.578231Z","iopub.execute_input":"2026-01-01T07:27:35.578566Z","iopub.status.idle":"2026-01-01T07:27:35.581994Z","shell.execute_reply.started":"2026-01-01T07:27:35.578544Z","shell.execute_reply":"2026-01-01T07:27:35.581334Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_transform = A.Compose([\n    A.Resize(256,256),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.2),\n    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n    ToTensorV2()\n])\n\nval_test_transform = A.Compose([\n    A.Resize(256,256),\n    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n    ToTensorV2()\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.583144Z","iopub.execute_input":"2026-01-01T07:27:35.583445Z","iopub.status.idle":"2026-01-01T07:27:35.601846Z","shell.execute_reply.started":"2026-01-01T07:27:35.583418Z","shell.execute_reply":"2026-01-01T07:27:35.601261Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class XRayLungDataset(Dataset):\n    def __init__(self, images, masks, transform=None):\n        self.images = images\n        self.masks  = masks\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img  = cv2.imread(self.images[idx], cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(self.masks[idx],  cv2.IMREAD_GRAYSCALE)\n\n        # Convert to 3-channel for ResNet\n        img = np.stack([img,img,img], axis=-1)\n\n        # Normalize and convert to float32\n        img  = (img / 255.0).astype(np.float32)\n        mask = (mask / 255.0).astype(np.float32)\n\n        if self.transform:\n            augmented = self.transform(image=img, mask=mask)\n            img  = augmented[\"image\"]\n            mask = augmented[\"mask\"]\n\n        return img, mask.unsqueeze(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.602686Z","iopub.execute_input":"2026-01-01T07:27:35.602941Z","iopub.status.idle":"2026-01-01T07:27:35.617336Z","shell.execute_reply.started":"2026-01-01T07:27:35.602912Z","shell.execute_reply":"2026-01-01T07:27:35.616716Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# class XRayLungDataset(Dataset):\n#     def __init__(self, images, masks, transform=None):\n#         self.images = images\n#         self.masks = masks\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.images)\n\n#     def __getitem__(self, idx):\n#         img = cv2.imread(self.images[idx], cv2.IMREAD_GRAYSCALE)\n#         mask= cv2.imread(self.masks[idx], cv2.IMREAD_GRAYSCALE)\n\n#         # convert to 3 channels for pretrained ResNet\n#         img = np.stack([img, img, img], axis=-1)\n\n#         img = img / 255.0\n#         mask= mask / 255.0\n\n#         if self.transform:\n#             augmented = self.transform(image=img, mask=mask)\n#             img  = augmented[\"image\"]\n#             mask = augmented[\"mask\"]\n\n#         return img, mask.unsqueeze(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.618272Z","iopub.execute_input":"2026-01-01T07:27:35.618643Z","iopub.status.idle":"2026-01-01T07:27:35.635748Z","shell.execute_reply.started":"2026-01-01T07:27:35.618612Z","shell.execute_reply":"2026-01-01T07:27:35.635138Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"all_images = sorted([os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)])\nall_masks  = sorted([os.path.join(MASK_DIR, f) for f in os.listdir(MASK_DIR)])\n\nX_train, X_temp, y_train, y_temp = train_test_split(all_images, all_masks, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test   = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.636526Z","iopub.execute_input":"2026-01-01T07:27:35.636751Z","iopub.status.idle":"2026-01-01T07:27:35.664018Z","shell.execute_reply.started":"2026-01-01T07:27:35.636719Z","shell.execute_reply":"2026-01-01T07:27:35.663322Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_loader = DataLoader(XRayLungDataset(X_train, y_train, train_transform), batch_size=8, shuffle=True)\nval_loader   = DataLoader(XRayLungDataset(X_val,   y_val,   val_test_transform), batch_size=8, shuffle=False)\ntest_loader  = DataLoader(XRayLungDataset(X_test,  y_test,  val_test_transform), batch_size=1, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.664947Z","iopub.execute_input":"2026-01-01T07:27:35.665259Z","iopub.status.idle":"2026-01-01T07:27:35.669503Z","shell.execute_reply.started":"2026-01-01T07:27:35.665216Z","shell.execute_reply":"2026-01-01T07:27:35.668806Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class AttentionBlock(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__()\n        self.W_g = nn.Conv2d(F_g, F_int, 1)\n        self.W_x = nn.Conv2d(F_l, F_int, 1)\n        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.Sigmoid())\n        self.relu= nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        psi = self.relu(self.W_g(g) + self.W_x(x))\n        psi = self.psi(psi)\n        return x * psi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.671612Z","iopub.execute_input":"2026-01-01T07:27:35.671886Z","iopub.status.idle":"2026-01-01T07:27:35.685559Z","shell.execute_reply.started":"2026-01-01T07:27:35.671864Z","shell.execute_reply":"2026-01-01T07:27:35.684824Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class ResNet50_Attention_UNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n\n        self.e1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n        self.e2 = resnet.layer1\n        self.e3 = resnet.layer2\n        self.e4 = resnet.layer3\n        self.e5 = resnet.layer4\n\n        self.up4 = nn.ConvTranspose2d(2048, 1024, 2, 2)\n        self.att4= AttentionBlock(1024,1024,512)\n        self.d4  = nn.Conv2d(2048,1024,3,padding=1)\n        self.up3 = nn.ConvTranspose2d(1024, 512, 2,2)\n        self.att3= AttentionBlock(512,512,256)\n        self.d3  = nn.Conv2d(1024,512,3,padding=1)\n        self.up2 = nn.ConvTranspose2d(512,256,2,2)\n        self.att2= AttentionBlock(256,256,128)\n        self.d2  = nn.Conv2d(512,256,3,padding=1)\n        self.up1 = nn.ConvTranspose2d(256, 64,2,2)\n        self.d1  = nn.Conv2d(128,64,3,padding=1)\n        self.out = nn.Conv2d(64,1,1)\n\n    def forward(self,x):\n        e1 = self.e1(x)\n        e2 = self.e2(e1)\n        e3 = self.e3(e2)\n        e4 = self.e4(e3)\n        e5 = self.e5(e4)\n\n        d4 = self.up4(e5)\n        e4 = self.att4(d4, e4)\n        d4 = self.d4(torch.cat([d4,e4],1))\n\n        d3 = self.up3(d4)\n        e3 = self.att3(d3, e3)\n        d3 = self.d3(torch.cat([d3,e3],1))\n\n        d2 = self.up2(d3)\n        e2 = self.att2(d2, e2)\n        d2 = self.d2(torch.cat([d2,e2],1))\n\n        d1 = self.up1(d2)\n        if d1.shape[2:] != e1.shape[2:]:\n            e1 = nn.functional.interpolate(e1, size=d1.shape[2:], mode=\"bilinear\", align_corners=False)\n        d1 = self.d1(torch.cat([d1,e1],1))\n\n        return torch.sigmoid(self.out(d1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.686521Z","iopub.execute_input":"2026-01-01T07:27:35.686786Z","iopub.status.idle":"2026-01-01T07:27:35.700905Z","shell.execute_reply.started":"2026-01-01T07:27:35.686765Z","shell.execute_reply":"2026-01-01T07:27:35.700326Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def dice_score(pred, target):\n    smooth=1e-6\n    pred=pred.view(-1)\n    target=target.view(-1)\n    return (2*(pred*target).sum()+smooth)/((pred+target).sum()+smooth)\n\ndef iou_score(pred,target):\n    smooth=1e-6\n    pred=pred.view(-1); target=target.view(-1)\n    inter=(pred*target).sum()\n    union=pred.sum()+target.sum()-inter\n    return (inter+smooth)/(union+smooth)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.701750Z","iopub.execute_input":"2026-01-01T07:27:35.701951Z","iopub.status.idle":"2026-01-01T07:27:35.717630Z","shell.execute_reply.started":"2026-01-01T07:27:35.701931Z","shell.execute_reply":"2026-01-01T07:27:35.716915Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model = ResNet50_Attention_UNet().to(device)\n# criterion = nn.BCELoss()\n# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# EPOCHS = 20\n\n# for epoch in range(EPOCHS):\n#     model.train()\n#     train_loss = 0\n\n#     for imgs, masks in tqdm(train_loader):\n#         # 1️⃣ Move tensors to device (GPU or CPU)\n#         imgs, masks = imgs.to(device), masks.to(device)\n\n#         # 2️⃣ Convert masks to float32 to match BCELoss\n#         masks = masks.float()  # <-- THIS IS THE FIX\n\n#         # 3️⃣ Forward pass\n#         preds = model(imgs)\n\n#         # 4️⃣ Compute loss\n#         loss = criterion(preds, masks)\n\n#         # 5️⃣ Backpropagation\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         train_loss += loss.item()\n\n#     print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.718543Z","iopub.execute_input":"2026-01-01T07:27:35.718896Z","iopub.status.idle":"2026-01-01T07:27:35.730157Z","shell.execute_reply.started":"2026-01-01T07:27:35.718869Z","shell.execute_reply":"2026-01-01T07:27:35.729534Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = ResNet50_Attention_UNet().to(device)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nEPOCHS = 10\n\ndef dice_score(pred, target):\n    smooth = 1e-6\n    pred = pred.view(-1)\n    target = target.view(-1)\n    return (2*(pred*target).sum()+smooth)/((pred+target).sum()+smooth)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    train_dice = 0\n\n    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n        imgs, masks = imgs.to(device), masks.to(device)\n        masks = masks.float()  # ensure BCELoss compatibility\n\n        preds = model(imgs)\n        loss = criterion(preds, masks)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        train_dice += dice_score(preds, masks).item()\n\n    train_loss /= len(train_loader)\n    train_dice /= len(train_loader)\n\n    # -------------- Validation -----------------\n    model.eval()\n    val_loss = 0\n    val_dice = 0\n\n    with torch.no_grad():\n        for imgs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n            imgs, masks = imgs.to(device), masks.to(device)\n            masks = masks.float()\n\n            preds = model(imgs)\n            loss = criterion(preds, masks)\n\n            val_loss += loss.item()\n            val_dice += dice_score(preds, masks).item()\n\n    val_loss /= len(val_loader)\n    val_dice /= len(val_loader)\n\n    print(f\"\\nEpoch [{epoch+1}/{EPOCHS}] \"\n          f\"Train Loss: {train_loss:.4f} | Train Dice: {train_dice:.4f} \"\n          f\"| Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T07:27:35.731038Z","iopub.execute_input":"2026-01-01T07:27:35.731268Z","iopub.status.idle":"2026-01-01T08:05:25.760500Z","shell.execute_reply.started":"2026-01-01T07:27:35.731247Z","shell.execute_reply":"2026-01-01T08:05:25.759883Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 206MB/s] \nEpoch 1/10 - Training: 100%|██████████| 62/62 [03:23<00:00,  3.29s/it]\nEpoch 1/10 - Validation: 100%|██████████| 14/14 [00:37<00:00,  2.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [1/10] Train Loss: 0.2046 | Train Dice: 0.7544 | Val Loss: 1.1064 | Val Dice: 0.0317\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.09s/it]\nEpoch 2/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [2/10] Train Loss: 0.0736 | Train Dice: 0.9169 | Val Loss: 0.8693 | Val Dice: 0.0972\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.10s/it]\nEpoch 3/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [3/10] Train Loss: 0.0606 | Train Dice: 0.9319 | Val Loss: 1.0401 | Val Dice: 0.2127\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.09s/it]\nEpoch 4/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [4/10] Train Loss: 0.0533 | Train Dice: 0.9395 | Val Loss: 1.7582 | Val Dice: 0.0027\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.08s/it]\nEpoch 5/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [5/10] Train Loss: 0.0505 | Train Dice: 0.9425 | Val Loss: 0.6023 | Val Dice: 0.3939\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 - Training: 100%|██████████| 62/62 [03:10<00:00,  3.08s/it]\nEpoch 6/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [6/10] Train Loss: 0.0501 | Train Dice: 0.9426 | Val Loss: 1.4725 | Val Dice: 0.0097\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.09s/it]\nEpoch 7/10 - Validation: 100%|██████████| 14/14 [00:34<00:00,  2.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [7/10] Train Loss: 0.0512 | Train Dice: 0.9430 | Val Loss: 1.7631 | Val Dice: 0.0027\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.09s/it]\nEpoch 8/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [8/10] Train Loss: 0.0501 | Train Dice: 0.9429 | Val Loss: 0.3767 | Val Dice: 0.4823\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.09s/it]\nEpoch 9/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [9/10] Train Loss: 0.0448 | Train Dice: 0.9491 | Val Loss: 0.1651 | Val Dice: 0.8365\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 - Training: 100%|██████████| 62/62 [03:11<00:00,  3.08s/it]\nEpoch 10/10 - Validation: 100%|██████████| 14/14 [00:33<00:00,  2.41s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpoch [10/10] Train Loss: 0.0433 | Train Dice: 0.9495 | Val Loss: 0.5147 | Val Dice: 0.4232\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model=ResNet50_Attention_UNet().to(device)\n# criterion=nn.BCELoss()\n# optimizer=optim.Adam(model.parameters(), lr=1e-4)\n\n# for epoch in range(20):\n#     model.train()\n#     loss_sum=0\n#     for imgs, masks in tqdm(train_loader):\n#         imgs, masks = imgs.to(device), masks.to(device)\n#         preds = model(imgs)\n#         loss  = criterion(preds, masks)\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#         loss_sum += loss.item()\n\n#     print(f\"Epoch {epoch+1} - Train Loss: {loss_sum/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:05:25.761456Z","iopub.execute_input":"2026-01-01T08:05:25.761694Z","iopub.status.idle":"2026-01-01T08:05:25.765148Z","shell.execute_reply.started":"2026-01-01T08:05:25.761672Z","shell.execute_reply":"2026-01-01T08:05:25.764503Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model.eval()\ndice, iou = 0,0\nwith torch.no_grad():\n    for imgs, masks in test_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model(imgs)\n        dice += dice_score(preds, masks).item()\n        iou  += iou_score(preds, masks).item()\n\nprint(f\"Test Dice: {dice/len(test_loader):.4f}\")\nprint(f\"Test IoU : {iou/len(test_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:05:25.766085Z","iopub.execute_input":"2026-01-01T08:05:25.766353Z","iopub.status.idle":"2026-01-01T08:06:02.800503Z","shell.execute_reply.started":"2026-01-01T08:05:25.766327Z","shell.execute_reply":"2026-01-01T08:06:02.799574Z"}},"outputs":[{"name":"stdout","text":"Test Dice: 0.4361\nTest IoU : 0.2823\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef compute_metrics(pred, target, threshold=0.5):\n    \"\"\"\n    pred : torch.Tensor (B,1,H,W) predicted mask probabilities\n    target : torch.Tensor (B,1,H,W) ground truth\n    threshold : probability threshold to convert to binary mask\n    returns : dict of metrics (dice, iou, precision, recall, f1, specificity, accuracy)\n    \"\"\"\n    pred_bin = (pred > threshold).float()\n    target_bin = target.float()\n    \n    TP = (pred_bin * target_bin).sum()\n    FP = ((pred_bin == 1) & (target_bin == 0)).sum()\n    FN = ((pred_bin == 0) & (target_bin == 1)).sum()\n    TN = ((pred_bin == 0) & (target_bin == 0)).sum()\n    \n    dice = (2*TP) / (2*TP + FP + FN + 1e-6)\n    iou  = TP / (TP + FP + FN + 1e-6)\n    precision = TP / (TP + FP + 1e-6)\n    recall    = TP / (TP + FN + 1e-6)\n    f1        = 2*precision*recall / (precision + recall + 1e-6)\n    specificity = TN / (TN + FP + 1e-6)\n    accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-6)\n    \n    return {\n        \"dice\": dice.item(),\n        \"iou\": iou.item(),\n        \"precision\": precision.item(),\n        \"recall\": recall.item(),\n        \"f1\": f1.item(),\n        \"specificity\": specificity.item(),\n        \"accuracy\": accuracy.item()\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:06:02.801394Z","iopub.execute_input":"2026-01-01T08:06:02.801668Z","iopub.status.idle":"2026-01-01T08:06:02.808157Z","shell.execute_reply.started":"2026-01-01T08:06:02.801646Z","shell.execute_reply":"2026-01-01T08:06:02.807575Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nEPOCHS = 20\n\ntrain_history = defaultdict(list)\nval_history = defaultdict(list)\n\nfor epoch in range(EPOCHS):\n    # ---------------- TRAIN -------------------\n    model.train()\n    train_loss_sum = 0\n    train_metrics_sum = defaultdict(float)\n    \n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        masks = masks.float()\n        \n        preds = model(imgs)\n        loss = criterion(preds, masks)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss_sum += loss.item()\n        \n        metrics = compute_metrics(preds, masks)\n        for k in metrics:\n            train_metrics_sum[k] += metrics[k]\n    \n    # Average metrics for the epoch\n    n_train = len(train_loader)\n    train_history[\"loss\"].append(train_loss_sum / n_train)\n    for k in train_metrics_sum:\n        train_history[k].append(train_metrics_sum[k]/n_train)\n    \n    # ---------------- VALIDATION -------------------\n    model.eval()\n    val_loss_sum = 0\n    val_metrics_sum = defaultdict(float)\n    \n    with torch.no_grad():\n        for imgs, masks in val_loader:\n            imgs, masks = imgs.to(device), masks.to(device)\n            masks = masks.float()\n            \n            preds = model(imgs)\n            loss = criterion(preds, masks)\n            val_loss_sum += loss.item()\n            \n            metrics = compute_metrics(preds, masks)\n            for k in metrics:\n                val_metrics_sum[k] += metrics[k]\n    \n    n_val = len(val_loader)\n    val_history[\"loss\"].append(val_loss_sum / n_val)\n    for k in val_metrics_sum:\n        val_history[k].append(val_metrics_sum[k]/n_val)\n    \n    # --------- Print Epoch Summary -------------\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_history['loss'][-1]:.4f} | Dice: {train_history['dice'][-1]:.4f} | IoU: {train_history['iou'][-1]:.4f}\")\n    print(f\"Val   Loss: {val_history['loss'][-1]:.4f} | Dice: {val_history['dice'][-1]:.4f} | IoU: {val_history['iou'][-1]:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T08:06:02.808933Z","iopub.execute_input":"2026-01-01T08:06:02.809207Z","execution_failed":"2026-01-01T08:25:38.963Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 0.0428 | Dice: 0.9660 | IoU: 0.9343\nVal   Loss: 0.0987 | Dice: 0.9292 | IoU: 0.8680\n\nEpoch 2/20\nTrain Loss: 0.0421 | Dice: 0.9668 | IoU: 0.9359\nVal   Loss: 0.3581 | Dice: 0.5825 | IoU: 0.4137\n\nEpoch 3/20\nTrain Loss: 0.0390 | Dice: 0.9685 | IoU: 0.9391\nVal   Loss: 0.6071 | Dice: 0.3750 | IoU: 0.2431\n\nEpoch 4/20\nTrain Loss: 0.0385 | Dice: 0.9691 | IoU: 0.9400\nVal   Loss: 0.5784 | Dice: 0.2613 | IoU: 0.1542\n\nEpoch 5/20\nTrain Loss: 0.0362 | Dice: 0.9704 | IoU: 0.9426\nVal   Loss: 0.1210 | Dice: 0.9102 | IoU: 0.8357\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(train_history['loss'], label=\"Train Loss\")\nplt.plot(val_history['loss'], label=\"Val Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-01T08:25:38.963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmodel.eval()\nwith torch.no_grad():\n    for imgs, masks in test_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        masks = masks.float()\n        \n        preds = model(imgs)\n        pred_mask = (preds > 0.5).float()\n        \n        img_np = imgs[0].cpu().permute(1,2,0).numpy()\n        mask_np = masks[0,0].cpu().numpy()\n        pred_np = pred_mask[0,0].cpu().numpy()\n        \n        plt.figure(figsize=(12,4))\n        plt.subplot(1,3,1); plt.imshow(img_np, cmap='gray'); plt.title(\"X-ray Image\")\n        plt.subplot(1,3,2); plt.imshow(mask_np, cmap='gray'); plt.title(\"Ground Truth Mask\")\n        plt.subplot(1,3,3); plt.imshow(pred_np, cmap='gray'); plt.title(\"Predicted Mask\")\n        plt.show()\n        break  # show one batch\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-01T08:25:38.963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nk = 5\nkf = KFold(n_splits=k, shuffle=True, random_state=42)\n\nall_images = sorted([os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)])\nall_masks  = sorted([os.path.join(MASK_DIR, f) for f in os.listdir(MASK_DIR)])\n\nfold = 1\nfold_metrics = []\n\nfor train_idx, val_idx in kf.split(all_images):\n    print(f\"Fold {fold}/{k}\")\n    X_train = [all_images[i] for i in train_idx]\n    y_train = [all_masks[i] for i in train_idx]\n    X_val   = [all_images[i] for i in val_idx]\n    y_val   = [all_masks[i] for i in val_idx]\n\n    # Create loaders, model, optimizer (same as above)\n    # Train model (same training loop)\n    # Compute metrics on validation set\n    # Save mean Dice / IoU for this fold\n    fold_metrics.append(val_dice)  # or dict with all metrics\n    fold += 1\n\nmean_dice = np.mean(fold_metrics)\nstd_dice  = np.std(fold_metrics)\nprint(f\"5-Fold CV Dice Mean ± Std: {mean_dice:.4f} ± {std_dice:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-01T08:25:38.963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import wilcoxon\n\n# Suppose dice_model1 and dice_model2 are lists of fold dice scores\nstat, p = wilcoxon(dice_model1, dice_model2)\nprint(f\"Wilcoxon test p-value: {p:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-01T08:25:38.963Z"}},"outputs":[],"execution_count":null}]}