{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T17:54:31.678468Z","iopub.execute_input":"2025-12-19T17:54:31.679533Z","iopub.status.idle":"2025-12-19T17:54:35.269097Z","shell.execute_reply.started":"2025-12-19T17:54:31.679489Z","shell.execute_reply":"2025-12-19T17:54:35.268204Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric \\\n-f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:08:26.494235Z","iopub.execute_input":"2025-12-19T18:08:26.494609Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch_geometric\n\nprint(\"Torch version:\", torch.__version__)\nprint(\"Torch Geometric version:\", torch_geometric.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:06:56.322485Z","iopub.execute_input":"2025-12-19T18:06:56.323109Z","iopub.status.idle":"2025-12-19T18:06:59.532120Z","shell.execute_reply.started":"2025-12-19T18:06:56.323079Z","shell.execute_reply":"2025-12-19T18:06:59.531223Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/128358742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch Geometric version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torch_geometric'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Install PyTorch Geometric dependencies\n!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T17:54:35.270760Z","iopub.execute_input":"2025-12-19T17:54:35.271206Z","execution_failed":"2025-12-19T18:06:22.147Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport timm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch_geometric.nn import GATConv\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_DIR = \"/kaggle/input/brain-tumor-mri-dataset/Training\"\nIMG_SIZE = 224\nNUM_CLASSES = 4\nBATCH_SIZE = 16\nEPOCHS = 20\nLR = 1e-4\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_dataset_numpy(dataset_dir, img_size):\n    images = []\n    labels = []\n\n    for label in os.listdir(dataset_dir):\n        class_path = os.path.join(dataset_dir, label)\n        if not os.path.isdir(class_path):\n            continue\n\n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (img_size, img_size))\n            images.append(img)\n            labels.append(label)\n\n    images = np.array(images, dtype=np.float32) / 255.0\n    labels = np.array(labels)\n\n    return images, labels\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y = load_dataset_numpy(DATASET_DIR, IMG_SIZE)\nprint(\"Images shape:\", X.shape)\nprint(\"Labels shape:\", y.shape)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NumpyDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img = torch.tensor(self.images[idx]).permute(2, 0, 1)\n        label = torch.tensor(self.labels[idx])\n        return img, label\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = NumpyDataset(X_train, y_train)\ntest_dataset  = NumpyDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Swin_LSTM_GAT(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        # Swin Transformer\n        self.swin = timm.create_model(\n            \"swin_tiny_patch4_window7_224\",\n            pretrained=True,\n            num_classes=0\n        )\n\n        # LSTM\n        self.lstm = nn.LSTM(\n            input_size=768,\n            hidden_size=512,\n            batch_first=True\n        )\n\n        # Graph Attention Network\n        self.gat1 = GATConv(512, 256, heads=4)\n        self.gat2 = GATConv(256 * 4, 128, heads=1)\n\n        # Classifier\n        self.fc = nn.Sequential(\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        B = x.size(0)\n\n        swin_feat = self.swin(x)           # (B, 768)\n        swin_feat = swin_feat.unsqueeze(1)\n\n        lstm_out, _ = self.lstm(swin_feat) # (B, 1, 512)\n        lstm_feat = lstm_out.squeeze(1)\n\n        # Fully connected graph\n        edge_index = torch.combinations(\n            torch.arange(B), r=2\n        ).t().to(x.device)\n\n        gat_out = self.gat1(lstm_feat, edge_index)\n        gat_out = torch.relu(gat_out)\n        gat_out = self.gat2(gat_out, edge_index)\n\n        return self.fc(gat_out)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Swin_LSTM_GAT(NUM_CLASSES).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train():\n    model.train()\n    for epoch in range(EPOCHS):\n        total_loss = 0\n        for imgs, labels in train_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        imgs = imgs.to(device)\n        outputs = model(imgs)\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        y_pred.extend(preds)\n        y_true.extend(labels.numpy())\n\nprint(\"Accuracy:\", accuracy_score(y_true, y_pred))\nprint(classification_report(y_true, y_pred, target_names=encoder.classes_))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-19T18:06:22.148Z"}},"outputs":[],"execution_count":null}]}