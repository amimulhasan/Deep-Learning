{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:49.697324Z","iopub.execute_input":"2025-12-12T13:16:49.697662Z","iopub.status.idle":"2025-12-12T13:16:54.222632Z","shell.execute_reply.started":"2025-12-12T13:16:49.697640Z","shell.execute_reply":"2025-12-12T13:16:54.221962Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nfrom glob import glob\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\ntry:\n    from skimage.filters import threshold_otsu\nexcept:\n    threshold_otsu = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:54.223836Z","iopub.execute_input":"2025-12-12T13:16:54.224239Z","iopub.status.idle":"2025-12-12T13:16:58.055579Z","shell.execute_reply.started":"2025-12-12T13:16:54.224214Z","shell.execute_reply":"2025-12-12T13:16:58.054969Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"CONFIG = {\n    'DATA_DIR': '/kaggle/input/x-ray-images/images',  # change this to your dataset folder\n    'IMG_SIZE': 224,\n    'NUM_CLASSES': 2,  # background + your object\n    'BATCH_SIZE': 8,\n    'EPOCHS': 30,\n    'LR': 1e-4,\n    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'CHECKPOINT_DIR': './checkpoints'\n}\n\nos.makedirs(CONFIG['CHECKPOINT_DIR'], exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:58.056395Z","iopub.execute_input":"2025-12-12T13:16:58.056834Z","iopub.status.idle":"2025-12-12T13:16:58.116236Z","shell.execute_reply.started":"2025-12-12T13:16:58.056808Z","shell.execute_reply":"2025-12-12T13:16:58.115558Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def list_images(folder: str, ext: str = '.png'):\n    pattern = str(Path(folder) / f\"**/*{ext}\")\n    return sorted([p for p in glob(pattern, recursive=True)])\n\ndef read_image(path: str, size: int):\n    img = cv2.imread(path)\n    if img is None:\n        raise FileNotFoundError(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (size, size))\n    return img\n\ndef pseudo_mask_otsu(img: np.ndarray):\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    if threshold_otsu:\n        t = threshold_otsu(gray)\n        mask = (gray > t).astype(np.uint8)\n    else:\n        _, mask = cv2.threshold(gray, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((3,3), np.uint8))\n    return mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:58.117931Z","iopub.execute_input":"2025-12-12T13:16:58.118158Z","iopub.status.idle":"2025-12-12T13:16:58.133816Z","shell.execute_reply.started":"2025-12-12T13:16:58.118141Z","shell.execute_reply":"2025-12-12T13:16:58.132995Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MedicalSegDataset(Dataset):\n    def __init__(self, image_paths, img_size=224):\n        self.image_paths = image_paths\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = read_image(img_path, self.img_size)\n        mask = pseudo_mask_otsu(img)\n        mask = cv2.resize(mask.astype(np.uint8), (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n\n        img = torch.from_numpy(img.transpose(2,0,1)).float() / 255.0\n        mask = torch.from_numpy(mask).long()\n        return img, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:58.134702Z","iopub.execute_input":"2025-12-12T13:16:58.135070Z","iopub.status.idle":"2025-12-12T13:16:58.151444Z","shell.execute_reply.started":"2025-12-12T13:16:58.135044Z","shell.execute_reply":"2025-12-12T13:16:58.150650Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        return self.conv(x)\n\nclass UpConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ConvBlock(in_ch, out_ch)\n        )\n    def forward(self, x):\n        return self.up(x)\n\nclass HybridUNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.enc1 = ConvBlock(3, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = ConvBlock(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = ConvBlock(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = ConvBlock(256, 512)\n        self.pool4 = nn.MaxPool2d(2)\n\n        # Transformer Bottleneck\n        self.trans = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=512, nhead=8),\n            num_layers=1\n        )\n\n        self.up4 = UpConv(512, 256)\n        self.up3 = UpConv(256+256, 128)\n        self.up2 = UpConv(128+128, 64)\n        self.up1 = UpConv(64+64, 32)\n        self.final = nn.Conv2d(32, num_classes, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        e4 = self.enc4(self.pool3(e3))\n\n        # Transformer bottleneck\n        b = e4.flatten(2).permute(2,0,1)\n        b = self.trans(b)\n        b = b.permute(1,2,0).view(e4.shape)\n\n        d4 = self.up4(b)\n        d3 = self.up3(torch.cat([d4, e3], dim=1))\n        d2 = self.up2(torch.cat([d3, e2], dim=1))\n        d1 = self.up1(torch.cat([d2, e1], dim=1))\n        out = self.final(d1)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:58.152269Z","iopub.execute_input":"2025-12-12T13:16:58.152618Z","iopub.status.idle":"2025-12-12T13:16:58.164236Z","shell.execute_reply.started":"2025-12-12T13:16:58.152600Z","shell.execute_reply":"2025-12-12T13:16:58.163526Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def dice_loss(pred, target, eps=1e-6):\n    pred = F.softmax(pred, dim=1)\n    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0,3,1,2).float()\n    inter = (pred * target_onehot).sum(dim=(2,3))\n    denom = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3))\n    dice = (2*inter + eps) / (denom + eps)\n    return 1 - dice.mean()\n\ndef iou_score(pred, target, num_classes=2):\n    pred_labels = pred.argmax(dim=1)\n    ious = []\n    for cls in range(num_classes):\n        pred_c = (pred_labels == cls)\n        target_c = (target == cls)\n        inter = (pred_c & target_c).sum().item()\n        union = (pred_c | target_c).sum().item()\n        ious.append(inter / union if union > 0 else 1.0)\n    return np.mean(ious)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:58.165221Z","iopub.execute_input":"2025-12-12T13:16:58.165473Z","iopub.status.idle":"2025-12-12T13:16:58.180116Z","shell.execute_reply.started":"2025-12-12T13:16:58.165450Z","shell.execute_reply":"2025-12-12T13:16:58.179534Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"images = list_images(CONFIG['DATA_DIR'], ext='.png')\nn = len(images)\nsplit = int(0.8 * n)\ntrain_imgs = images[:split]\nval_imgs = images[split:]\n\ntrain_ds = MedicalSegDataset(train_imgs, img_size=CONFIG['IMG_SIZE'])\nval_ds = MedicalSegDataset(val_imgs, img_size=CONFIG['IMG_SIZE'])\n\ntrain_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:58.180866Z","iopub.execute_input":"2025-12-12T13:16:58.181118Z","iopub.status.idle":"2025-12-12T13:17:00.761215Z","shell.execute_reply.started":"2025-12-12T13:16:58.181093Z","shell.execute_reply":"2025-12-12T13:17:00.760639Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = CONFIG['DEVICE']\nmodel = HybridUNet(num_classes=CONFIG['NUM_CLASSES']).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['LR'])\n\nbest_iou = 0.0\n\nfor epoch in range(1, CONFIG['EPOCHS']+1):\n    model.train()\n    total_loss = 0.0\n    for imgs, masks in train_loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        optimizer.zero_grad()\n        logits = model(imgs)\n        loss = F.cross_entropy(logits, masks) + dice_loss(logits, masks)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * imgs.size(0)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    val_iou = 0.0\n    with torch.no_grad():\n        for imgs, masks in val_loader:\n            imgs, masks = imgs.to(device), masks.to(device)\n            logits = model(imgs)\n            val_loss += (F.cross_entropy(logits, masks) + dice_loss(logits, masks)).item() * imgs.size(0)\n            val_iou += iou_score(logits, masks, CONFIG['NUM_CLASSES']) * imgs.size(0)\n    val_loss /= len(val_loader.dataset)\n    val_iou /= len(val_loader.dataset)\n\n    print(f\"Epoch {epoch}/{CONFIG['EPOCHS']}  Train Loss: {total_loss/len(train_loader.dataset):.4f}  Val Loss: {val_loss:.4f}  Val IoU: {val_iou:.4f}\")\n\n    if val_iou > best_iou:\n        best_iou = val_iou\n        torch.save(model.state_dict(), os.path.join(CONFIG['CHECKPOINT_DIR'], 'best_model.pth'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:17:00.762000Z","iopub.execute_input":"2025-12-12T13:17:00.762245Z","iopub.status.idle":"2025-12-12T13:17:04.784922Z","shell.execute_reply.started":"2025-12-12T13:17:00.762224Z","shell.execute_reply":"2025-12-12T13:17:04.783762Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1174351820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: input and target batch or spatial sizes don't match: target [8, 224, 224], input [8, 2, 448, 448]"],"ename":"RuntimeError","evalue":"input and target batch or spatial sizes don't match: target [8, 224, 224], input [8, 2, 448, 448]","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"def predict_and_visualize(model, img_path, img_size=224, device='cpu'):\n    model.eval()\n    img = read_image(img_path, img_size)\n    inp = torch.from_numpy(img.transpose(2,0,1)).float().unsqueeze(0).to(device)/255.0\n    with torch.no_grad():\n        logits = model(inp)\n        pred = logits.argmax(dim=1).squeeze(0).cpu().numpy()\n    \n    overlay = (pred / pred.max() * 255).astype(np.uint8)\n    overlay = cv2.applyColorMap(overlay, cv2.COLORMAP_JET)\n    base = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    blended = cv2.addWeighted(base, 0.6, overlay, 0.4, 0)\n    return blended\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:17:04.785548Z","iopub.status.idle":"2025-12-12T13:17:04.785780Z","shell.execute_reply.started":"2025-12-12T13:17:04.785665Z","shell.execute_reply":"2025-12-12T13:17:04.785675Z"}},"outputs":[],"execution_count":null}]}