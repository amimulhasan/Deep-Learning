{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13727329,
          "sourceType": "datasetVersion",
          "datasetId": 8733622
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amimulhasan/Deep-Learning/blob/main/grad_cam_cnn_vit_gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        (os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "execution_failed": "2025-11-25T18:01:03.443Z"
        },
        "id": "u-SHHgTkM907"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# full_script_with_gradcam_v3.py\n",
        "# ---------------------------\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------\n",
        "# USER SETTINGS (change if needed)\n",
        "# ------------------------\n",
        "dataset_dir = \"/kaggle/input/alz-b-1100/alzheimer_new_11/alzheimer_new\"  # <--- update path if needed\n",
        "img_size = (128, 128)\n",
        "expected_classes = 8    # number of classes in your dataset\n",
        "patch_size = 16\n",
        "projection_dim = 64\n",
        "transformer_layers = 4\n",
        "num_heads = 8\n",
        "batch_size = 32\n",
        "epochs = 5               # set to 100 for full training\n",
        "num_examples = 8         # number of validation examples to save Grad-CAM for\n",
        "\n",
        "# ------------------------\n",
        "# LOAD DATA\n",
        "# ------------------------\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "class_names = sorted([d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))])\n",
        "print(\"âœ… Classes found:\", class_names)\n",
        "if len(class_names) != expected_classes:\n",
        "    raise ValueError(f\"Expected exactly {expected_classes} classes, found {len(class_names)}: {class_names}\")\n",
        "\n",
        "for idx, class_name in enumerate(class_names):\n",
        "    class_path = os.path.join(dataset_dir, class_name)\n",
        "    print(f\"ðŸ” Processing class '{class_name}' ({idx})\")\n",
        "    for root, _, files in os.walk(class_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                img = Image.open(file_path).convert('RGB')\n",
        "                img = img.resize(img_size)\n",
        "                img_array = np.array(img, dtype=np.float32) / 255.0  # normalized to [0,1]\n",
        "                if img_array.shape == (img_size[0], img_size[1], 3):\n",
        "                    X.append(img_array)\n",
        "                    y.append(idx)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Failed to process {file_path}: {e}\")\n",
        "\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.int32)\n",
        "print(f\"\\nâœ… Dataset loaded: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "# ------------------------\n",
        "# TRAIN/VAL SPLIT\n",
        "# ------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "\n",
        "# ------------------------\n",
        "# CUSTOM LAYERS (serializable)\n",
        "# ------------------------\n",
        "@register_keras_serializable()\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super(Patches, self).__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = tf.shape(patches)[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n",
        "\n",
        "@register_keras_serializable()\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super(PatchEncoder, self).__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
        "\n",
        "    def call(self, patches):\n",
        "        batch = tf.shape(patches)[0]\n",
        "        seq_len = tf.shape(patches)[1]\n",
        "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "        pos_embeddings = self.position_embedding(positions)\n",
        "        proj = self.projection(patches)\n",
        "        return proj + pos_embeddings\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"num_patches\": self.num_patches, \"projection_dim\": self.projection_dim})\n",
        "        return config\n",
        "\n",
        "# ------------------------\n",
        "# MODEL BUILD (fixed GRU input shape)\n",
        "# ------------------------\n",
        "def build_hybrid_model(input_shape=(128,128,3), patch_size=16, projection_dim=64, transformer_layers=4, num_heads=8, expected_classes=8):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # CNN branch\n",
        "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "    x_cnn = layers.Flatten()(x)\n",
        "\n",
        "    # ViT branch\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n",
        "    encoded = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "        # Self-attention block\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded)\n",
        "        attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(x1, x1)\n",
        "        x2 = layers.Add()([attn_out, encoded])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        encoded = layers.Add()([x3, x2])\n",
        "\n",
        "    # Project tokens to fixed feature size for GRU (no None dims)\n",
        "    token_features = layers.Dense(256, activation='relu')(encoded)   # shape: (batch, num_patches, 256)\n",
        "    vit_out = layers.GRU(256)(token_features)                        # GRU input fully-defined\n",
        "\n",
        "    # Merge and classify\n",
        "    merged = layers.concatenate([x_cnn, vit_out])\n",
        "    merged = layers.Dense(256, activation='relu')(merged)\n",
        "    merged = layers.Dropout(0.3)(merged)\n",
        "    outputs = layers.Dense(expected_classes, activation='softmax')(merged)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Build and compile\n",
        "model = build_hybrid_model(input_shape=(img_size[0], img_size[1], 3),\n",
        "                           patch_size=patch_size,\n",
        "                           projection_dim=projection_dim,\n",
        "                           transformer_layers=transformer_layers,\n",
        "                           num_heads=num_heads,\n",
        "                           expected_classes=expected_classes)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ------------------------\n",
        "# TRAIN (adjust epochs as you like)\n",
        "# ------------------------\n",
        "start_time = time.time()\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)\n",
        "print(\"Training finished. Time:\", time.time() - start_time)\n",
        "\n",
        "# ------------------------\n",
        "# GRAD-CAM UTILITIES\n",
        "# ------------------------\n",
        "def find_last_conv_layer(model):\n",
        "    for layer in reversed(model.layers):\n",
        "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "            return layer.name\n",
        "    return None\n",
        "\n",
        "def make_gradcam_heatmap(model, img_array, last_conv_layer_name, class_index=None):\n",
        "    \"\"\"\n",
        "    img_array: HxWxC in [0,1]\n",
        "    returns: heatmap HxW normalized 0..1\n",
        "    \"\"\"\n",
        "    img_tensor = tf.expand_dims(img_array, axis=0)\n",
        "    last_conv = model.get_layer(last_conv_layer_name)\n",
        "    grad_model = tf.keras.models.Model([model.inputs], [last_conv.output, model.output])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_tensor)\n",
        "        if class_index is None:\n",
        "            class_index = tf.argmax(predictions[0])\n",
        "        loss = predictions[:, class_index]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
        "    heatmap = tf.maximum(heatmap, 0)\n",
        "    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-9)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def overlay_gradcam_on_image(image, heatmap, alpha=0.4):\n",
        "    heatmap_uint8 = np.uint8(255 * heatmap)\n",
        "    jet = plt.cm.jet(heatmap_uint8 / 255.0)[:, :, :3]\n",
        "    jet_resized = tf.image.resize(jet, (image.shape[0], image.shape[1])).numpy()\n",
        "    overlay = image * (1 - alpha) + jet_resized * alpha\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "    return overlay\n",
        "\n",
        "# ------------------------\n",
        "# GUIDED BACKPROP (guided relu)\n",
        "# ------------------------\n",
        "@tf.custom_gradient\n",
        "def guided_relu(x):\n",
        "    def grad(dy):\n",
        "        return tf.cast(dy > 0, \"float32\") * tf.cast(x > 0, \"float32\") * dy\n",
        "    return tf.nn.relu(x), grad\n",
        "\n",
        "def make_guided_model(original_model):\n",
        "    \"\"\"\n",
        "    Clone the original architecture, load weights, then replace relu activations with guided_relu.\n",
        "    We avoid model.from_config to prevent custom-layer serialization issues.\n",
        "    \"\"\"\n",
        "    # Rebuild model via call to build_hybrid_model (same signature)\n",
        "    guided = build_hybrid_model(input_shape=(img_size[0], img_size[1], 3),\n",
        "                                patch_size=patch_size,\n",
        "                                projection_dim=projection_dim,\n",
        "                                transformer_layers=transformer_layers,\n",
        "                                num_heads=num_heads,\n",
        "                                expected_classes=expected_classes)\n",
        "    guided.set_weights(original_model.get_weights())\n",
        "\n",
        "    # Replace relu activations on layers that have activation attribute\n",
        "    for layer in guided.layers:\n",
        "        if hasattr(layer, \"activation\") and layer.activation == tf.nn.relu:\n",
        "            layer.activation = guided_relu\n",
        "\n",
        "    # Also replace Activation layers if present\n",
        "    for i, layer in enumerate(guided.layers):\n",
        "        if isinstance(layer, tf.keras.layers.Activation):\n",
        "            if layer.activation == tf.nn.relu:\n",
        "                guided.layers[i] = tf.keras.layers.Activation(guided_relu)\n",
        "\n",
        "    return guided\n",
        "\n",
        "# Build guided model\n",
        "guided_model = make_guided_model(model)\n",
        "\n",
        "def guided_backprop(guided_model, img_array, class_index=None):\n",
        "    \"\"\"\n",
        "    Returns guided backpropagation saliency (HxWx3) in [0,1]\n",
        "    \"\"\"\n",
        "    img_tensor = tf.expand_dims(img_array, axis=0)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img_tensor)\n",
        "        preds = guided_model(img_tensor)\n",
        "        if class_index is None:\n",
        "            class_index = tf.argmax(preds[0])\n",
        "        loss = preds[:, class_index]\n",
        "\n",
        "    grads = tape.gradient(loss, img_tensor)[0].numpy()\n",
        "    # Keep positive gradients (guided backprop behavior)\n",
        "    grads = np.maximum(grads, 0.0)\n",
        "    # normalize per image\n",
        "    denom = np.max(grads) - np.min(grads) + 1e-9\n",
        "    grads = (grads - np.min(grads)) / denom\n",
        "    return grads\n",
        "\n",
        "# ------------------------\n",
        "# RUN & SAVE VISUALIZATIONS (Grad-CAM, Guided BP, Guided Grad-CAM)\n",
        "# ------------------------\n",
        "out_dir = \"gradcam_outputs\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "last_conv_name = find_last_conv_layer(model)\n",
        "if last_conv_name is None:\n",
        "    raise RuntimeError(\"No Conv2D found for Grad-CAM target. Check model.\")\n",
        "\n",
        "print(\"Using Conv layer for Grad-CAM:\", last_conv_name)\n",
        "\n",
        "num_examples = min(num_examples, X_val.shape[0])\n",
        "indices = np.random.choice(range(X_val.shape[0]), size=num_examples, replace=False)\n",
        "\n",
        "for idx in indices:\n",
        "    img = X_val[idx]\n",
        "    true_label = int(y_val[idx])\n",
        "    preds = model.predict(np.expand_dims(img, 0))\n",
        "    pred_label = int(np.argmax(preds[0]))\n",
        "\n",
        "    # Grad-CAM\n",
        "    heatmap = make_gradcam_heatmap(model, img, last_conv_name, class_index=pred_label)\n",
        "    heatmap_resized = tf.image.resize(heatmap[..., None], (img.shape[0], img.shape[1])).numpy()[...,0]\n",
        "    overlay = overlay_gradcam_on_image(img, heatmap_resized, alpha=0.45)\n",
        "\n",
        "    # Guided Backprop (on guided_model)\n",
        "    gb = guided_backprop(guided_model, img, class_index=pred_label)  # HxWx3\n",
        "\n",
        "    # Guided Grad-CAM: multiply per-channel guided backprop by single-channel heatmap\n",
        "    guided_gc = gb * np.expand_dims(heatmap_resized, axis=-1)\n",
        "    # Normalize guided_gradcam to [0,1] for visualization\n",
        "    guided_gc = (guided_gc - guided_gc.min()) / (guided_gc.max() - guided_gc.min() + 1e-9)\n",
        "\n",
        "    # Save figure\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    axs[0].imshow(img); axs[0].set_title(f\"Original\\ntrue={true_label}\"); axs[0].axis('off')\n",
        "    axs[1].imshow(heatmap_resized, cmap='jet'); axs[1].set_title(f\"Grad-CAM\\npred={pred_label}\"); axs[1].axis('off')\n",
        "    axs[2].imshow(overlay); axs[2].set_title(\"Grad-CAM Overlay\"); axs[2].axis('off')\n",
        "    axs[3].imshow(guided_gc); axs[3].set_title(\"Guided Grad-CAM\"); axs[3].axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"viz_idx{idx}_pred{pred_label}_true{true_label}.png\")\n",
        "    fig.savefig(out_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "print(\"âœ… All visualizations saved to:\", out_dir)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-11-25T18:01:03.444Z"
        },
        "id": "wwWwdYhjM91E"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}