{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this cell if packages missing. On Kaggle this is usually allowed.\n!pip install --upgrade pip --quiet\n!pip install timm==0.9.2 albumentations==1.4.3 scikit-image scikit-learn --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys, time, math, random\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import List, Tuple, Optional\n\nimport numpy as np\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom skimage.filters import threshold_otsu\nfrom sklearn.cluster import KMeans\n\n# reproducibility\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONFIG = {\n    # Point DATA_DIR to folder that contains all images (can be nested class-wise)\n    'DATA_DIR': '/kaggle/input/brain-tumor-mri-dataset/Training',  \n\n    # allowed image extensions\n    'IMG_EXTS': ('.png', '.jpg', '.jpeg'),\n\n    # image & training params\n    'IMG_SIZE': 256,\n    'NUM_CLASSES': 4,      # include background as class 0 if applicable\n    'BATCH_SIZE': 8,       # reduce if OOM\n    'EPOCHS': 12,\n    'LR': 1e-4,\n    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n\n    # backbone\n    'BACKBONE': 'swin_tiny_patch4_window7_224', # smaller default\n    'PRETRAINED': True,\n\n    # training utils\n    'CHECKPOINT_DIR': '/kaggle/working/checkpoints',\n    'GENERATED_MASKS_DIR': '/kaggle/working/masks_auto',\n    'MODE': 'auto_mask',   # we will auto-generate masks\n    'NUM_WORKERS': 0       # set 0 for Kaggle kernel stability; change if desired\n}\n\nos.makedirs(CONFIG['CHECKPOINT_DIR'], exist_ok=True)\nos.makedirs(CONFIG['GENERATED_MASKS_DIR'], exist_ok=True)\n\nprint(\"Device:\", CONFIG['DEVICE'])\nprint(\"Images root:\", CONFIG['DATA_DIR'])\nprint(\"Masks saved to:\", CONFIG['GENERATED_MASKS_DIR'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collect_images(root_dir: str, exts=('.png','.jpg','.jpeg')) -> List[str]:\n    p = Path(root_dir)\n    files = []\n    for ext in exts:\n        files += [str(x) for x in p.rglob(f'*{ext}')]\n    files = sorted(files)\n    return files\n\ndef read_image(path: str, size: int) -> np.ndarray:\n    img = cv2.imread(path)\n    if img is None:\n        raise FileNotFoundError(f\"Image not found or unreadable: {path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if (img.shape[0], img.shape[1]) != (size, size):\n        img = cv2.resize(img, (size, size), interpolation=cv2.INTER_LINEAR)\n    return img\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_pseudo_mask(img: np.ndarray, num_classes: int, sample_limit=20000) -> np.ndarray:\n    \"\"\"\n    img: HxWx3 RGB (resized already)\n    returns HxW uint8 mask with values 0..num_classes-1\n    \"\"\"\n    h, w, _ = img.shape\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    try:\n        t = threshold_otsu(gray)\n        bin_mask = (gray > t).astype(np.uint8)\n    except Exception:\n        _, bin_mask = cv2.threshold(gray, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n    # morphological cleaning\n    kernel = np.ones((5,5), np.uint8)\n    clean = cv2.morphologyEx(bin_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n    clean = cv2.morphologyEx(clean, cv2.MORPH_CLOSE, kernel)\n\n    if num_classes == 2:\n        return clean.astype(np.uint8)\n\n    # Multi-class: KMeans clustering on RGB of whole image (fast sampling)\n    Z = img.reshape(-1, 3).astype(np.float32)\n    n_samples = min(sample_limit, Z.shape[0])\n    # sample indices without replacement\n    sample_idx = np.random.choice(Z.shape[0], n_samples, replace=False)\n    sample = Z[sample_idx]\n    K = num_classes\n    km = KMeans(n_clusters=K, n_init=3, random_state=SEED)\n    km.fit(sample)\n    labels = km.predict(Z).reshape(h, w).astype(np.uint8)\n\n    # choose cluster with lowest mean intensity as background\n    centers = km.cluster_centers_\n    center_means = centers.mean(axis=1)\n    bg_cluster = int(np.argmin(center_means))\n\n    remapped = np.zeros_like(labels)\n    cur = 1\n    for c in range(K):\n        if c == bg_cluster:\n            remapped[labels==c] = 0\n        else:\n            remapped[labels==c] = cur\n            cur += 1\n            if cur >= num_classes:\n                cur = num_classes - 1\n    return remapped.astype(np.uint8)\n\ndef save_mask(mask: np.ndarray, out_path: str):\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    # write as 8-bit single channel image\n    cv2.imwrite(out_path, mask.astype(np.uint8))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = collect_images(CONFIG['DATA_DIR'], CONFIG['IMG_EXTS'])\nprint(f\"Found {len(images)} images under {CONFIG['DATA_DIR']}\")\n\nif len(images) == 0:\n    raise SystemExit(f\"No images found in {CONFIG['DATA_DIR']}. Check path.\")\n\n# generate masks (skips if mask already exists)\nfor i, img_path in enumerate(images):\n    fname = os.path.basename(img_path)\n    out_mask = os.path.join(CONFIG['GENERATED_MASKS_DIR'], fname)\n    if os.path.exists(out_mask):\n        if i % 200 == 0:\n            print(f\"[{i}] mask exists, skipping {fname}\")\n        continue\n    try:\n        img = read_image(img_path, CONFIG['IMG_SIZE'])\n        mask = generate_pseudo_mask(img, CONFIG['NUM_CLASSES'])\n        save_mask(mask, out_mask)\n    except Exception as e:\n        print(f\"Failed to generate mask for {img_path} -> {e}\")\n    if (i+1) % 200 == 0 or i==len(images)-1:\n        print(f\"[{i+1}/{len(images)}] processed {fname}\")\n\nprint(\"Mask generation completed. Masks saved in:\", CONFIG['GENERATED_MASKS_DIR'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MedicalSegDataset(Dataset):\n    def __init__(self, image_paths: List[str], mask_dir: str, img_size:int=256, transform=None):\n        self.image_paths = image_paths\n        self.mask_dir = mask_dir\n        self.img_size = img_size\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        fname = os.path.basename(img_path)\n        mask_path = os.path.join(self.mask_dir, fname)\n\n        img = read_image(img_path, self.img_size)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        if mask is None:\n            mask = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n\n        if self.transform:\n            aug = self.transform(image=img, mask=mask)\n            img = aug['image']\n            mask = aug['mask']\n        else:\n            img = torch.from_numpy(img.transpose(2,0,1)).float() / 255.0\n            mask = torch.from_numpy(mask).long()\n\n        return img, mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self,x): return self.conv(x)\n\nclass UpConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ConvBlock(in_ch, out_ch)\n        )\n    def forward(self,x): return self.up(x)\n\nclass SwinGRUSegmenter(nn.Module):\n    def __init__(self, backbone_name='swin_tiny_patch4_window7_224', pretrained=True, num_classes=4):\n        super().__init__()\n        # create backbone (features only)\n        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, features_only=True, out_indices=(0,1,2,3))\n        feats = self.backbone.feature_info.channels()\n        # channel reducers\n        self.reduce4 = nn.Conv2d(feats[-1], 512, 1)\n        self.reduce3 = nn.Conv2d(feats[-2], 256, 1)\n        self.reduce2 = nn.Conv2d(feats[-3], 128, 1)\n        self.reduce1 = nn.Conv2d(feats[-4], 64, 1)\n\n        self.gru_in_dim = feats[-2]\n        self.gru = nn.GRU(input_size=self.gru_in_dim, hidden_size=256, batch_first=True)\n        self.gru_proj = nn.Linear(256, 256)\n\n        self.dec4 = ConvBlock(512, 256)\n        self.up3 = UpConv(256+256, 128)\n        self.up2 = UpConv(128+128, 64)\n        self.up1 = UpConv(64+64, 32)\n        self.final_conv = nn.Sequential(nn.Conv2d(32,32,3,padding=1), nn.ReLU(inplace=True), nn.Conv2d(32, CONFIG['NUM_CLASSES'],1))\n\n    def forward(self,x):\n        feats = self.backbone(x)\n        s1,s2,s3,s4 = feats\n        r4 = self.reduce4(s4); r3 = self.reduce3(s3); r2 = self.reduce2(s2); r1 = self.reduce1(s1)\n        d4 = self.dec4(r4)\n        b,c,h,w = r3.shape\n        tokens = r3.view(b,c,h*w).permute(0,2,1)\n        gru_out, _ = self.gru(tokens)\n        pooled = gru_out.mean(dim=1)\n        pooled = self.gru_proj(pooled).unsqueeze(-1).unsqueeze(-1)\n        pooled = pooled.expand(-1,-1,d4.shape[2],d4.shape[3])\n        d4 = d4 + pooled\n        u3 = F.interpolate(d4, scale_factor=2, mode='bilinear', align_corners=False); u3 = torch.cat([u3, r3], dim=1); u3 = self.up3(u3)\n        u2 = F.interpolate(u3, scale_factor=2, mode='bilinear', align_corners=False); u2 = torch.cat([u2, r2], dim=1); u2 = self.up2(u2)\n        u1 = F.interpolate(u2, scale_factor=2, mode='bilinear', align_corners=False); u1 = torch.cat([u1, r1], dim=1); u1 = self.up1(u1)\n        out = self.final_conv(u1)\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transforms\ntrain_tf = A.Compose([\n    A.Resize(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.3),\n    A.Normalize(),\n    ToTensorV2(),\n])\nval_tf = A.Compose([\n    A.Resize(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']),\n    A.Normalize(),\n    ToTensorV2(),\n])\n\nall_images = collect_images(CONFIG['DATA_DIR'], CONFIG['IMG_EXTS'])\nn = len(all_images)\nprint(\"Total images:\", n)\nif n == 0:\n    raise SystemExit(\"No images found. Check CONFIG['DATA_DIR'].\")\n\n# split\nidxs = np.arange(n)\nnp.random.shuffle(idxs)\nsplit = int(0.8 * n)\ntrain_imgs = [all_images[i] for i in idxs[:split]]\nval_imgs   = [all_images[i] for i in idxs[split:]]\n\ntrain_ds = MedicalSegDataset(train_imgs, CONFIG['GENERATED_MASKS_DIR'], img_size=CONFIG['IMG_SIZE'], transform=train_tf)\nval_ds   = MedicalSegDataset(val_imgs, CONFIG['GENERATED_MASKS_DIR'], img_size=CONFIG['IMG_SIZE'], transform=val_tf)\n\ntrain_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\nval_loader   = DataLoader(val_ds,   batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n\nprint(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(pred, target, eps=1e-6):\n    pred = F.softmax(pred, dim=1)\n    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0,3,1,2).float()\n    inter = (pred * target_onehot).sum(dim=(2,3))\n    denom = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3))\n    dice = (2*inter + eps)/(denom + eps)\n    return 1 - dice.mean()\n\ndef iou_score(pred, target, num_classes):\n    pred_labels = pred.argmax(dim=1)\n    ious = []\n    for cls in range(num_classes):\n        pred_c = (pred_labels == cls)\n        target_c = (target == cls)\n        inter = (pred_c & target_c).sum().item()\n        union = (pred_c | target_c).sum().item()\n        if union == 0:\n            ious.append(1.0)\n        else:\n            ious.append(inter/union)\n    return np.mean(ious)\n\ndevice = CONFIG['DEVICE']\nmodel = SwinGRUSegmenter(backbone_name=CONFIG['BACKBONE'], pretrained=CONFIG['PRETRAINED'], num_classes=CONFIG['NUM_CLASSES']).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['LR'])\nscaler = torch.cuda.amp.GradScaler() if device.startswith('cuda') else None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch():\n    model.train()\n    total_loss = 0.0\n    for imgs, masks in train_loader:\n        imgs = imgs.to(device); masks = masks.to(device)\n        optimizer.zero_grad()\n        if scaler is not None:\n            with torch.cuda.amp.autocast():\n                logits = model(imgs)\n                ce = F.cross_entropy(logits, masks)\n                d = dice_loss(logits, masks)\n                loss = ce + d\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            logits = model(imgs)\n            ce = F.cross_entropy(logits, masks)\n            d = dice_loss(logits, masks)\n            loss = ce + d\n            loss.backward(); optimizer.step()\n        total_loss += loss.item() * imgs.size(0)\n    return total_loss / len(train_loader.dataset)\n\ndef validate_epoch():\n    model.eval()\n    total_loss = 0.0; total_iou = 0.0\n    with torch.no_grad():\n        for imgs, masks in val_loader:\n            imgs = imgs.to(device); masks = masks.to(device)\n            if scaler is not None:\n                with torch.cuda.amp.autocast():\n                    logits = model(imgs)\n            else:\n                logits = model(imgs)\n            ce = F.cross_entropy(logits, masks)\n            d = dice_loss(logits, masks)\n            loss = ce + d\n            total_loss += loss.item() * imgs.size(0)\n            total_iou += iou_score(logits, masks, CONFIG['NUM_CLASSES']) * imgs.size(0)\n    return total_loss / len(val_loader.dataset), total_iou / len(val_loader.dataset)\n\nbest_iou = 0.0\nfor epoch in range(1, CONFIG['EPOCHS']+1):\n    t0 = time.time()\n    train_loss = train_one_epoch()\n    val_loss, val_iou = validate_epoch()\n    t1 = time.time()\n    print(f\"Epoch {epoch}/{CONFIG['EPOCHS']} time {t1-t0:.1f}s train_loss {train_loss:.4f} val_loss {val_loss:.4f} val_iou {val_iou:.4f}\")\n    ckpt = {'epoch':epoch, 'model_state':model.state_dict(), 'optim_state':optimizer.state_dict(), 'val_iou':val_iou}\n    torch.save(ckpt, os.path.join(CONFIG['CHECKPOINT_DIR'], f'epoch_{epoch}.pth'))\n    if val_iou > best_iou:\n        best_iou = val_iou\n        torch.save(ckpt, os.path.join(CONFIG['CHECKPOINT_DIR'], 'best.pth'))\n\nprint(\"Training finished. Best val IoU:\", best_iou)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef predict_mask(model, img_path):\n    model.eval()\n    img = read_image(img_path, CONFIG['IMG_SIZE'])\n    inp = torch.from_numpy(img.transpose(2,0,1)).float().unsqueeze(0)/255.0\n    inp = inp.to(device)\n    with torch.no_grad():\n        if scaler is not None:\n            with torch.cuda.amp.autocast():\n                logits = model(inp)\n        else:\n            logits = model(inp)\n        pred = logits.argmax(dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n    return img, pred\n\n# show 4 samples from val set (or less if fewer)\nn_show = min(4, len(val_imgs))\nfor i in range(n_show):\n    p = val_imgs[i]\n    img, pred = predict_mask(model, p)\n    gt = cv2.imread(os.path.join(CONFIG['GENERATED_MASKS_DIR'], os.path.basename(p)), cv2.IMREAD_GRAYSCALE)\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,3,1); plt.imshow(img); plt.title(\"Image\"); plt.axis('off')\n    plt.subplot(1,3,2); plt.imshow(pred, cmap='jet'); plt.title(\"Pred mask\"); plt.axis('off')\n    plt.subplot(1,3,3); plt.imshow(gt, cmap='gray'); plt.title(\"Pseudo GT mask\"); plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# zip generated masks or checkpoints\nimport shutil\nsh_masks = '/kaggle/working/masks_auto.zip'\nsh_ckpt  = '/kaggle/working/checkpoints.zip'\nshutil.make_archive('/kaggle/working/masks_auto', 'zip', CONFIG['GENERATED_MASKS_DIR'])\nshutil.make_archive('/kaggle/working/checkpoints', 'zip', CONFIG['CHECKPOINT_DIR'])\nprint(\"Zipped masks to\", sh_masks)\nprint(\"Zipped checkpoints to\", sh_ckpt)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}