{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:54:15.334351Z","iopub.execute_input":"2025-12-22T07:54:15.334963Z","iopub.status.idle":"2025-12-22T07:54:26.155991Z","shell.execute_reply.started":"2025-12-22T07:54:15.334931Z","shell.execute_reply":"2025-12-22T07:54:26.155382Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Install PyTorch Geometric dependencies\n!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n\n# Install main torch_geometric package\n!pip install torch-geometric\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:58:31.400448Z","iopub.execute_input":"2025-12-22T07:58:31.401208Z","iopub.status.idle":"2025-12-22T07:58:43.455596Z","shell.execute_reply.started":"2025-12-22T07:58:31.401179Z","shell.execute_reply":"2025-12-22T07:58:43.454914Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.1.0+cu118.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp311-cp311-linux_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_cluster-1.6.3%2Bpt21cu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hCollecting torch-spline-conv\n  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt21cu118-cp311-cp311-linux_x86_64.whl (891 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.8/891.8 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse) (2024.2.0)\nInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\nSuccessfully installed torch-cluster-1.6.3+pt21cu118 torch-scatter-2.1.2+pt21cu118 torch-sparse-0.6.18+pt21cu118 torch-spline-conv-1.2.2+pt21cu118\nCollecting torch-geometric\n  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.13.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.10.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.1.3)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.22.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.10.5)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.7.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===============================\n# Hybrid Model: Attention + ViT + MCNN\n# 4-Class Dataset Pipeline\n# ===============================\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch.optim as optim\nimport numpy as np\n\n# ------------------------------\n# SETTINGS\n# ------------------------------\nIMG_SIZE = 128\nPATCH_SIZE = 16\nBATCH_SIZE = 8\nEPOCHS = 10\nNUM_CLASSES = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ------------------------------\n# DATASET\n# ------------------------------\nclass CustomMRI(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        class_map = {cls_name: idx for idx, cls_name in enumerate(os.listdir(data_dir))}\n        for cls_name, idx in class_map.items():\n            cls_dir = os.path.join(data_dir, cls_name)\n            for fname in os.listdir(cls_dir):\n                if fname.lower().endswith(('png', 'jpg', 'jpeg')):\n                    self.image_paths.append(os.path.join(cls_dir, fname))\n                    self.labels.append(idx)\n                \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, label\n\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ndataset_dir = \"/kaggle/input/brain-tumor-mri-dataset/Training\"\ndataset = CustomMRI(dataset_dir, transform=transform)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n# ------------------------------\n# MULTI-SCALE CNN\n# ------------------------------\nclass MultiScaleCNN(nn.Module):\n    def __init__(self, out_features=128):\n        super().__init__()\n        self.conv3 = nn.Conv2d(3, 32, 3, padding=1)\n        self.conv5 = nn.Conv2d(3, 32, 5, padding=2)\n        self.conv7 = nn.Conv2d(3, 32, 7, padding=3)\n        self.pool = nn.MaxPool2d(2)\n        self.fc = nn.Linear(32*3*(IMG_SIZE//2)*(IMG_SIZE//2), out_features)\n    \n    def forward(self, x):\n        x3 = F.relu(self.pool(self.conv3(x)))\n        x5 = F.relu(self.pool(self.conv5(x)))\n        x7 = F.relu(self.pool(self.conv7(x)))\n        x_cat = torch.cat([x3, x5, x7], dim=1)\n        x_flat = x_cat.view(x_cat.size(0), -1)\n        return self.fc(x_flat)\n\n# ------------------------------\n# SIMPLE ViT\n# ------------------------------\nclass SimpleViT(nn.Module):\n    def __init__(self, img_size=IMG_SIZE, patch_size=PATCH_SIZE, in_ch=3, emb_dim=128, num_heads=4, depth=4):\n        super().__init__()\n        self.patch_embed = nn.Conv2d(in_ch, emb_dim, kernel_size=patch_size, stride=patch_size)\n        num_patches = (img_size // patch_size) ** 2\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, emb_dim))\n        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n    \n    def forward(self, x):\n        B = x.size(0)\n        x = self.patch_embed(x).flatten(2).transpose(1,2)\n        x = x + self.pos_embed\n        x = self.transformer(x)\n        x = x.mean(dim=1)\n        return x\n\n# ------------------------------\n# ATTENTION BRANCH (replaces GAT)\n# ------------------------------\nclass AttentionBranch(nn.Module):\n    def __init__(self, in_dim=128, out_dim=128):\n        super().__init__()\n        self.query = nn.Linear(in_dim, out_dim)\n        self.key = nn.Linear(in_dim, out_dim)\n        self.value = nn.Linear(in_dim, out_dim)\n    \n    def forward(self, x):\n        # x: [batch_size, features]\n        Q = self.query(x).unsqueeze(1)\n        K = self.key(x).unsqueeze(1)\n        V = self.value(x).unsqueeze(1)\n        attn_scores = torch.bmm(Q, K.transpose(1,2)) / np.sqrt(Q.size(-1))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        out = torch.bmm(attn_weights, V).squeeze(1)\n        return out\n\n# ------------------------------\n# HYBRID MODEL\n# ------------------------------\nclass HybridModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        self.mcnn = MultiScaleCNN()\n        self.vit = SimpleViT()\n        self.attn = AttentionBranch()\n        self.fc = nn.Linear(128+128+128, num_classes)\n    \n    def forward(self, x):\n        x1 = self.mcnn(x)\n        x2 = self.vit(x)\n        x3 = self.attn(x1)\n        x_cat = torch.cat([x1, x2, x3], dim=1)\n        return self.fc(x_cat)\n\n# ------------------------------\n# TRAINING\n# ------------------------------\nmodel = HybridModel().to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, correct = 0, 0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        correct += (outputs.argmax(1) == labels).sum().item()\n    print(f\"Epoch {epoch+1}, Loss={total_loss:.4f}, Train Acc={correct/len(train_loader.dataset):.4f}\")\n\n# ------------------------------\n# VALIDATION\n# ------------------------------\nmodel.eval()\ncorrect = 0\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        outputs = model(imgs)\n        correct += (outputs.argmax(1) == labels).sum().item()\nval_acc = correct / len(val_loader.dataset)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:01:39.035930Z","iopub.execute_input":"2025-12-22T08:01:39.036581Z","iopub.status.idle":"2025-12-22T08:06:39.111829Z","shell.execute_reply.started":"2025-12-22T08:01:39.036554Z","shell.execute_reply":"2025-12-22T08:06:39.111035Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss=444.4494, Train Acc=0.7597\nEpoch 2, Loss=178.0936, Train Acc=0.8833\nEpoch 3, Loss=108.3268, Train Acc=0.9354\nEpoch 4, Loss=76.4142, Train Acc=0.9545\nEpoch 5, Loss=55.6940, Train Acc=0.9659\nEpoch 6, Loss=44.0605, Train Acc=0.9753\nEpoch 7, Loss=42.4205, Train Acc=0.9801\nEpoch 8, Loss=20.9212, Train Acc=0.9902\nEpoch 9, Loss=39.9999, Train Acc=0.9821\nEpoch 10, Loss=21.1738, Train Acc=0.9873\nValidation Accuracy: 0.9073\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ===============================\n# Hybrid Model: GAT + ViT + MCNN\n# 4-Class Dataset A-to-Z Pipeline\n# ===============================\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch.optim as optim\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.data import Data, Batch\n\nimport numpy as np\n\n# ------------------------------\n# SETTINGS\n# ------------------------------\nIMG_SIZE = 128\nPATCH_SIZE = 16\nBATCH_SIZE = 8\nEPOCHS = 10\nNUM_CLASSES = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ------------------------------\n# DATASET\n# ------------------------------\nclass CustomMRI(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        class_map = {cls_name: idx for idx, cls_name in enumerate(os.listdir(data_dir))}\n        for cls_name, idx in class_map.items():\n            cls_dir = os.path.join(data_dir, cls_name)\n            for fname in os.listdir(cls_dir):\n                if fname.lower().endswith(('png', 'jpg', 'jpeg')):\n                    self.image_paths.append(os.path.join(cls_dir, fname))\n                    self.labels.append(idx)\n                \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, label\n\n# Transforms\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# Dataset path\ndataset_dir = \"/kaggle/input/brain-tumor-mri-dataset/Training\"  # change to your dataset\ndataset = CustomMRI(dataset_dir, transform=transform)\n\n# Split\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n# ------------------------------\n# MULTI-SCALE CNN BRANCH\n# ------------------------------\nclass MultiScaleCNN(nn.Module):\n    def __init__(self, out_features=128):\n        super().__init__()\n        self.conv3 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n        self.conv7 = nn.Conv2d(3, 32, kernel_size=7, padding=3)\n        self.pool = nn.MaxPool2d(2)\n        self.fc = nn.Linear(32*3*(IMG_SIZE//2)*(IMG_SIZE//2), out_features)\n    \n    def forward(self, x):\n        x3 = F.relu(self.pool(self.conv3(x)))\n        x5 = F.relu(self.pool(self.conv5(x)))\n        x7 = F.relu(self.pool(self.conv7(x)))\n        x_cat = torch.cat([x3, x5, x7], dim=1)\n        x_flat = x_cat.view(x_cat.size(0), -1)\n        return self.fc(x_flat)\n\n# ------------------------------\n# SIMPLE ViT BRANCH\n# ------------------------------\nclass SimpleViT(nn.Module):\n    def __init__(self, img_size=IMG_SIZE, patch_size=PATCH_SIZE, in_ch=3, emb_dim=128, num_heads=4, depth=4):\n        super().__init__()\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.emb_dim = emb_dim\n        \n        self.patch_embed = nn.Conv2d(in_ch, emb_dim, kernel_size=patch_size, stride=patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, emb_dim))\n        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n    \n    def forward(self, x):\n        B = x.size(0)\n        x = self.patch_embed(x)           # B x emb_dim x H' x W'\n        x = x.flatten(2).transpose(1,2)  # B x num_patches x emb_dim\n        x = x + self.pos_embed\n        x = self.transformer(x)           # B x num_patches x emb_dim\n        x = x.mean(dim=1)                 # global pooling\n        return x\n\n# ------------------------------\n# GAT BRANCH\n# ------------------------------\nclass SimpleGAT(nn.Module):\n    def __init__(self, in_feats=128, hidden=64, out_feats=128):\n        super().__init__()\n        self.gat1 = GATConv(in_feats, hidden, heads=4, concat=True)\n        self.gat2 = GATConv(hidden*4, out_feats, heads=1, concat=True)\n    \n    def forward(self, x, edge_index):\n        x = F.elu(self.gat1(x, edge_index))\n        x = self.gat2(x, edge_index)\n        return x.mean(dim=0)\n\n# ------------------------------\n# HYBRID MODEL\n# ------------------------------\nclass HybridModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        self.mcnn = MultiScaleCNN()\n        self.vit = SimpleViT()\n        self.gat = SimpleGAT()\n        self.fc = nn.Linear(128+128+128, num_classes)\n    \n    def forward(self, img, node_features, edge_index):\n        x1 = self.mcnn(img)\n        x2 = self.vit(img)\n        x3 = self.gat(node_features, edge_index)\n        x = torch.cat([x1, x2, x3], dim=1)\n        return self.fc(x)\n\n# ------------------------------\n# INITIALIZE\n# ------------------------------\nmodel = HybridModel().to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n# ------------------------------\n# TRAINING LOOP\n# ------------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    correct = 0\n    for imgs, labels in train_loader:\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        # Generate graph from image patches\n        B = imgs.size(0)\n        node_features = imgs.unfold(2, PATCH_SIZE, PATCH_SIZE).unfold(3, PATCH_SIZE, PATCH_SIZE)\n        node_features = node_features.contiguous().view(B, -1, 3*PATCH_SIZE*PATCH_SIZE)\n        node_features = nn.Linear(3*PATCH_SIZE*PATCH_SIZE, 128).to(DEVICE)(node_features)\n        \n        # Fully connect graph for simplicity\n        num_nodes = node_features.size(1)\n        row = []\n        col = []\n        for i in range(num_nodes):\n            for j in range(num_nodes):\n                row.append(i)\n                col.append(j)\n        edge_index = torch.tensor([row, col], dtype=torch.long).to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(imgs, node_features[0], edge_index)  # node_features[0] for batch simplification\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        correct += (outputs.argmax(1) == labels).sum().item()\n    \n    acc = correct / len(train_loader.dataset)\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}, Train Acc: {acc:.4f}\")\n\n# ------------------------------\n# VALIDATION\n# ------------------------------\nmodel.eval()\ncorrect = 0\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        B = imgs.size(0)\n        node_features = imgs.unfold(2, PATCH_SIZE, PATCH_SIZE).unfold(3, PATCH_SIZE, PATCH_SIZE)\n        node_features = node_features.contiguous().view(B, -1, 3*PATCH_SIZE*PATCH_SIZE)\n        node_features = nn.Linear(3*PATCH_SIZE*PATCH_SIZE, 128).to(DEVICE)(node_features)\n        \n        num_nodes = node_features.size(1)\n        row = []\n        col = []\n        for i in range(num_nodes):\n            for j in range(num_nodes):\n                row.append(i)\n                col.append(j)\n        edge_index = torch.tensor([row, col], dtype=torch.long).to(DEVICE)\n        \n        outputs = model(imgs, node_features[0], edge_index)\n        correct += (outputs.argmax(1) == labels).sum().item()\nval_acc = correct / len(val_loader.dataset)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:58:47.469284Z","iopub.execute_input":"2025-12-22T07:58:47.469584Z","iopub.status.idle":"2025-12-22T07:58:52.819331Z","shell.execute_reply.started":"2025-12-22T07:58:47.469556Z","shell.execute_reply":"2025-12-22T07:58:52.818251Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n  import torch_geometric.typing\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2591049333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# node_features[0] for batch simplification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2591049333.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, node_features, edge_index)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"],"ename":"RuntimeError","evalue":"Tensors must have same number of dimensions: got 2 and 1","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# ===========================\n# A-to-Z Hybrid: GAT + ViT + MCNN\n# ===========================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets, models\nimport torch.optim as optim\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.data import Data, Batch\n\nimport numpy as np\nfrom PIL import Image\nimport os\n\n# --------------------------\n# SETTINGS\n# --------------------------\nIMG_SIZE = 128\nBATCH_SIZE = 8\nEPOCHS = 30\nNUM_CLASSES = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --------------------------\n# DATASET\n# --------------------------\nclass CustomMRI(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        class_map = {cls_name: idx for idx, cls_name in enumerate(os.listdir(data_dir))}\n        for cls_name, idx in class_map.items():\n            cls_dir = os.path.join(data_dir, cls_name)\n            for fname in os.listdir(cls_dir):\n                self.image_paths.append(os.path.join(cls_dir, fname))\n                self.labels.append(idx)\n                \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, label\n\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ndataset = CustomMRI(\"/kaggle/input/brain-tumor-mri-dataset/Training\", transform=transform)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n# --------------------------\n# MULTI-SCALE CNN BRANCH\n# --------------------------\nclass MultiScaleCNN(nn.Module):\n    def __init__(self, out_features=128):\n        super().__init__()\n        self.conv3 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n        self.conv7 = nn.Conv2d(3, 32, kernel_size=7, padding=3)\n        self.pool = nn.MaxPool2d(2)\n        self.fc = nn.Linear(32*3*(IMG_SIZE//2)*(IMG_SIZE//2), out_features)\n    \n    def forward(self, x):\n        x3 = F.relu(self.pool(self.conv3(x)))\n        x5 = F.relu(self.pool(self.conv5(x)))\n        x7 = F.relu(self.pool(self.conv7(x)))\n        x_cat = torch.cat([x3, x5, x7], dim=1)\n        x_flat = x_cat.view(x_cat.size(0), -1)\n        return self.fc(x_flat)\n\n# --------------------------\n# VIT BRANCH\n# --------------------------\nclass SimpleViT(nn.Module):\n    def __init__(self, img_size=IMG_SIZE, patch_size=16, in_ch=3, emb_dim=128, num_heads=4, depth=4):\n        super().__init__()\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.emb_dim = emb_dim\n        \n        self.patch_embed = nn.Conv2d(in_ch, emb_dim, kernel_size=patch_size, stride=patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, emb_dim))\n        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n    \n    def forward(self, x):\n        B = x.size(0)\n        x = self.patch_embed(x) # B x emb_dim x H' x W'\n        x = x.flatten(2).transpose(1,2) # B x num_patches x emb_dim\n        x = x + self.pos_embed\n        x = self.transformer(x) # B x num_patches x emb_dim\n        x = x.mean(dim=1) # global average pooling\n        return x\n\n# --------------------------\n# GAT BRANCH\n# --------------------------\nclass SimpleGAT(nn.Module):\n    def __init__(self, in_feats=128, hidden=64, out_feats=128):\n        super().__init__()\n        self.gat1 = GATConv(in_feats, hidden, heads=4, concat=True)\n        self.gat2 = GATConv(hidden*4, out_feats, heads=1, concat=True)\n    \n    def forward(self, x, edge_index):\n        x = F.elu(self.gat1(x, edge_index))\n        x = self.gat2(x, edge_index)\n        return x.mean(dim=0) # global node pooling\n\n# --------------------------\n# HYBRID MODEL\n# --------------------------\nclass HybridModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        self.mcnn = MultiScaleCNN()\n        self.vit = SimpleViT()\n        self.gat = SimpleGAT()\n        self.fc = nn.Linear(128+128+128, num_classes)\n    \n    def forward(self, img, node_features, edge_index):\n        x1 = self.mcnn(img)\n        x2 = self.vit(img)\n        x3 = self.gat(node_features, edge_index)\n        x = torch.cat([x1, x2, x3], dim=1)\n        return self.fc(x)\n\n# --------------------------\n# INITIALIZE MODEL\n# --------------------------\nmodel = HybridModel().to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n# --------------------------\n# TRAINING LOOP\n# --------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    correct = 0\n    for imgs, labels in train_loader:\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        # For demonstration: generate dummy node_features & edge_index\n        # In practice, construct graph from ROI or patches\n        B = imgs.size(0)\n        node_features = torch.randn(B, 10, 128).to(DEVICE)  # 10 nodes per image\n        edge_index = torch.tensor([[i,j] for i in range(10) for j in range(10)], dtype=torch.long).t().contiguous().to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(imgs, node_features, edge_index)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        correct += (outputs.argmax(1) == labels).sum().item()\n    \n    acc = correct / len(train_loader.dataset)\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}, Train Acc: {acc:.4f}\")\n\n# --------------------------\n# VALIDATION\n# --------------------------\nmodel.eval()\ncorrect = 0\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        B = imgs.size(0)\n        node_features = torch.randn(B, 10, 128).to(DEVICE)\n        edge_index = torch.tensor([[i,j] for i in range(10) for j in range(10)], dtype=torch.long).t().contiguous().to(DEVICE)\n        outputs = model(imgs, node_features, edge_index)\n        correct += (outputs.argmax(1) == labels).sum().item()\nval_acc = correct / len(val_loader.dataset)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:54:32.742630Z","iopub.status.idle":"2025-12-22T07:54:32.742910Z","shell.execute_reply.started":"2025-12-22T07:54:32.742752Z","shell.execute_reply":"2025-12-22T07:54:32.742781Z"}},"outputs":[],"execution_count":null}]}