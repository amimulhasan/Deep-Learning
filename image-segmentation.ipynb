{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T18:05:58.183898Z","iopub.execute_input":"2025-12-10T18:05:58.184498Z","iopub.status.idle":"2025-12-10T18:06:27.702429Z","shell.execute_reply.started":"2025-12-10T18:05:58.184472Z","shell.execute_reply":"2025-12-10T18:06:27.701812Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Run this cell once at top. It installs MONAI, timm and skimage etc.\n!pip install -q monai==1.3.0\n!pip install -q timm\n!pip install -q scikit-image\n!pip install -q opencv-python-headless\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T18:06:27.703628Z","iopub.execute_input":"2025-12-10T18:06:27.704037Z","iopub.status.idle":"2025-12-10T18:07:42.285968Z","shell.execute_reply.started":"2025-12-10T18:06:27.704017Z","shell.execute_reply":"2025-12-10T18:07:42.285232Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, sys, glob, shutil, random, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom skimage import filters, morphology, exposure\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom monai.networks.nets import SwinUNETR\nfrom monai.transforms import (\n    Compose, LoadImage, EnsureChannelFirst, Resize, ScaleIntensity, ToTensor\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T18:07:42.287004Z","iopub.execute_input":"2025-12-10T18:07:42.287376Z","iopub.status.idle":"2025-12-10T18:08:19.064166Z","shell.execute_reply.started":"2025-12-10T18:07:42.287348Z","shell.execute_reply":"2025-12-10T18:08:19.063496Z"}},"outputs":[{"name":"stderr","text":"2025-12-10 18:08:02.716619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765390082.883824      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765390082.935534      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Set the path to the lung images folder inside the Kaggle dataset\nDATA_DIR = \"/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets\"\nassert os.path.exists(DATA_DIR), f\"Dataset folder not found at {DATA_DIR}. Attach dataset to the notebook.\"\n\n# show folders and counts\nfolders = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\nprint(\"Detected folders:\", folders)\nfor f in folders:\n    p = os.path.join(DATA_DIR, f)\n    imgs = len([x for x in os.listdir(p) if x.lower().endswith(('.jpg','.png','.jpeg'))])\n    print(f, \"->\", imgs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T18:08:19.064941Z","iopub.execute_input":"2025-12-10T18:08:19.065584Z","iopub.status.idle":"2025-12-10T18:08:19.085389Z","shell.execute_reply.started":"2025-12-10T18:08:19.065563Z","shell.execute_reply":"2025-12-10T18:08:19.084627Z"}},"outputs":[{"name":"stdout","text":"Detected folders: ['lung_aca', 'lung_n', 'lung_scc']\nlung_aca -> 5000\nlung_n -> 5000\nlung_scc -> 5000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"WORKDIR = \"/kaggle/working/lung_work\"\nos.makedirs(WORKDIR, exist_ok=True)\nIMG_DIR = os.path.join(WORKDIR, \"images\")\nMASK_DIR = os.path.join(WORKDIR, \"pseudo_masks\")\nos.makedirs(IMG_DIR, exist_ok=True)\nos.makedirs(MASK_DIR, exist_ok=True)\n\n# copy lung folders (only) into working images folder (so original input untouched)\nlung_dirs = [d for d in folders if 'lung' in d.lower()]\nprint(\"lung_dirs:\", lung_dirs)\ncount=0\nfor d in lung_dirs:\n    src = os.path.join(DATA_DIR, d)\n    for f in os.listdir(src):\n        if f.lower().endswith(('.jpg','.png','.jpeg')):\n            dst = os.path.join(IMG_DIR, f)\n            if not os.path.exists(dst):\n                shutil.copy2(os.path.join(src, f), dst)\n                count += 1\nprint(\"Copied images to\", IMG_DIR, \"count =\", count)\nprint(\"Masks will be saved to\", MASK_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T18:08:19.087342Z","iopub.execute_input":"2025-12-10T18:08:19.087553Z","iopub.status.idle":"2025-12-10T18:09:37.396829Z","shell.execute_reply.started":"2025-12-10T18:08:19.087536Z","shell.execute_reply":"2025-12-10T18:09:37.396040Z"}},"outputs":[{"name":"stdout","text":"lung_dirs: ['lung_aca', 'lung_n', 'lung_scc']\nCopied images to /kaggle/working/lung_work/images count = 15000\nMasks will be saved to /kaggle/working/lung_work/pseudo_masks\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# This function tries a pipeline:\n# 1) Convert to HSV - threshold on saturation/value with Otsu\n# 2) If fails (too small region), fallback to KMeans clustering (k=2) on RGB\n# 3) Postprocess: median blur, remove small objects, closing\n# 4) Save mask as uint8 (0/255)\n\ndef generate_mask_general(img_path, min_area_px=500):\n    img_bgr = cv2.imread(img_path)\n    if img_bgr is None:\n        raise ValueError(\"Cannot read image: \" + img_path)\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    h, w = img_rgb.shape[:2]\n\n    # 1) HSV + Otsu on Saturation or Value\n    hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n    s = hsv[:,:,1]\n    v = hsv[:,:,2]\n\n    # try Otsu on saturation\n    try:\n        thr_s = filters.threshold_otsu(s)\n        mask_s = (s > thr_s).astype(np.uint8)\n    except Exception:\n        mask_s = np.zeros((h,w), dtype=np.uint8)\n\n    # try Otsu on value\n    try:\n        thr_v = filters.threshold_otsu(v)\n        mask_v = (v < thr_v).astype(np.uint8)  # darker tissue often corresponds to lesion\n    except Exception:\n        mask_v = np.zeros((h,w), dtype=np.uint8)\n\n    # combine heuristics\n    mask = (mask_s | mask_v).astype(np.uint8)\n\n    # morphological cleanup\n    mask = morphology.remove_small_objects(mask.astype(bool), min_size=min_area_px).astype(np.uint8)\n    mask = morphology.binary_closing(mask, morphology.disk(5)).astype(np.uint8)\n    mask = mask.astype(np.uint8)\n\n    # if mask too small or nearly empty, fallback to kmeans on RGB colors\n    if mask.sum() < 0.01 * h * w:  # less than 1% pixels\n        flat = img_rgb.reshape(-1,3).astype(np.float32)\n        kmeans = KMeans(n_clusters=2, random_state=42).fit(flat)\n        labels = kmeans.labels_.reshape(h,w)\n        # choose cluster with smaller mean brightness as probable tissue region\n        cluster_means = [flat[kmeans.labels_==i].mean() if np.any(kmeans.labels_==i) else 255 for i in range(2)]\n        chosen = int(np.argmin(cluster_means))\n        mask = (labels == chosen).astype(np.uint8)\n        mask = morphology.remove_small_objects(mask.astype(bool), min_size=min_area_px).astype(np.uint8)\n        mask = morphology.binary_closing(mask, morphology.disk(5)).astype(np.uint8)\n        mask = mask.astype(np.uint8)\n\n    # final smoothing and convert to 0/255\n    mask_uint8 = (mask*255).astype(np.uint8)\n    mask_uint8 = cv2.medianBlur(mask_uint8, 5)\n    # ensure non-empty: if still empty, mark whole image (fallback)\n    if mask_uint8.sum() == 0:\n        mask_uint8[:] = 0  # keep empty (we don't want false positive), but you can set to 255 if you prefer\n    return mask_uint8\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T18:09:37.397643Z","iopub.execute_input":"2025-12-10T18:09:37.398002Z","iopub.status.idle":"2025-12-10T18:09:37.407082Z","shell.execute_reply.started":"2025-12-10T18:09:37.397974Z","shell.execute_reply":"2025-12-10T18:09:37.406345Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# iterate images and save masks\nimg_paths = sorted(glob.glob(os.path.join(IMG_DIR, \"*.*\")))\nprint(\"Total images to process:\", len(img_paths))\n\n# Optionally process only first N for quick test; set N=None to do all\nN = None\nif N is not None:\n    img_paths = img_paths[:N]\n\nfor i, p in enumerate(img_paths):\n    try:\n        mask = generate_mask_general(p, min_area_px=400)\n        out_name = os.path.basename(p)\n        cv2.imwrite(os.path.join(MASK_DIR, out_name), mask)\n    except Exception as e:\n        print(\"Error for\", p, \":\", str(e))\n    if (i+1) % 200 == 0:\n        print(\"Processed\", i+1, \"images\")\n\nprint(\"Saved masks to\", MASK_DIR, \" count:\", len(os.listdir(MASK_DIR)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T18:09:37.407961Z","iopub.execute_input":"2025-12-10T18:09:37.408165Z"}},"outputs":[{"name":"stdout","text":"Total images to process: 15000\nProcessed 200 images\nProcessed 400 images\nProcessed 600 images\nProcessed 800 images\nProcessed 1000 images\nProcessed 1200 images\nProcessed 1400 images\nProcessed 1600 images\nProcessed 1800 images\nProcessed 2000 images\nProcessed 2200 images\nProcessed 2400 images\nProcessed 2600 images\nProcessed 2800 images\nProcessed 3000 images\nProcessed 3200 images\nProcessed 3400 images\nProcessed 3600 images\nProcessed 3800 images\nProcessed 4000 images\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# display a few images and corresponding masks\nsample = img_paths[:6]\nplt.figure(figsize=(12,8))\nfor i, p in enumerate(sample):\n    img = Image.open(p).convert(\"RGB\")\n    mask_p = os.path.join(MASK_DIR, os.path.basename(p))\n    mask = Image.open(mask_p).convert(\"L\") if os.path.exists(mask_p) else Image.new('L', img.size)\n    plt.subplot(3,4,2*i+1)\n    plt.imshow(img); plt.title(\"Image\"); plt.axis('off')\n    plt.subplot(3,4,2*i+2)\n    plt.imshow(img); plt.imshow(mask, cmap='jet', alpha=0.45); plt.title(\"Overlay mask\"); plt.axis('off')\nplt.tight_layout()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build dataframe of (image, mask)\nrows=[]\nfor img_path in sorted(glob.glob(os.path.join(IMG_DIR, \"*.*\"))):\n    mask_path = os.path.join(MASK_DIR, os.path.basename(img_path))\n    if os.path.exists(mask_path):\n        rows.append({\"image\": img_path, \"mask\": mask_path})\ndf = pd.DataFrame(rows)\nprint(\"Total pairs:\", len(df))\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n\ntrain_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\nprint(\"Train:\", len(train_df), \"Val:\", len(val_df))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegDatasetTorch(Dataset):\n    def __init__(self, df, img_size=(256,256)):\n        self.df = df.reset_index(drop=True)\n        self.img_size = img_size\n        # using MONAI transforms for simplicity\n        self.transform_img = Compose([\n            LoadImage(image_only=True),\n            EnsureChannelFirst(),\n            Resize(spatial_size=self.img_size),\n            ScaleIntensity(),\n            ToTensor()\n        ])\n        self.transform_mask = Compose([\n            LoadImage(image_only=True),\n            EnsureChannelFirst(),\n            Resize(spatial_size=self.img_size),\n            ToTensor()\n        ])\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = self.transform_img(row.image)    # shape [C,H,W], float, scaled\n        mask = self.transform_mask(row.mask)  # [1,H,W]\n        # ensure mask is binary 0/1\n        mask = (mask > 0.5).float()\n        return img.float(), mask.float()\n\n# create dataloaders\nIMG_SIZE = (256,256)\ntrain_ds = SegDatasetTorch(train_df, img_size=IMG_SIZE)\nval_ds = SegDatasetTorch(val_df, img_size=IMG_SIZE)\ntrain_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\nprint(\"Dataloaders ready. Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build SwinUNETR with MONAI\n# in_channels = 3 for RGB, out_channels = 1 (binary mask)\nmodel = SwinUNETR(\n    img_size=IMG_SIZE,\n    in_channels=3,\n    out_channels=1,\n    feature_size=48,   # reduce feature_size if GPU memory low\n    drop_rate=0.0,\n).to(device)\n\n# losses and optimizer\nbce_loss = nn.BCEWithLogitsLoss()\ndef dice_loss_logits(logits, target, smooth=1e-6):\n    probs = torch.sigmoid(logits)\n    inter = (probs * target).sum(dim=(2,3))\n    denom = probs.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    loss = 1 - ((2*inter + smooth) / (denom + smooth))\n    return loss.mean()\n\noptimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\nprint(\"Model params (millions):\", sum(p.numel() for p in model.parameters())/1e6)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 12\nbest_val_dice = 0.0\nsave_path = \"/kaggle/working/swin_unetr_best.pth\"\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    train_loss = 0.0\n    train_batches = 0\n    for imgs, masks in train_loader:\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        preds = model(imgs)  # logits [B,1,H,W]\n        if preds.shape != masks.shape:\n            preds = F.interpolate(preds, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n        loss_bce = bce_loss(preds, masks)\n        loss_dice = dice_loss_logits(preds, masks)\n        loss = loss_bce + loss_dice\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        train_batches += 1\n    train_loss = train_loss / max(1, train_batches)\n\n    # validation\n    model.eval()\n    val_losses = 0.0\n    val_batches = 0\n    dice_scores = []\n    with torch.no_grad():\n        for imgs, masks in val_loader:\n            imgs = imgs.to(device)\n            masks = masks.to(device)\n            preds = model(imgs)\n            if preds.shape != masks.shape:\n                preds = F.interpolate(preds, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n            loss_bce = bce_loss(preds, masks)\n            loss_dice = dice_loss_logits(preds, masks)\n            val_losses += (loss_bce.item() + loss_dice.item())\n            val_batches += 1\n            # compute dice per batch\n            probs = torch.sigmoid(preds)\n            pred_bin = (probs > 0.5).float()\n            inter = (pred_bin * masks).sum(dim=(2,3))\n            denom = pred_bin.sum(dim=(2,3)) + masks.sum(dim=(2,3)) + 1e-6\n            dice_batch = (2*inter / denom).mean().item()\n            dice_scores.append(dice_batch)\n    val_loss = val_losses / max(1, val_batches)\n    val_dice = np.mean(dice_scores) if dice_scores else 0.0\n\n    print(f\"Epoch {epoch}/{EPOCHS} TrainLoss:{train_loss:.4f} ValLoss:{val_loss:.4f} ValDice:{val_dice:.4f}\")\n\n    # save best\n    if val_dice > best_val_dice:\n        best_val_dice = val_dice\n        torch.save(model.state_dict(), save_path)\n        print(\"Saved best model with ValDice:\", best_val_dice)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load best model (if saved) and show some predictions\nif os.path.exists(\"/kaggle/working/swin_unetr_best.pth\"):\n    model.load_state_dict(torch.load(\"/kaggle/working/swin_unetr_best.pth\", map_location=device))\nmodel.eval()\n\n# get first validation batch\nimgs, masks = next(iter(val_loader))\nimgs = imgs.to(device)\nwith torch.no_grad():\n    preds = model(imgs)\n    probs = torch.sigmoid(preds).cpu().numpy()\nimgs = imgs.cpu().numpy()\nmasks = masks.cpu().numpy()\n\nn_show = min(6, imgs.shape[0])\nplt.figure(figsize=(12,4*n_show//3 + 2))\nfor i in range(n_show):\n    img = np.transpose(imgs[i], (1,2,0))\n    # undo intensity scaling (ScaleIntensity scales to 0-1 approx)\n    img = np.clip(img, 0, 1)\n    pred_mask = probs[i,0]\n    gt_mask = masks[i,0]\n    plt.subplot(n_show,3,3*i+1); plt.imshow(img); plt.axis('off'); plt.title('Image')\n    plt.subplot(n_show,3,3*i+2); plt.imshow(img); plt.imshow(pred_mask, alpha=0.45); plt.axis('off'); plt.title('Pred overlay')\n    plt.subplot(n_show,3,3*i+3); plt.imshow(gt_mask, cmap='gray'); plt.axis('off'); plt.title('Pseudo GT')\nplt.tight_layout()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Best model saved at:\", \"/kaggle/working/swin_unetr_best.pth\")\n\ndef predict_on_image(model, img_path, out_size=(256,256), thr=0.5):\n    model.eval()\n    img = np.array(Image.open(img_path).convert(\"RGB\"))\n    # resize\n    img_res = cv2.resize(img, out_size[::-1])  # (W,H)\n    img_norm = img_res.astype(np.float32) / 255.0\n    # MONAI transforms: channel first, tensor\n    inp = torch.tensor(img_norm.transpose(2,0,1)[None]).float().to(device)\n    with torch.no_grad():\n        logits = model(inp)\n        probs = torch.sigmoid(logits)[0,0].cpu().numpy()\n    mask = (probs > thr).astype(np.uint8) * 255\n    return img_res, probs, mask\n\n# Example usage:\n# img_res, probs, mask = predict_on_image(model, train_df.iloc[0]['image'])\n# plt.imshow(img_res); plt.imshow(mask, alpha=0.4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}