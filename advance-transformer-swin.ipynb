{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11351640,"sourceType":"datasetVersion","datasetId":7103234}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:48:43.965478Z","iopub.execute_input":"2025-12-09T16:48:43.966020Z","iopub.status.idle":"2025-12-09T16:48:46.047610Z","shell.execute_reply.started":"2025-12-09T16:48:43.965993Z","shell.execute_reply":"2025-12-09T16:48:46.046746Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# run if timm not present\n!pip install timm --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:48:46.049327Z","iopub.execute_input":"2025-12-09T16:48:46.049663Z","iopub.status.idle":"2025-12-09T16:49:55.272050Z","shell.execute_reply.started":"2025-12-09T16:48:46.049643Z","shell.execute_reply":"2025-12-09T16:49:55.271218Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport random\nimport math\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport timm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc, precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# CONFIG - edit paths / hyperparams\nDATA_DIR = \"/kaggle/input/breakhis-400x/train\"   # change if needed\nOUT_DIR = Path(\"./outputs\")\nOUT_DIR.mkdir(exist_ok=True)\nIMG_SIZE = 224\nBATCH_SIZE = 8\nEPOCHS = 12\nLR = 1e-4\nRANDOM_SEED = 42\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Patch extraction / selection settings\nPATCH_SCALES = [1/4, 1/8]   # fractions of image side -> coarse and fine (e.g., 1/4 -> 56x56 for 224)\nTOP_K_PATCHES = 16          # number of patches selected per image\nPATCH_RESIZE = 128          # each patch will be resized to this before feeding backbone (to balance detail)\n\n# Progressive freezing schedule (in epochs)\nFREEZE_STAGE_1 = 2  # freeze backbone, train GRU+classifier\nFREEZE_STAGE_2 = 4  # unfreeze backbone last layer\n# Stage 3: unfreeze all (remaining epochs)\n\n# Mixup / CutMix\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\nMIXPROB = 0.5  # probability to apply mix augmentation\n\nSEED = RANDOM_SEED\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:49:55.273020Z","iopub.execute_input":"2025-12-09T16:49:55.273321Z","iopub.status.idle":"2025-12-09T16:50:06.598782Z","shell.execute_reply.started":"2025-12-09T16:49:55.273271Z","shell.execute_reply":"2025-12-09T16:50:06.598003Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c142bcf4830>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"VALID_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'}\n\ndef safe_load_image(p, size=IMG_SIZE):\n    try:\n        img = Image.open(p).convert(\"RGB\")\n        img = img.resize((size, size), Image.BILINEAR)\n        arr = np.array(img)\n        if arr.dtype != np.uint8:\n            arr = arr.astype(np.uint8)\n        if arr.ndim != 3 or arr.shape[2] != 3:\n            arr = np.zeros((size, size, 3), dtype=np.uint8)\n        return arr\n    except Exception:\n        return np.zeros((size, size, 3), dtype=np.uint8)\n\n# Build file list and classes (assumes structure DATA_DIR/class_x/images)\ndata_root = Path(DATA_DIR)\nassert data_root.exists(), f\"{DATA_DIR} not found\"\nclasses = [d.name for d in sorted(data_root.iterdir()) if d.is_dir()]\nprint(\"Classes detected:\", classes)\nfilepaths = []\nlabels = []\nfor i, cls in enumerate(classes):\n    p = data_root/cls\n    for f in sorted(p.rglob(\"*\")):\n        if f.suffix.lower() in VALID_EXTS and f.is_file():\n            filepaths.append(str(f))\n            labels.append(i)\nprint(\"Total samples:\", len(filepaths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:06.599686Z","iopub.execute_input":"2025-12-09T16:50:06.600104Z","iopub.status.idle":"2025-12-09T16:50:07.230569Z","shell.execute_reply.started":"2025-12-09T16:50:06.600082Z","shell.execute_reply":"2025-12-09T16:50:07.229829Z"}},"outputs":[{"name":"stdout","text":"Classes detected: ['benign', 'malignant']\nTotal samples: 1184\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n    filepaths, labels, test_size=0.2, random_state=SEED, stratify=labels\n)\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    train_val_paths, train_val_labels, test_size=0.125, random_state=SEED, stratify=train_val_labels\n)  # ~10% val of full\n\nprint(\"train\", len(train_paths), \"val\", len(val_paths), \"test\", len(test_paths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:07.232135Z","iopub.execute_input":"2025-12-09T16:50:07.232450Z","iopub.status.idle":"2025-12-09T16:50:07.244959Z","shell.execute_reply.started":"2025-12-09T16:50:07.232429Z","shell.execute_reply":"2025-12-09T16:50:07.244250Z"}},"outputs":[{"name":"stdout","text":"train 828 val 119 test 237\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef mixup_data(x, y, alpha=MIXUP_ALPHA):\n    if alpha <= 0:\n        return x, y, 1.0, None\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef cutmix_data(x, y, alpha=CUTMIX_ALPHA):\n    if alpha <= 0:\n        return x, y, 1.0, None\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(x.device)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n    # adjust lambda according to pixel ratio\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n    y_a, y_b = y, y[index]\n    return x, y_a, y_b, lam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:07.245640Z","iopub.execute_input":"2025-12-09T16:50:07.245919Z","iopub.status.idle":"2025-12-09T16:50:07.262319Z","shell.execute_reply.started":"2025-12-09T16:50:07.245900Z","shell.execute_reply":"2025-12-09T16:50:07.261727Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class BalancedFocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, reduction='mean', eps=1e-9):\n        super().__init__()\n        self.gamma = gamma\n        self.reduction = reduction\n        self.eps = eps\n        self.ce = nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, logits, targets, class_weights=None):\n        \"\"\"\n        logits: [B, C], targets: [B]\n        class_weights: tensor of shape [C] or None\n        \"\"\"\n        logp = F.log_softmax(logits, dim=1)\n        p = torch.exp(logp)\n        ce_loss = F.nll_loss(logp, targets, reduction='none', weight=class_weights)\n        pt = p.gather(1, targets.unsqueeze(1)).squeeze(1)  # p_t\n        focal_factor = (1 - pt) ** self.gamma\n        loss = focal_factor * ce_loss\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:07.263213Z","iopub.execute_input":"2025-12-09T16:50:07.263439Z","iopub.status.idle":"2025-12-09T16:50:07.281395Z","shell.execute_reply.started":"2025-12-09T16:50:07.263423Z","shell.execute_reply":"2025-12-09T16:50:07.280671Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# transforms for raw input (basic augmentations)\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\neval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\n\nclass ImageFileDataset(Dataset):\n    def __init__(self, paths, labels, transform=None):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        arr = safe_load_image(p, size=IMG_SIZE)\n        img = Image.fromarray(arr)\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, label\n\ntrain_ds = ImageFileDataset(train_paths, train_labels, transform=train_transform)\nval_ds = ImageFileDataset(val_paths, val_labels, transform=eval_transform)\ntest_ds = ImageFileDataset(test_paths, test_labels, transform=eval_transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:07.282701Z","iopub.execute_input":"2025-12-09T16:50:07.282930Z","iopub.status.idle":"2025-12-09T16:50:07.299122Z","shell.execute_reply.started":"2025-12-09T16:50:07.282912Z","shell.execute_reply":"2025-12-09T16:50:07.298545Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class HybridSwinGRU(nn.Module):\n    def __init__(self, swin_name='swin_tiny_patch4_window7_224', pretrained=True, \n                 patch_scales=PATCH_SCALES, top_k=TOP_K_PATCHES, patch_resize=PATCH_RESIZE,\n                 gru_hidden=512, gru_layers=1, num_classes=2, bidirectional=False, dropout=0.3):\n        super().__init__()\n        self.patch_scales = patch_scales\n        self.top_k = top_k\n        self.patch_resize = patch_resize\n\n        # backbone for patches & global image (use same weights)\n        # use features_only=False but create a model exposing forward_features if available\n        self.backbone = timm.create_model(swin_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n        # some timm models expose forward_features; otherwise we call backbone(x) which gives pooled features\n        # get feature dim by a dummy pass later\n        # GRU and classifier will be initialized after we detect feat_dim\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n        self.gru_hidden = gru_hidden\n        self.gru_layers = gru_layers\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n\n        # We'll create GRU & classifier lazily once we know the feature dim\n        self.feat_dim = None\n        self.gru = None\n        self.classifier = None\n\n        # Hybrid attention fusion: small MLP that takes [global_feat || gru_feat] -> gate\n        self.fusion_gate = None\n\n    def _extract_patches(self, imgs):\n        \"\"\"\n        imgs: tensor [B, C, H, W] in range [0,1] normalized already\n        returns: list of lists of patch tensors per image:\n            patches_tensor: [B, num_patches_total, C, patchH, patchW]\n        We'll produce patches per scale, non-overlapping grid, resize each patch to patch_resize.\n        \"\"\"\n        B, C, H, W = imgs.shape\n        device = imgs.device\n        all_patches = []\n        for b in range(B):\n            img = imgs[b]  # C,H,W\n            patches_for_image = []\n            # convert to PIL for cropping easier (or use tensor ops). We'll use tensor ops for speed.\n            # convert to [H,W,C] numpy for simple crop: but that would move to CPU.\n            # Instead do tensor cropping.\n            for scale in self.patch_scales:\n                side = int(H * scale)\n                if side < 8:\n                    continue\n                # number of patches per side\n                num = max(1, H // side)\n                stride = side\n                for y in range(0, H - side + 1, stride):\n                    for x in range(0, W - side + 1, stride):\n                        p = img[:, y:y+side, x:x+side]  # C,side,side\n                        # resize patch to patch_resize\n                        p_resized = F.interpolate(p.unsqueeze(0), size=(self.patch_resize, self.patch_resize), mode='bilinear', align_corners=False).squeeze(0)\n                        patches_for_image.append(p_resized)\n            if len(patches_for_image) == 0:\n                # fallback: whole image resized\n                p_resized = F.interpolate(img.unsqueeze(0), size=(self.patch_resize, self.patch_resize), mode='bilinear', align_corners=False).squeeze(0)\n                patches_for_image.append(p_resized)\n            all_patches.append(torch.stack(patches_for_image, dim=0))  # [num_patches, C, patch_resize, patch_resize]\n        # pad to same count across batch: convert to list of tensors (variable lengths)\n        return all_patches  # list length B of tensors [Ni, C, ph, pw]\n\n    @staticmethod\n    def _score_patches(patches_tensor):\n        \"\"\"\n        simple importance score per patch: per-patch variance over RGB channels (higher->more informative)\n        patches_tensor: [N, C, h, w] (cpu or device)\n        returns: scores numpy array shape [N]\n        \"\"\"\n        # compute variance per patch (on CPU or GPU)\n        with torch.no_grad():\n            # flatten spatial and channel dims\n            vals = patches_tensor.view(patches_tensor.size(0), -1)\n            var = vals.var(dim=1)  # variance\n            # also consider entropy like measure: approximate by normalized histogram? skip for performance\n            return var  # tensor N\n\n    def _ensure_feat_dim(self, device):\n        if self.feat_dim is None:\n            self.backbone.eval()\n            dummy = torch.zeros(1, 3, self.patch_resize, self.patch_resize).to(device)\n            with torch.no_grad():\n                feat = self.backbone(dummy)  # returns pooled features [1, feat_dim]\n            if feat.dim() == 4:\n                # some backbones may return feature map, pool it\n                feat = feat.mean(dim=[2,3])\n            self.feat_dim = feat.shape[1]\n            # now create GRU and classifier with correct dims\n            self.gru = nn.GRU(input_size=self.feat_dim, hidden_size=self.gru_hidden, num_layers=self.gru_layers,\n                              batch_first=True, bidirectional=self.bidirectional).to(device)\n            self.classifier = nn.Sequential(\n                nn.Linear(self.gru_hidden * self.num_directions, self.gru_hidden // 2),\n                nn.GELU(),\n                nn.Dropout(self.dropout.p if isinstance(self.dropout, nn.Dropout) else 0.3),\n                nn.Linear(self.gru_hidden // 2, len(classes))\n            ).to(device)\n            # fusion gate MLP: takes concat [global_feat, gru_feat] -> sigmoid gate scalar (or vector)\n            fusion_in = self.feat_dim + (self.gru_hidden * self.num_directions)\n            self.fusion_gate = nn.Sequential(\n                nn.Linear(fusion_in, fusion_in // 2),\n                nn.GELU(),\n                nn.Linear(fusion_in // 2, self.num_directions * self.gru_hidden if False else 1),\n                nn.Sigmoid()\n            ).to(device)\n\n    def forward(self, x):\n        \"\"\"\n        x: [B, C, H, W], normalized tensor\n        returns logits [B, num_classes]\n        \"\"\"\n        B = x.size(0)\n        device = x.device\n        # ensure feature dim structures created\n        self._ensure_feat_dim(device)\n\n        # 1) global feature: use backbone pooled features on whole image\n        # For timm create_model(..., num_classes=0, global_pool='avg') returns pooled features when calling model(x)\n        global_feat = self.backbone(x)  # [B, feat_dim] (pooled)\n        if global_feat.dim() == 4:\n            global_feat = global_feat.mean(dim=[2,3])  # pool\n\n        # 2) multi-scale patches extraction\n        all_patches = self._extract_patches(x)  # list B of [Ni, C, ph, pw]\n\n        # 3) for each image: score patches and select top_k, then run backbone on selected patches to get embeddings\n        seq_embeddings = []\n        for b in range(B):\n            patches_b = all_patches[b]  # [Ni, C, ph, pw]\n            scores = self._score_patches(patches_b)  # tensor [Ni]\n            # pick top_k indices\n            topk = min(self.top_k, patches_b.size(0))\n            vals, idxs = torch.topk(scores, k=topk, largest=True)\n            selected = patches_b[idxs]  # [topk, C, ph, pw]\n            # pass selected patches through backbone to get embeddings\n            # process in batch\n            with torch.no_grad():\n                emb = self.backbone(selected.to(device))  # [topk, feat_dim] or maybe pooled output\n            if emb.dim() == 4:\n                emb = emb.mean(dim=[2,3])\n            seq_embeddings.append(emb)  # list of [topk, feat_dim]\n\n        # pad sequence to same length top_k if variable; stack to [B, T, feat_dim]\n        T = max([s.size(0) for s in seq_embeddings])\n        feat_dim = self.feat_dim\n        seq_tensor = torch.zeros(B, T, feat_dim, device=device)\n        for b in range(B):\n            s = seq_embeddings[b]\n            seq_tensor[b, :s.size(0), :] = s\n\n        # 4) GRU over sequence of patch embeddings\n        gru_out, h_n = self.gru(seq_tensor)  # gru_out [B, T, hidden*dir]\n        # aggregate: take last time-step output\n        last_out = gru_out[:, -1, :]  # [B, hidden*dir]\n\n        # 5) attention fusion w/ global feature\n        # concat\n        concat = torch.cat([global_feat, last_out], dim=1)  # [B, feat_dim + hidden*dir]\n        gate = self.fusion_gate(concat)  # [B, 1] or [B, hidden]\n        if gate.size(1) == 1:\n            fused = gate * last_out + (1 - gate) * global_feat[:, :last_out.size(1)]\n        else:\n            fused = gate * last_out + (1 - gate) * global_feat\n\n        fused = self.dropout(fused)\n        logits = self.classifier(fused)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:07.299942Z","iopub.execute_input":"2025-12-09T16:50:07.300252Z","iopub.status.idle":"2025-12-09T16:50:07.537237Z","shell.execute_reply.started":"2025-12-09T16:50:07.300234Z","shell.execute_reply":"2025-12-09T16:50:07.536523Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = HybridSwinGRU(swin_name='swin_tiny_patch4_window7_224', pretrained=True,\n                      patch_scales=PATCH_SCALES, top_k=TOP_K_PATCHES, patch_resize=PATCH_RESIZE,\n                      gru_hidden=512, gru_layers=1, num_classes=len(classes),\n                      bidirectional=False, dropout=0.3).to(DEVICE)\n\n# compute class weights inverse frequency for balanced focal\nfrom collections import Counter\ncnt = Counter(train_labels)\nfreq = np.array([cnt[i] for i in range(len(classes))], dtype=float)\nclass_weights = torch.tensor((freq.sum() / (freq + 1e-8)), dtype=torch.float32).to(DEVICE)  # inverse freq\nclass_weights = class_weights / class_weights.sum()  # normalized\n\ncriterion = BalancedFocalLoss(gamma=2.0).to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:07.537945Z","iopub.execute_input":"2025-12-09T16:50:07.538124Z","iopub.status.idle":"2025-12-09T16:50:10.028146Z","shell.execute_reply.started":"2025-12-09T16:50:07.538110Z","shell.execute_reply":"2025-12-09T16:50:10.027345Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ce51fc3adf4b588818c1a366570fc2"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def apply_mix_augment(x, y):\n    # randomly choose mixup or cutmix or none\n    r = random.random()\n    if r < MIXPROB/2:\n        # mixup\n        mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=MIXUP_ALPHA)\n        return mixed_x, (y_a, y_b, lam), 'mixup'\n    elif r < MIXPROB:\n        # cutmix\n        mixed_x, y_a, y_b, lam = cutmix_data(x.clone(), y, alpha=CUTMIX_ALPHA)\n        return mixed_x, (y_a, y_b, lam), 'cutmix'\n    else:\n        return x, (y, None, 1.0), 'none'\n\ndef compute_loss(logits, y_info, criterion, class_weights=None):\n    # y_info: (y_a, y_b, lam) or (y, None, 1)\n    y_a, y_b, lam = y_info\n    if y_b is None:\n        return criterion(logits, y_a, class_weights)\n    else:\n        # combine focal losses: lam*loss(a) + (1-lam)*loss(b)\n        loss_a = criterion(logits, y_a, class_weights)\n        loss_b = criterion(logits, y_b, class_weights)\n        return lam * loss_a + (1 - lam) * loss_b\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:10.029019Z","iopub.execute_input":"2025-12-09T16:50:10.029248Z","iopub.status.idle":"2025-12-09T16:50:10.035023Z","shell.execute_reply.started":"2025-12-09T16:50:10.029230Z","shell.execute_reply":"2025-12-09T16:50:10.034342Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def set_requires_grad(module, flag):\n    for p in module.parameters():\n        p.requires_grad = flag\n\n# freeze schedule:\n# Stage 1: freeze backbone entirely for first FREEZE_STAGE_1 epochs\n# Stage 2: unfreeze backbone last block (if possible) for next (FREEZE_STAGE_2 - FREEZE_STAGE_1) epochs\n# Stage 3: unfreeze all remaining epochs\n\n# Helper to unfreeze last layer blocks: for timm swin, try to unfreeze layers in model if named\ndef unfreeze_backbone_last(module):\n    # try to unfreeze topmost parameters by name heuristics\n    for name, param in module.named_parameters():\n        if any(x in name for x in ['layers', 'block', 'stages', 'layer4', 'norm', 'head']):\n            param.requires_grad = True\n        else:\n            param.requires_grad = False\n\n# Stage 1\nset_requires_grad(model.backbone, False)\nset_requires_grad(model.gru, True)\nset_requires_grad(model.classifier, True)\nset_requires_grad(model.fusion_gate, True)\n\nhistory = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'val_f1':[]}\nbest_val_f1 = -1.0\nbest_path = OUT_DIR/\"best_hybrid_model.pth\"\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    # progressive unfreeze logic\n    if epoch == FREEZE_STAGE_1 + 1:\n        print(\"Stage 2: Partially unfreezing backbone last blocks\")\n        # try to unfreeze last stage (heuristic)\n        try:\n            unfreeze_backbone_last(model.backbone)\n        except:\n            set_requires_grad(model.backbone, True)  # fallback\n    if epoch == FREEZE_STAGE_2 + 1:\n        print(\"Stage 3: Unfreezing entire backbone for fine-tuning\")\n        set_requires_grad(model.backbone, True)\n\n    running_loss = 0.0\n    preds_all = []\n    targets_all = []\n    for imgs, labels in tqdm(train_loader, leave=False):\n        imgs = imgs.to(DEVICE)\n        labels_t = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n\n        # apply mix augment\n        imgs_aug, y_info, aug_type = apply_mix_augment(imgs, labels_t)\n\n        optimizer.zero_grad()\n        logits = model(imgs_aug)  # model handles patch extraction -> GRU -> fusion\n        loss = compute_loss(logits, y_info, criterion, class_weights)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        preds_all.extend(torch.argmax(logits, dim=1).detach().cpu().numpy().tolist())\n        targets_all.extend(labels_t.detach().cpu().numpy().tolist())\n\n    train_loss = running_loss / len(train_loader)\n    train_acc = accuracy_score(targets_all, preds_all)\n    history['train_loss'].append(train_loss)\n    history['train_acc'] = history.get('train_acc', []) + [train_acc]\n\n    # validation\n    model.eval()\n    v_loss = 0.0\n    v_preds = []\n    v_targets = []\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs = imgs.to(DEVICE)\n            labels_t = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n            logits = model(imgs)\n            loss = criterion(logits, labels_t, class_weights)\n            v_loss += loss.item()\n            v_preds.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n            v_targets.extend(labels_t.cpu().numpy().tolist())\n\n    val_loss = v_loss / len(val_loader)\n    val_acc = accuracy_score(v_targets, v_preds)\n    val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(v_targets, v_preds, average='weighted', zero_division=0)\n\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['val_f1'].append(val_f1)\n\n    print(f\"Epoch {epoch}/{EPOCHS} TrainLoss {train_loss:.4f} TrainAcc {train_acc:.4f} ValLoss {val_loss:.4f} ValAcc {val_acc:.4f} ValF1 {val_f1:.4f}\")\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), best_path)\n        print(\"Saved best model:\", best_path)\n\n    scheduler.step()\nprint(\"Training complete. Best val f1:\", best_val_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:10.035843Z","iopub.execute_input":"2025-12-09T16:50:10.036574Z","iopub.status.idle":"2025-12-09T16:50:10.137852Z","shell.execute_reply.started":"2025-12-09T16:50:10.036543Z","shell.execute_reply":"2025-12-09T16:50:10.136824Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2466663655.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Stage 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mset_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mset_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mset_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mset_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion_gate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/2466663655.py\u001b[0m in \u001b[0;36mset_requires_grad\u001b[0;34m(module, flag)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# freeze schedule:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'parameters'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'parameters'","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# load best model\nmodel.load_state_dict(torch.load(best_path, map_location=DEVICE))\nmodel.eval()\n\ny_true = []\ny_pred = []\ny_proba = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(test_loader, leave=False):\n        imgs = imgs.to(DEVICE)\n        labels_t = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n        logits = model(imgs)\n        probs = F.softmax(logits, dim=1)[:,1].cpu().numpy()  # prob of class 1\n        preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n        y_true.extend(labels)\n        y_pred.extend(preds)\n        y_proba.extend(probs.tolist())\n\nacc = accuracy_score(y_true, y_pred)\nprec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Test Acc:\", acc, \"F1:\", f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:10.138250Z","iopub.status.idle":"2025-12-09T16:50:10.138500Z","shell.execute_reply.started":"2025-12-09T16:50:10.138392Z","shell.execute_reply":"2025-12-09T16:50:10.138403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# History arrays\nepochs_range = list(range(1, len(history['train_loss'])+1))\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs_range, history['train_loss'], marker='o', label='train_loss')\nplt.plot(epochs_range, history['val_loss'], marker='o', label='val_loss')\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"loss_curve.png\", dpi=300); plt.show()\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs_range, history['train_acc'], marker='o', label='train_acc')\nplt.plot(epochs_range, history['val_acc'], marker='o', label='val_acc')\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"acc_curve.png\", dpi=300); plt.show()\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs_range, history['val_f1'], marker='o', label='val_f1')\nplt.xlabel(\"Epoch\"); plt.ylabel(\"F1\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"f1_curve.png\", dpi=300); plt.show()\n\n# Confusion matrix\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\nplt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.title(\"Confusion Matrix\")\nplt.savefig(OUT_DIR/\"confusion_matrix.png\", dpi=300); plt.show()\n\n# ROC + PR\nfpr, tpr, _ = roc_curve(y_true, y_proba)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(6,5)); plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.4f}\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"roc_curve.png\", dpi=300); plt.show()\n\nprec_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba)\nplt.figure(figsize=(6,5)); plt.plot(recall_vals, prec_vals); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.grid(True)\nplt.savefig(OUT_DIR/\"pr_curve.png\", dpi=300); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:10.139562Z","iopub.status.idle":"2025-12-09T16:50:10.139784Z","shell.execute_reply.started":"2025-12-09T16:50:10.139685Z","shell.execute_reply":"2025-12-09T16:50:10.139695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# collect embeddings from backbone global features for test set\nembs = []\nlabels_list = []\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        imgs = imgs.to(DEVICE)\n        # get global feature (pooled) via backbone call inside our model: call model.backbone\n        feat = model.backbone(imgs)  # pooled\n        if feat.dim() == 4:\n            feat = feat.mean(dim=[2,3])\n        embs.append(feat.cpu().numpy())\n        labels_list.extend(labels)\nembs = np.concatenate(embs, axis=0)\ntsne = TSNE(n_components=2, random_state=SEED)\ntsne_feats = tsne.fit_transform(embs)\n\nplt.figure(figsize=(8,6))\nplt.scatter(tsne_feats[:,0], tsne_feats[:,1], c=labels_list, cmap='coolwarm', s=6)\nplt.colorbar(ticks=range(len(classes))); plt.title(\"t-SNE of global features\")\nplt.savefig(OUT_DIR/\"tsne.png\", dpi=300); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:10.141192Z","iopub.status.idle":"2025-12-09T16:50:10.141436Z","shell.execute_reply.started":"2025-12-09T16:50:10.141327Z","shell.execute_reply":"2025-12-09T16:50:10.141341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def grad_cam_on_image(model, img_tensor, class_idx=1):\n    model.eval()\n    # hook to capture features and gradients from backbone (attempt to access feature maps)\n    gradients = []\n    activations = []\n    def forward_hook(module, inp, out):\n        activations.append(out.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    # try to register hook on backbone's layer before pooling if possible\n    # heuristics: try layer named \"layers\" or last conv layer\n    target_module = None\n    for name, m in model.backbone.named_modules():\n        if any(k in name for k in ['layer4', 'stages', 'layers', 'blocks']) :\n            target_module = m\n    if target_module is None:\n        target_module = list(model.backbone.children())[-1]\n\n    h_f = target_module.register_forward_hook(forward_hook)\n    h_b = target_module.register_backward_hook(backward_hook)\n\n    img = img_tensor.unsqueeze(0).to(DEVICE)\n    img.requires_grad = True\n    logits = model(img)\n    pred_class = logits.argmax(dim=1).item()\n    loss = logits[0, class_idx] if class_idx is not None else logits[0, pred_class]\n    model.zero_grad()\n    loss.backward(retain_graph=True)\n\n    # get gradients and activations\n    if len(gradients) == 0 or len(activations) == 0:\n        print(\"Could not get activations/gradients from backbone for Grad-CAM\")\n        h_f.remove(); h_b.remove()\n        return None\n\n    grads = gradients[-1][0]  # [C, H, W]\n    acts = activations[-1][0]  # [C, H, W]\n    weights = grads.mean(dim=(1,2))  # [C]\n    cam = (weights.view(-1,1,1) * acts).sum(dim=0).cpu().numpy()\n    cam = np.maximum(cam, 0)\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    h_f.remove(); h_b.remove()\n    return cam, pred_class\n\n# Example usage:\nsample_img_path = test_paths[0]\narr = safe_load_image(sample_img_path, size=IMG_SIZE)\npil = transforms.ToTensor()(Image.fromarray(arr))\nresult = grad_cam_on_image(model, pil, class_idx=None)\nif result is not None:\n    cam, pred = result\n    plt.imshow(arr); plt.imshow(cam, cmap='jet', alpha=0.4); plt.title(f\"Pred {pred}\"); plt.axis('off')\n    plt.savefig(OUT_DIR/\"gradcam_example.png\", dpi=300); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T16:50:10.142623Z","iopub.status.idle":"2025-12-09T16:50:10.143093Z","shell.execute_reply.started":"2025-12-09T16:50:10.142970Z","shell.execute_reply":"2025-12-09T16:50:10.142987Z"}},"outputs":[],"execution_count":null}]}