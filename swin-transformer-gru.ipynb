{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11351640,"sourceType":"datasetVersion","datasetId":7103234}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amimulahasanrofik/swin-transformer-gru?scriptVersionId=284867524\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T06:59:54.945543Z","iopub.execute_input":"2025-12-09T06:59:54.946149Z","iopub.status.idle":"2025-12-09T06:59:55.540665Z","shell.execute_reply.started":"2025-12-09T06:59:54.946122Z","shell.execute_reply":"2025-12-09T06:59:55.540074Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# run this cell if timm not preinstalled on Kaggle\n!pip install timm --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T06:59:55.541925Z","iopub.execute_input":"2025-12-09T06:59:55.542135Z","iopub.status.idle":"2025-12-09T06:59:58.820672Z","shell.execute_reply.started":"2025-12-09T06:59:55.54212Z","shell.execute_reply":"2025-12-09T06:59:58.819743Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Cell 2\nimport os\nfrom pathlib import Path\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n\n# try timm\ntry:\n    import timm\n    TIMM_AVAILABLE = True\nexcept Exception:\n    TIMM_AVAILABLE = False\n\n# ==== CONFIG: change DATA_DIR to the exact Kaggle input folder name if needed ====\nDATA_DIR = \"/kaggle/input/breakhis-400x/train\"   # <- replace if your path differs\nOUT_DIR = \"/kaggle/working/outputs\"\nIMG_SIZE = 224       # Swin standard\nRANDOM_SEED = 42\n\nBATCH_SIZE = 16\nEPOCHS = 8\nLR = 1e-4\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nos.makedirs(OUT_DIR, exist_ok=True)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\nprint(\"Device:\", DEVICE, \"TIMM available:\", TIMM_AVAILABLE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T06:59:58.8218Z","iopub.execute_input":"2025-12-09T06:59:58.822075Z","iopub.status.idle":"2025-12-09T06:59:58.829929Z","shell.execute_reply.started":"2025-12-09T06:59:58.822054Z","shell.execute_reply":"2025-12-09T06:59:58.829142Z"}},"outputs":[{"name":"stdout","text":"Device: cuda TIMM available: True\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Cell 3\nVALID_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'}\n\ndef safe_load_image(path, size=IMG_SIZE):\n    \"\"\"\n    Load image reliably and return uint8 numpy array shape (H,W,3)\n    If load fails, returns a black image.\n    \"\"\"\n    try:\n        img = Image.open(path).convert(\"RGB\")\n        img = img.resize((size, size), Image.BILINEAR)\n        arr = np.array(img)\n        if arr.dtype != np.uint8:\n            arr = (arr).astype(np.uint8)\n        # ensure shape H,W,3\n        if arr.ndim != 3 or arr.shape[2] != 3:\n            arr = np.zeros((size, size, 3), dtype=np.uint8)\n        return arr\n    except Exception as e:\n        # fallback: black image\n        return np.zeros((size, size, 3), dtype=np.uint8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T06:59:58.830717Z","iopub.execute_input":"2025-12-09T06:59:58.830923Z","iopub.status.idle":"2025-12-09T06:59:58.846295Z","shell.execute_reply.started":"2025-12-09T06:59:58.830909Z","shell.execute_reply":"2025-12-09T06:59:58.845627Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Cell 4\ndata_root = Path(DATA_DIR)\nassert data_root.exists(), f\"DATA_DIR {DATA_DIR} not found. Check Kaggle input folder name.\"\n\n# Detect class folders automatically (BreakHis usually has subfolders per class or subtype).\nclasses = [d.name for d in sorted(data_root.iterdir()) if d.is_dir()]\n# If the dataset contains nested structure (e.g., magnification subfolders), try to find image folders\nif len(classes) == 0:\n    raise RuntimeError(f\"No class folders found inside {DATA_DIR}\")\n\nprint(\"Detected class folders (top-level):\", classes)\n\nimages = []\nlabels = []\n\n# We try two common patterns:\n# 1) top-level class folders containing image files\n# 2) top-level folders containing subfolders (e.g., magnification or patient) that contain images\nfor cls_idx, cls in enumerate(classes):\n    cls_path = data_root / cls\n    # if this folder contains image files directly\n    files = [p for p in sorted(cls_path.iterdir()) if p.is_file() and p.suffix.lower() in VALID_EXTS]\n    if len(files) > 0:\n        for p in tqdm(files, desc=f\"Loading images in {cls}\"):\n            arr = safe_load_image(p, size=IMG_SIZE)\n            images.append(arr)\n            labels.append(cls_idx)\n    else:\n        # iterate subfolders\n        for sub in sorted(cls_path.iterdir()):\n            if sub.is_dir():\n                for p in sorted(sub.iterdir()):\n                    if p.is_file() and p.suffix.lower() in VALID_EXTS:\n                        arr = safe_load_image(p, size=IMG_SIZE)\n                        images.append(arr)\n                        labels.append(cls_idx)\n\nimages = np.stack(images, axis=0).astype(np.uint8)  # shape [N,H,W,3]\nlabels = np.array(labels, dtype=np.int64)\nprint(\"Loaded total samples:\", len(images))\nprint(\"Image array shape:\", images.shape)\nprint(\"Label distribution:\", np.bincount(labels))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T06:59:58.84832Z","iopub.execute_input":"2025-12-09T06:59:58.848512Z","iopub.status.idle":"2025-12-09T07:00:14.621739Z","shell.execute_reply.started":"2025-12-09T06:59:58.848498Z","shell.execute_reply":"2025-12-09T07:00:14.621042Z"}},"outputs":[{"name":"stdout","text":"Detected class folders (top-level): ['benign', 'malignant']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading images in benign:   0%|          | 0/382 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c578d7d911a4cefb97a4f37c66fe784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading images in malignant:   0%|          | 0/802 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bcdb56ed4704cfb90f994a44991da35"}},"metadata":{}},{"name":"stdout","text":"Loaded total samples: 1184\nImage array shape: (1184, 224, 224, 3)\nLabel distribution: [382 802]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Cell 5\nnp.savez_compressed(Path(OUT_DIR)/\"breakhis_224.npz\", images=images, labels=labels)\nprint(\"Saved NPZ to\", Path(OUT_DIR)/\"breakhis_224.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:00:14.622527Z","iopub.execute_input":"2025-12-09T07:00:14.622803Z","iopub.status.idle":"2025-12-09T07:00:22.490894Z","shell.execute_reply.started":"2025-12-09T07:00:14.622779Z","shell.execute_reply":"2025-12-09T07:00:22.490236Z"}},"outputs":[{"name":"stdout","text":"Saved NPZ to /kaggle/working/outputs/breakhis_224.npz\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Cell 6\ntest_frac = 0.2\nval_frac = 0.1  # fraction of remaining for validation\n\nX_trainval, X_test, y_trainval, y_test = train_test_split(images, labels, test_size=test_frac, random_state=RANDOM_SEED, stratify=labels)\nif val_frac > 0:\n    val_fraction_of_trainval = val_frac / (1 - test_frac)\n    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=val_fraction_of_trainval, random_state=RANDOM_SEED, stratify=y_trainval)\nelse:\n    X_train, y_train = X_trainval, y_trainval\n    X_val, y_val = np.array([]), np.array([])\n\nprint(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:00:22.491727Z","iopub.execute_input":"2025-12-09T07:00:22.492276Z","iopub.status.idle":"2025-12-09T07:00:22.623763Z","shell.execute_reply.started":"2025-12-09T07:00:22.492246Z","shell.execute_reply":"2025-12-09T07:00:22.623152Z"}},"outputs":[{"name":"stdout","text":"Train: 828 Val: 119 Test: 237\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Cell 7\nclass NumpyImageSequenceDataset(Dataset):\n    \"\"\"\n    For single-image classification but returns shape [T, C, H, W] with T=1.\n    \"\"\"\n    def __init__(self, images_np, labels_np, augment=False):\n        self.images = images_np\n        self.labels = labels_np\n        self.augment = augment\n        self.to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n        ])\n        self.aug = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10)\n        ])\n    def __len__(self):\n        return len(self.images)\n    def __getitem__(self, idx):\n        arr = self.images[idx]\n        # ensure uint8 H,W,3\n        arr = np.array(arr)\n        if arr.dtype != np.uint8:\n            arr = arr.astype(np.uint8)\n        if arr.ndim != 3 or arr.shape[2] != 3:\n            arr = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n        pil = Image.fromarray(arr)\n        if self.augment:\n            pil = self.aug(pil)\n        x = self.to_tensor(pil)  # C,H,W\n        # treat as sequence length 1\n        x = x.unsqueeze(0)  # 1,C,H,W\n        y = int(self.labels[idx])\n        return x, y\n\ndef collate_fn(batch):\n    xs, ys = zip(*batch)\n    # xs are [1,C,H,W] each; stack to [B,1,C,H,W]\n    xs = torch.cat(xs, dim=0) if isinstance(xs[0], torch.Tensor) and xs[0].shape[0] != 1 else torch.stack(xs, dim=0)\n    # ensure shape [B, T, C, H, W]\n    if xs.ndim == 4:\n        xs = xs.unsqueeze(1)  # unlikely, but safe\n    ys = torch.tensor(ys, dtype=torch.long)\n    return xs, ys\n\ntrain_ds = NumpyImageSequenceDataset(X_train, y_train, augment=True)\nval_ds = NumpyImageSequenceDataset(X_val, y_val, augment=False) if len(X_val)>0 else None\ntest_ds = NumpyImageSequenceDataset(X_test, y_test, augment=False)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=lambda b: (torch.cat([x for x,y in b], dim=0).unsqueeze(1).squeeze(1) if False else collate_fn(b)))\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn) if val_ds is not None else None\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)\n\nprint(\"Dataloaders: train batches\", len(train_loader), \"test batches\", len(test_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:00:22.624537Z","iopub.execute_input":"2025-12-09T07:00:22.62487Z","iopub.status.idle":"2025-12-09T07:00:22.636872Z","shell.execute_reply.started":"2025-12-09T07:00:22.624852Z","shell.execute_reply":"2025-12-09T07:00:22.636205Z"}},"outputs":[{"name":"stdout","text":"Dataloaders: train batches 52 test batches 15\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Cell 8\nclass BackboneFeatureMap(nn.Module):\n    \"\"\"\n    Returns the last feature-map (B, C, H, W).\n    Uses timm.create_model(..., features_only=True) when available.\n    Fallback to torchvision ResNet50 up to layer4 to get feature map.\n    \"\"\"\n    def __init__(self, swin_name='swin_tiny_patch4_window7_224', pretrained=True):\n        super().__init__()\n        self.use_swin = False\n        if TIMM_AVAILABLE:\n            try:\n                # features_only returns list of feature maps; pick last\n                self.net = timm.create_model(swin_name, pretrained=pretrained, features_only=True, out_indices=[-1])\n                self.use_swin = True\n                # to inspect output channels we will run a dummy later\n                print(\"Using timm swin features-only:\", swin_name)\n            except Exception as e:\n                print(\"timm.features_only failed:\", e)\n                self.use_swin = False\n        if not self.use_swin:\n            # build ResNet50 up to layer4\n            import torchvision.models as models\n            resnet = models.resnet50(pretrained=pretrained)\n            # take layers up to layer4 (exclude avgpool and fc)\n            self.net = nn.Sequential(\n                resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n                resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n            )\n            print(\"Using torchvision ResNet50 feature extractor as fallback\")\n\n    def forward(self, x):\n        \"\"\"\n        x: [B, C, H, W]\n        returns: feature map tensor [B, C_feat, Hf, Wf]\n        \"\"\"\n        out = self.net(x)\n        # timm returns list when features_only; if so take first element\n        if isinstance(out, list):\n            out = out[0]\n        return out\n\nclass SwinGRUSpatial(nn.Module):\n    \"\"\"\n    Takes backbone feature map [B,C,H,W], reshapes to sequence [B, T=H*W, C],\n    passes through GRU across spatial tokens and classifies from final hidden state.\n    \"\"\"\n    def __init__(self, backbone, feat_channels, gru_hidden=512, gru_layers=1, num_classes=2, bidirectional=False, dropout=0.3):\n        super().__init__()\n        self.backbone = backbone\n        self.feat_channels = feat_channels\n        self.gru_hidden = gru_hidden\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n        self.gru = nn.GRU(input_size=feat_channels, hidden_size=gru_hidden, num_layers=gru_layers, batch_first=True, bidirectional=bidirectional)\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Sequential(\n            nn.Linear(gru_hidden * self.num_directions, gru_hidden//2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(gru_hidden//2, num_classes)\n        )\n\n    def forward(self, x_seq):\n        \"\"\"\n        x_seq: [B, T, C, H, W]  where T=1 for single images\n        We'll process each image separately through backbone then run GRU over spatial tokens.\n        \"\"\"\n        B, T, C, H, W = x_seq.shape\n        # flatten batch and time to process images in one forward\n        x_flat = x_seq.view(B*T, C, H, W)\n        feat_map = self.backbone(x_flat)  # [B*T, C_feat, Hf, Wf]\n        bf, cf, hf, wf = feat_map.shape\n        # reshape to sequence of spatial tokens\n        feat_tokens = feat_map.view(bf, cf, hf*wf).permute(0, 2, 1)  # [B*T, T_spatial, C_feat]\n        # pass through GRU\n        out, h_n = self.gru(feat_tokens)  # h_n: [num_layers * num_directions, batch, hidden]\n        # take last layer's hidden states\n        if self.num_directions == 2:\n            last_fwd = h_n[-2]\n            last_bwd = h_n[-1]\n            h_final = torch.cat([last_fwd, last_bwd], dim=1)  # [B*T, hidden*2]\n        else:\n            h_final = h_n[-1]  # [B*T, hidden]\n        h_final = self.dropout(h_final)\n        logits = self.classifier(h_final)  # [B*T, num_classes]\n        logits = logits.view(B, T, -1)  # [B, T, num_classes]\n        # if T==1, squeeze time dim\n        logits = logits.mean(dim=1)  # average over T (here T=1)\n        return logits\n\n# instantiate backbone\nbackbone = BackboneFeatureMap(pretrained=True).to(DEVICE)\n\n# run dummy tensor to inspect feature channels dynamically\n_dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\nwith torch.no_grad():\n    feat = backbone(_dummy)  # forward pass\n    if isinstance(feat, list):  # timm.features_only returns list\n        feat = feat[0]\n    feat_channels = feat.shape[1]  # C_feat, exact GRU input size\n\nprint(\"Detected backbone feature channels:\", feat_channels)\n\n# create model with correct feat_channels\nmodel = SwinGRUSpatial(\n    backbone=backbone,\n    feat_channels=feat_channels,  # use detected channels\n    gru_hidden=512,\n    gru_layers=1,\n    num_classes=len(classes),\n    bidirectional=False,\n    dropout=0.3\n).to(DEVICE)\n\nprint(\"Model ready on\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:00:22.637564Z","iopub.execute_input":"2025-12-09T07:00:22.637812Z","iopub.status.idle":"2025-12-09T07:00:23.975996Z","shell.execute_reply.started":"2025-12-09T07:00:22.637788Z","shell.execute_reply":"2025-12-09T07:00:23.975212Z"}},"outputs":[{"name":"stdout","text":"Using timm swin features-only: swin_tiny_patch4_window7_224\nDetected backbone feature channels: 7\nModel ready on cuda\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Cell 9\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:00:23.976775Z","iopub.execute_input":"2025-12-09T07:00:23.977004Z","iopub.status.idle":"2025-12-09T07:00:23.981912Z","shell.execute_reply.started":"2025-12-09T07:00:23.976987Z","shell.execute_reply":"2025-12-09T07:00:23.981121Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Cell 10\nfrom tqdm.notebook import tqdm\n\ndef train_one_epoch(model, loader, optimizer, device):\n    model.train()\n    losses = []\n    preds = []\n    targets = []\n    loop = tqdm(loader, leave=False)\n    for batch in loop:\n        try:\n            x, y = batch\n        except Exception as e:\n            print(\"Batch collate error:\", e)\n            continue\n        x = x.to(device)   # [B, T, C, H, W]\n        y = y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)  # [B, num_classes]\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        p = logits.argmax(dim=1).detach().cpu().numpy()\n        preds.extend(p.tolist())\n        targets.extend(y.detach().cpu().numpy().tolist())\n        loop.set_postfix(loss=np.mean(losses))\n    if len(targets)==0:\n        return float('nan'), 0, 0, 0, 0\n    acc = accuracy_score(targets, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(targets, preds, average='weighted', zero_division=0)\n    return np.mean(losses), acc, prec, rec, f1\n\n@torch.no_grad()\ndef validate(model, loader, device):\n    model.eval()\n    losses = []\n    preds = []\n    targets = []\n    loop = tqdm(loader, leave=False)\n    for batch in loop:\n        try:\n            x, y = batch\n        except Exception as e:\n            print(\"Batch collate error (val):\", e)\n            continue\n        x = x.to(device)\n        y = y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n        losses.append(loss.item())\n        p = logits.argmax(dim=1).cpu().numpy()\n        preds.extend(p.tolist())\n        targets.extend(y.cpu().numpy().tolist())\n    if len(targets)==0:\n        return float('nan'), 0, 0, 0, 0, np.zeros((len(classes), len(classes)), dtype=int)\n    acc = accuracy_score(targets, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(targets, preds, average='weighted', zero_division=0)\n    cm = confusion_matrix(targets, preds)\n    return np.mean(losses), acc, prec, rec, f1, cm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:00:23.98268Z","iopub.execute_input":"2025-12-09T07:00:23.982911Z","iopub.status.idle":"2025-12-09T07:00:23.997337Z","shell.execute_reply.started":"2025-12-09T07:00:23.98289Z","shell.execute_reply":"2025-12-09T07:00:23.996539Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Cell 11\nbest_val_f1 = -1.0\nbest_path = Path(OUT_DIR)/\"best_model.pth\"\n\nfor epoch in range(1, EPOCHS+1):\n    print(f\"Epoch {epoch}/{EPOCHS}\")\n    train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(model, train_loader, optimizer, DEVICE)\n    print(f\"Train loss {train_loss:.4f} acc {train_acc:.4f} f1 {train_f1:.4f}\")\n    if val_loader is not None:\n        val_loss, val_acc, val_prec, val_rec, val_f1, val_cm = validate(model, val_loader, DEVICE)\n        print(f\"Val   loss {val_loss:.4f} acc {val_acc:.4f} f1 {val_f1:.4f}\")\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), best_path)\n            print(\"Saved best model to\", best_path, \"val_f1=\", best_val_f1)\n    else:\n        # save last epoch\n        torch.save(model.state_dict(), Path(OUT_DIR)/f\"model_epoch_{epoch}.pth\")\n    scheduler.step()\n\nprint(\"Training finished.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:00:23.99808Z","iopub.execute_input":"2025-12-09T07:00:23.998375Z","iopub.status.idle":"2025-12-09T07:03:36.379151Z","shell.execute_reply.started":"2025-12-09T07:00:23.99836Z","shell.execute_reply":"2025-12-09T07:03:36.37822Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.5398 acc 0.7343 f1 0.7235\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.4336 acc 0.7899 f1 0.7574\nSaved best model to /kaggle/working/outputs/best_model.pth val_f1= 0.757390972707288\nEpoch 2/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.3657 acc 0.8804 f1 0.8784\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.2557 acc 0.8992 f1 0.9004\nSaved best model to /kaggle/working/outputs/best_model.pth val_f1= 0.9004042123178385\nEpoch 3/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.3455 acc 0.8720 f1 0.8702\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.2402 acc 0.9244 f1 0.9235\nSaved best model to /kaggle/working/outputs/best_model.pth val_f1= 0.9235032493694862\nEpoch 4/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.2922 acc 0.8937 f1 0.8930\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.1384 acc 0.9580 f1 0.9581\nSaved best model to /kaggle/working/outputs/best_model.pth val_f1= 0.9581255427072014\nEpoch 5/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.2217 acc 0.9179 f1 0.9176\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.3308 acc 0.9160 f1 0.9181\nEpoch 6/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.3222 acc 0.8889 f1 0.8877\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.1444 acc 0.9580 f1 0.9575\nEpoch 7/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.2121 acc 0.9263 f1 0.9261\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.2516 acc 0.9076 f1 0.9100\nEpoch 8/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train loss 0.1173 acc 0.9601 f1 0.9602\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Val   loss 0.1480 acc 0.9496 f1 0.9502\nTraining finished.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Cell 12\nif best_path.exists():\n    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n    print(\"Loaded best model from\", best_path)\n\ntest_loss, test_acc, test_prec, test_rec, test_f1, test_cm = validate(model, test_loader, DEVICE)\nprint(\"Test loss {:.4f} acc {:.4f} f1 {:.4f}\".format(test_loss, test_acc, test_f1))\nprint(\"Confusion matrix:\\n\", test_cm)\n\n# detailed classification report\ny_true = []\ny_pred = []\nmodel.eval()\nwith torch.no_grad():\n    for x, y in test_loader:\n        x = x.to(DEVICE)\n        logits = model(x)\n        preds = logits.argmax(dim=1).cpu().numpy()\n        y_pred.extend(preds.tolist())\n        y_true.extend(y.numpy().tolist())\n\nprint(classification_report(y_true, y_pred, target_names=classes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:03:36.380841Z","iopub.execute_input":"2025-12-09T07:03:36.381447Z","iopub.status.idle":"2025-12-09T07:03:41.547828Z","shell.execute_reply.started":"2025-12-09T07:03:36.381421Z","shell.execute_reply":"2025-12-09T07:03:41.546811Z"}},"outputs":[{"name":"stdout","text":"Loaded best model from /kaggle/working/outputs/best_model.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Test loss 0.2472 acc 0.8945 f1 0.8931\nConfusion matrix:\n [[ 60  16]\n [  9 152]]\n              precision    recall  f1-score   support\n\n      benign       0.87      0.79      0.83        76\n   malignant       0.90      0.94      0.92       161\n\n    accuracy                           0.89       237\n   macro avg       0.89      0.87      0.88       237\nweighted avg       0.89      0.89      0.89       237\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Cell 13\nidx = 0\nsample = X_test[idx]\ntrue_label = int(y_test[idx])\npil = Image.fromarray(sample)\nx = transforms.ToTensor()(pil)\nx = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(x)\nx = x.unsqueeze(0).unsqueeze(0).to(DEVICE)  # [1, T=1, C, H, W]\nmodel.eval()\nwith torch.no_grad():\n    logits = model(x)\n    pred = int(logits.argmax(dim=1).cpu().item())\nprint(\"True:\", classes[true_label], \"Pred:\", classes[pred])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T07:03:41.550811Z","iopub.execute_input":"2025-12-09T07:03:41.551063Z","iopub.status.idle":"2025-12-09T07:03:41.719679Z","shell.execute_reply.started":"2025-12-09T07:03:41.551039Z","shell.execute_reply":"2025-12-09T07:03:41.718886Z"}},"outputs":[{"name":"stdout","text":"True: benign Pred: malignant\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom sklearn.metrics import confusion_matrix\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Output directory for saved models and figures\nOUT_DIR = Path(\"./output\")\nOUT_DIR.mkdir(exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# assume model is already created: model = SwinGRUSpatial(...)\nmodel = model.to(DEVICE)\n\n# optimizer\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\n# learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n# number of epochs\nEPOCHS = 12\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nval_losses = []\ntrain_accs = []\nval_accs = []\ntrain_f1s = []\nval_f1s = []\nbest_val_f1 = -1.0\nbest_path = OUT_DIR/\"best_model.pth\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(1, EPOCHS+1):\n    print(f\"Epoch {epoch}/{EPOCHS}\")\n    \n    # training step\n    train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(model, train_loader, optimizer, DEVICE)\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    train_f1s.append(train_f1)\n    \n    # validation step\n    val_loss, val_acc, val_prec, val_rec, val_f1, val_cm = validate(model, val_loader, DEVICE)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    val_f1s.append(val_f1)\n    \n    print(f\"Train loss {train_loss:.4f} acc {train_acc:.4f} f1 {train_f1:.4f}\")\n    print(f\"Val   loss {val_loss:.4f} acc {val_acc:.4f} f1 {val_f1:.4f}\")\n    \n    # save best model\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), best_path)\n        print(\"Saved best model to\", best_path, \"val_f1=\", best_val_f1)\n    \n    scheduler.step()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(range(1, EPOCHS+1), train_losses, label='Train Loss', marker='o')\nplt.plot(range(1, EPOCHS+1), val_losses, label='Val Loss', marker='o')\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(OUT_DIR/\"loss_curve.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(range(1, EPOCHS+1), train_accs, label='Train Accuracy', marker='o')\nplt.plot(range(1, EPOCHS+1), val_accs, label='Val Accuracy', marker='o')\nplt.title(\"Training & Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.savefig(OUT_DIR/\"accuracy_curve.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(range(1, EPOCHS+1), train_f1s, label='Train F1-Score', marker='o')\nplt.plot(range(1, EPOCHS+1), val_f1s, label='Val F1-Score', marker='o')\nplt.title(\"Training & Validation F1-Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"F1-Score\")\nplt.legend()\nplt.grid(True)\nplt.savefig(OUT_DIR/\"f1_curve.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load best model\nmodel.load_state_dict(torch.load(best_path, map_location=DEVICE))\nmodel.eval()\n\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for x, y in test_loader:\n        x = x.to(DEVICE)\n        logits = model(x)\n        preds = logits.argmax(dim=1).cpu().numpy()\n        y_pred.extend(preds.tolist())\n        y_true.extend(y.numpy().tolist())\n\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.savefig(OUT_DIR/\"confusion_matrix.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}