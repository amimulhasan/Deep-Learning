{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:55:35.223831Z","iopub.execute_input":"2025-12-10T16:55:35.224194Z","iopub.status.idle":"2025-12-10T16:56:09.784460Z","shell.execute_reply.started":"2025-12-10T16:55:35.224142Z","shell.execute_reply":"2025-12-10T16:56:09.783600Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Kaggle kernels usually have torch, torchvision preinstalled.\n# But install timm and grad-cam and albumentations if not present.\n# Run this cell once (it may take a minute)\n!pip install -q timm==0.9.2\n!pip install -q pytorch-grad-cam==1.5.2\n!pip install -q albumentations==1.3.0\n!pip install -q opencv-python-headless\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:56:09.785902Z","iopub.execute_input":"2025-12-10T16:56:09.786346Z","iopub.status.idle":"2025-12-10T16:58:36.595045Z","shell.execute_reply.started":"2025-12-10T16:56:09.786320Z","shell.execute_reply":"2025-12-10T16:58:36.594030Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam==1.5.2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam==1.5.2\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install pytorch-grad-cam\n!pip install monai-weekly==1.4.0\n!pip install timm\n!pip install pytorch-grad-cam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T17:01:52.530336Z","iopub.execute_input":"2025-12-10T17:01:52.530652Z","iopub.status.idle":"2025-12-10T17:02:00.432650Z","shell.execute_reply.started":"2025-12-10T17:01:52.530619Z","shell.execute_reply":"2025-12-10T17:02:00.431594Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement monai-weekly==1.4.0 (from versions: 0.4.dev2047, 0.4.dev2048, 0.4.dev2049, 0.4.dev2050, 0.5.dev2051, 0.5.dev2052, 0.5.dev2101, 0.5.dev2102, 0.5.dev2103, 0.5.dev2104, 0.5.dev2105, 0.5.dev2106, 0.5.dev2107, 0.5.dev2108, 0.5.dev2109, 0.5.dev2110, 0.5.dev2111, 0.5.dev2112, 0.5.dev2113, 0.5.dev2114, 0.5.dev2115, 0.6.dev2116, 0.6.dev2117, 0.6.dev2118, 0.6.dev2119, 0.6.dev2120, 0.6.dev2121, 0.6.dev2122, 0.6.dev2123, 0.6.dev2124, 0.6.dev2125, 0.6.dev2126, 0.6.dev2127, 0.7.dev2128, 0.7.dev2129, 0.7.dev2130, 0.7.dev2131, 0.7.dev2132, 0.7.dev2133, 0.7.dev2134, 0.7.dev2135, 0.7.dev2136, 0.7.dev2137, 0.7.dev2138, 0.8.dev2139, 0.8.dev2140, 0.8.dev2141, 0.8.dev2142, 0.8.dev2143, 0.8.dev2144, 0.8.dev2145, 0.8.dev2146, 0.8.dev2147, 0.9.dev2148, 0.9.dev2149, 0.9.dev2150, 0.9.dev2151, 0.9.dev2152, 0.9.dev2201, 0.9.dev2202, 0.9.dev2203, 0.9.dev2204, 0.9.dev2205, 0.9.dev2206, 0.9.dev2207, 0.9.dev2208, 0.9.dev2209, 0.9.dev2210, 0.9.dev2211, 0.9.dev2212, 0.9.dev2213, 0.9.dev2214, 0.9.dev2215, 0.9.dev2216, 0.9.dev2217, 0.9.dev2218, 0.9.dev2219, 0.9.dev2220, 0.9.dev2221, 0.9.dev2222, 0.9.dev2223, 0.9.dev2224, 0.10.dev2225, 0.10.dev2226, 0.10.dev2227, 0.10.dev2228, 0.10.dev2229, 0.10.dev2230, 0.10.dev2231, 0.10.dev2232, 0.10.dev2233, 0.10.dev2234, 0.10.dev2235, 0.10.dev2236, 0.10.dev2237, 1.1.dev2238, 1.1.dev2239, 1.1.dev2240, 1.1.dev2241, 1.1.dev2242, 1.1.dev2243, 1.1.dev2244, 1.1.dev2245, 1.1.dev2246, 1.1.dev2247, 1.1.dev2248, 1.1.dev2249, 1.1.dev2250, 1.1.dev2251, 1.2.dev2252, 1.2.dev2301, 1.2.dev2302, 1.2.dev2304, 1.2.dev2305, 1.2.dev2306, 1.2.dev2307, 1.2.dev2308, 1.2.dev2309, 1.2.dev2310, 1.2.dev2311, 1.2.dev2312, 1.2.dev2313, 1.2.dev2314, 1.2.dev2315, 1.2.dev2316, 1.2.dev2317, 1.2.dev2318, 1.2.dev2319, 1.2.dev2320, 1.2.dev2321, 1.2.dev2322, 1.2.dev2323, 1.3.dev2324, 1.3.dev2325, 1.3.dev2326, 1.3.dev2327, 1.3.dev2328, 1.3.dev2329, 1.3.dev2330, 1.3.dev2331, 1.3.dev2332, 1.3.dev2333, 1.3.dev2334, 1.3.dev2335, 1.3.dev2336, 1.3.dev2337, 1.3.dev2338, 1.3.dev2339, 1.3.dev2340, 1.3.dev2341, 1.4.dev2342, 1.4.dev2343, 1.4.dev2344, 1.4.dev2345, 1.4.dev2346, 1.4.dev2347, 1.4.dev2348, 1.4.dev2349, 1.4.dev2350, 1.4.dev2351, 1.4.dev2352, 1.4.dev2353, 1.4.dev2401, 1.4.dev2402, 1.4.dev2403, 1.4.dev2404, 1.4.dev2405, 1.4.dev2406, 1.4.dev2407, 1.4.dev2408, 1.4.dev2409, 1.4.dev2410, 1.4.dev2411, 1.4.dev2412, 1.4.dev2413, 1.4.dev2414, 1.4.dev2415, 1.4.dev2416, 1.4.dev2417, 1.4.dev2418, 1.4.dev2419, 1.4.dev2420, 1.4.dev2421, 1.4.dev2422, 1.4.dev2423, 1.4.dev2424, 1.4.dev2425, 1.4.dev2426, 1.4.dev2427, 1.4.dev2428, 1.4.dev2429, 1.4.dev2430, 1.4.dev2431, 1.4.dev2434, 1.4.dev2435, 1.4.dev2436, 1.4.dev2437, 1.4.dev2438, 1.4.dev2439, 1.4.dev2440, 1.4.dev2441, 1.5.dev2442, 1.5.dev2443, 1.5.dev2444, 1.5.dev2445, 1.5.dev2446, 1.5.dev2447, 1.5.dev2448, 1.5.dev2449, 1.5.dev2450, 1.5.dev2451, 1.5.dev2452, 1.5.dev2501, 1.5.dev2502, 1.5.dev2503, 1.5.dev2504, 1.5.dev2505, 1.5.dev2506, 1.5.dev2507, 1.5.dev2508, 1.5.dev2509, 1.5.dev2510, 1.5.dev2511, 1.5.dev2512, 1.5.dev2513, 1.5.dev2514, 1.5.dev2515, 1.5.dev2516, 1.5.dev2517, 1.5.dev2518, 1.5.dev2519, 1.5.dev2520, 1.5.dev2521, 1.5.dev2522, 1.5.dev2523, 1.6.dev2524, 1.6.dev2525, 1.6.dev2526, 1.6.dev2527, 1.6.dev2528, 1.6.dev2529, 1.6.dev2530, 1.6.dev2531, 1.6.dev2532, 1.6.dev2533, 1.6.dev2534, 1.6.dev2535, 1.6.dev2536, 1.6.dev2537, 1.6.dev2538, 1.6.dev2539, 1.6.dev2540, 1.6.dev2541, 1.6.dev2542, 1.6.dev2543, 1.6.dev2544, 1.6.dev2545, 1.6.dev2546, 1.6.dev2547, 1.6.dev2548, 1.6.dev2549)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for monai-weekly==1.4.0\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (0.9.2)\nRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.3)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7->timm) (1.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (1.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.2.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7->timm) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (2025.10.5)\n\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install monai==1.3.0\n!pip install nibabel\n!pip install pytorch-grad-cam\n!pip install timm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T17:03:54.916278Z","iopub.execute_input":"2025-12-10T17:03:54.916903Z","iopub.status.idle":"2025-12-10T17:04:07.044968Z","shell.execute_reply.started":"2025-12-10T17:03:54.916878Z","shell.execute_reply":"2025-12-10T17:04:07.044190Z"}},"outputs":[{"name":"stdout","text":"Collecting monai==1.3.0\n  Downloading monai-1.3.0-202310121228-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from monai==1.3.0) (2.2.6)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai==1.3.0) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai==1.3.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai==1.3.0) (3.0.3)\nDownloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: monai\nSuccessfully installed monai-1.3.0\nRequirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\nRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.2.6)\nRequirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (25.0)\nRequirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.15.0)\n\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (0.9.2)\nRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.3)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7->timm) (1.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm) (1.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.2.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7->timm) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->timm) (2025.10.5)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport shutil\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom monai.networks.nets import SwinUNETR\nfrom monai.transforms import (\n    LoadImage,\n    EnsureChannelFirst,\n    Resize,\n    ScaleIntensity,\n    ToTensor\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T17:04:11.298874Z","iopub.execute_input":"2025-12-10T17:04:11.299715Z","iopub.status.idle":"2025-12-10T17:04:39.883588Z","shell.execute_reply.started":"2025-12-10T17:04:11.299679Z","shell.execute_reply":"2025-12-10T17:04:39.882775Z"}},"outputs":[{"name":"stderr","text":"2025-12-10 17:04:24.244248: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765386264.401550      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765386264.450804      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/usr/local/lib/python3.11/dist-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"import os, sys, random, math, time, shutil\nfrom pathlib import Path\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nimport torchvision.transforms as T\n\nimport timm\nfrom einops import rearrange\n\n# Grad-CAM\nfrom pytorch_grad_cam import GradCAM, GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.model_selection import train_test_split\nfrom skimage import morphology\nfrom skimage.transform import resize as sk_resize\n\n# seed\ndef seed_everything(seed=42):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\nseed_everything(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T17:02:21.133737Z","iopub.execute_input":"2025-12-10T17:02:21.134027Z","iopub.status.idle":"2025-12-10T17:02:21.153328Z","shell.execute_reply.started":"2025-12-10T17:02:21.134006Z","shell.execute_reply":"2025-12-10T17:02:21.152361Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/646395656.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Grad-CAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_grad_cam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradCAMPlusPlus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_grad_cam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_targets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierOutputTarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_grad_cam'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pytorch_grad_cam'","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# Kaggle dataset path\nDATA_DIR = \"/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets\"\nassert os.path.exists(DATA_DIR), f\"Dataset folder not found at {DATA_DIR}. Attach dataset to notebook.\"\n\n# list folders and counts\nfolders = [d for d in sorted(os.listdir(DATA_DIR)) if os.path.isdir(os.path.join(DATA_DIR,d))]\nprint(\"Top-level folders:\", folders)\nfor f in folders:\n    p = os.path.join(DATA_DIR,f)\n    # count only images\n    imgs = len([x for x in os.listdir(p) if x.lower().endswith(('.jpg','.png','.jpeg'))])\n    print(f, imgs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.003749Z","iopub.status.idle":"2025-12-10T16:58:46.003969Z","shell.execute_reply.started":"2025-12-10T16:58:46.003864Z","shell.execute_reply":"2025-12-10T16:58:46.003874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WORKDIR = \"/kaggle/working/lc_lung_threeclass\"\nos.makedirs(WORKDIR, exist_ok=True)\n\n# detect lung folders\nall_dirs = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR,d))]\nlung_dirs = [d for d in all_dirs if 'lung' in d.lower()]\nprint(\"Detected lung folders:\", lung_dirs)\nif not lung_dirs:\n    raise SystemExit(\"No lung folders detected. Check dataset structure.\")\n\n# copy to working dir (symlinks/copy)\nfor d in lung_dirs:\n    src = os.path.join(DATA_DIR, d)\n    dst = os.path.join(WORKDIR, d)\n    if not os.path.exists(dst):\n        shutil.copytree(src, dst)\nprint(\"Copied lung folders to:\", WORKDIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.004707Z","iopub.status.idle":"2025-12-10T16:58:46.004942Z","shell.execute_reply.started":"2025-12-10T16:58:46.004835Z","shell.execute_reply":"2025-12-10T16:58:46.004846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows=[]\nfor cls in sorted(os.listdir(WORKDIR)):\n    cls_path = os.path.join(WORKDIR, cls)\n    if not os.path.isdir(cls_path): continue\n    for f in os.listdir(cls_path):\n        if f.lower().endswith(('.png','.jpg','.jpeg')):\n            rows.append({\"image\": os.path.join(cls_path,f), \"class\": cls})\ndf = pd.DataFrame(rows)\nprint(\"Total images (lung classes):\", len(df))\ndf['label'] = df['class'].astype('category').cat.codes\nlabel_map = dict(enumerate(df['class'].astype('category').cat.categories))\nprint(\"Label map:\", label_map)\ntrain_df, val_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\nprint(\"Train/Val:\", len(train_df), len(val_df))\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.005870Z","iopub.status.idle":"2025-12-10T16:58:46.006079Z","shell.execute_reply.started":"2025-12-10T16:58:46.005979Z","shell.execute_reply":"2025-12-10T16:58:46.005989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HistClasDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = np.array(Image.open(row.image).convert(\"RGB\"))\n        if self.transforms:\n            augmented = self.transforms(image=img)\n            img = augmented['image']\n        label = int(row.label)\n        return img, label, row.image\n\ntrain_tf = A.Compose([\n    A.Resize(224,224),\n    A.RandomRotate90(),\n    A.HorizontalFlip(),\n    A.VerticalFlip(),\n    A.Normalize(),\n    ToTensorV2()\n])\nval_tf = A.Compose([A.Resize(224,224), A.Normalize(), ToTensorV2()])\n\ntrain_ds = HistClasDataset(train_df, transforms=train_tf)\nval_ds = HistClasDataset(val_df, transforms=val_tf)\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\nprint(\"Loaders ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.007489Z","iopub.status.idle":"2025-12-10T16:58:46.007827Z","shell.execute_reply.started":"2025-12-10T16:58:46.007652Z","shell.execute_reply":"2025-12-10T16:58:46.007676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = models.resnet18(pretrained=True)\nclf.fc = nn.Linear(clf.fc.in_features, len(label_map))\nclf = clf.to(device)\n\noptimizer = torch.optim.Adam(clf.parameters(), lr=2e-4)\ncriterion = nn.CrossEntropyLoss()\n\ndef train_one_epoch(model, loader):\n    model.train()\n    tot, acc = 0, 0\n    for imgs, labels, _ in loader:\n        imgs = imgs.to(device).float()\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        out = model(imgs)\n        loss = criterion(out, labels)\n        loss.backward()\n        optimizer.step()\n        _, preds = out.max(1)\n        tot += labels.size(0)\n        acc += (preds==labels).sum().item()\n    return acc/tot\n\ndef validate(model, loader):\n    model.eval()\n    tot, acc = 0, 0\n    with torch.no_grad():\n        for imgs, labels, _ in loader:\n            imgs = imgs.to(device).float()\n            labels = labels.to(device)\n            out = model(imgs)\n            _, preds = out.max(1)\n            tot += labels.size(0)\n            acc += (preds==labels).sum().item()\n    return acc/tot\n\n# Train for a few epochs for decent CAMs\nEPOCHS_C = 3\nfor epoch in range(EPOCHS_C):\n    tr_acc = train_one_epoch(clf, train_loader)\n    va_acc = validate(clf, val_loader)\n    print(f\"[Classifier] Epoch {epoch+1}/{EPOCHS_C} TrainAcc {tr_acc:.4f} ValAcc {va_acc:.4f}\")\n\n# save classifier weights\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\ntorch.save(clf.state_dict(), \"/kaggle/working/models/resnet_clf.pth\")\nprint(\"Classifier saved at /kaggle/working/models/resnet_clf.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.009028Z","iopub.status.idle":"2025-12-10T16:58:46.009344Z","shell.execute_reply.started":"2025-12-10T16:58:46.009171Z","shell.execute_reply":"2025-12-10T16:58:46.009189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use GradCAM++ targeting last layer\ntarget_layer = clf.layer4[-1]\ncam = GradCAMPlusPlus(model=clf, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n\nPSEUDODIR = \"/kaggle/working/pseudo_masks\"\nos.makedirs(PSEUDODIR, exist_ok=True)\n\ndef image_to_float_np(img_path, target_size=(224,224)):\n    im = np.array(Image.open(img_path).convert(\"RGB\").resize(target_size))\n    im_float = im.astype(np.float32)/255.0\n    return im_float\n\nall_images = df['image'].tolist()\nprint(\"Creating pseudo masks for\", len(all_images), \"images. This may take several minutes.\")\nfor idx, img_path in enumerate(all_images):\n    img_float = image_to_float_np(img_path, target_size=(224,224))\n    # preprocess as train transforms (normalize)\n    # Albumentations Normalize uses mean=0.0 std=1.0 by default? we used A.Normalize() which uses imagenet mean/std\n    # so replicate that for model forward\n    img_norm = (img_float - np.array([0.485,0.456,0.406])) / np.array([0.229,0.224,0.225])\n    inp = torch.tensor(np.transpose(img_norm, (2,0,1))[None]).float().to(device)\n    with torch.no_grad():\n        logits = clf(inp)\n        pred = int(logits.argmax(dim=1).cpu().item())\n    targets = [ClassifierOutputTarget(pred)]\n    gray_cam = cam(input_tensor=inp, targets=targets)[0]  # HxW normalized 0-1 (224x224)\n    # resize cam to original image size\n    orig = np.array(Image.open(img_path).convert(\"RGB\"))\n    cam_resized = sk_resize(gray_cam, (orig.shape[0], orig.shape[1]), preserve_range=True)\n    # threshold -> binary mask (heuristic)\n    thresh = max(0.25, 0.35 * cam_resized.max())  # safety\n    bin_mask = (cam_resized > thresh).astype(np.uint8)\n    # morphological cleaning\n    bin_mask = morphology.remove_small_objects(bin_mask.astype(bool), min_size=300).astype(np.uint8)\n    bin_mask = morphology.binary_closing(bin_mask, morphology.disk(5)).astype(np.uint8)\n    mask_path = os.path.join(PSEUDODIR, os.path.basename(img_path))\n    Image.fromarray((bin_mask*255).astype(np.uint8)).save(mask_path)\n    if (idx+1) % 500 == 0:\n        print(\"Processed:\", idx+1)\nprint(\"Pseudo masks saved to:\", PSEUDODIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.010590Z","iopub.status.idle":"2025-12-10T16:58:46.010833Z","shell.execute_reply.started":"2025-12-10T16:58:46.010702Z","shell.execute_reply":"2025-12-10T16:58:46.010712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seg_rows=[]\nfor row in df.itertuples():\n    img = row.image\n    mask = os.path.join(PSEUDODIR, os.path.basename(img))\n    if os.path.exists(mask):\n        seg_rows.append({\"image\": img, \"mask\": mask, \"label\": row.label})\nseg_df = pd.DataFrame(seg_rows)\nprint(\"Seg pairs found:\", len(seg_df))\nif len(seg_df)==0:\n    raise SystemExit(\"No pseudo masks found. Check PSEUDODIR.\")\n\nseg_train, seg_val = train_test_split(seg_df, test_size=0.15, stratify=seg_df['label'], random_state=42)\nseg_train = seg_train.reset_index(drop=True)\nseg_val = seg_val.reset_index(drop=True)\nprint(\"Seg Train/Val:\", len(seg_train), len(seg_val))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.012300Z","iopub.status.idle":"2025-12-10T16:58:46.012545Z","shell.execute_reply.started":"2025-12-10T16:58:46.012432Z","shell.execute_reply":"2025-12-10T16:58:46.012454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegDataset(Dataset):\n    def __init__(self, df, transforms=None, img_size=384):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n        self.img_size = img_size\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = np.array(Image.open(row.image).convert(\"RGB\"))\n        mask = np.array(Image.open(row.mask).convert(\"L\"))\n        mask = (mask > 127).astype(np.uint8)\n        if self.transforms:\n            augmented = self.transforms(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        # ensure mask shape [H,W] then add channel\n        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n        return img.float(), mask.float()\n\ntrain_tf_seg = A.Compose([\n    A.Resize(384,384),\n    A.RandomRotate90(),\n    A.HorizontalFlip(),\n    A.VerticalFlip(),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\nval_tf_seg = A.Compose([\n    A.Resize(384,384),\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n    ToTensorV2()\n])\n\ntrain_ds_seg = SegDataset(seg_train, transforms=train_tf_seg, img_size=384)\nval_ds_seg = SegDataset(seg_val, transforms=val_tf_seg, img_size=384)\ntrain_loader_seg = DataLoader(train_ds_seg, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\nval_loader_seg = DataLoader(val_ds_seg, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\nprint(\"Seg dataloaders ready. Train batches:\", len(train_loader_seg), \"Val batches:\", len(val_loader_seg))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.013475Z","iopub.status.idle":"2025-12-10T16:58:46.013743Z","shell.execute_reply.started":"2025-12-10T16:58:46.013621Z","shell.execute_reply":"2025-12-10T16:58:46.013634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We'll use timm swin model features_only to get multi-scale features\nclass PatchEmbedSwin(nn.Module):\n    def __init__(self, model_name='swin_base_patch4_window7_224', pretrained=True):\n        super().__init__()\n        self.swin = timm.create_model(model_name, pretrained=pretrained, features_only=True, out_indices=(0,1,2,3))\n    def forward(self, x):\n        return self.swin(x)  # list of feature maps [f1,f2,f3,f4]\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_ch, skip_ch, out_ch):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch + skip_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x, skip):\n        x = self.up(x)\n        if x.shape[-2:] != skip.shape[-2:]:\n            x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n        x = torch.cat([x, skip], dim=1)\n        return self.conv(x)\n\nclass SwinUNetR(nn.Module):\n    def __init__(self, n_classes=1, swin_name='swin_base_patch4_window7_224', pretrained=True):\n        super().__init__()\n        self.encoder = PatchEmbedSwin(model_name=swin_name, pretrained=pretrained)\n        # dummy forward to find channels (use small tensor)\n        with torch.no_grad():\n            dummy = torch.randn(1,3,384,384)\n            feats = self.encoder(dummy)\n            chs = [f.shape[1] for f in feats]  # [c1,c2,c3,c4]\n        # decoder blocks\n        self.decoder4 = DecoderBlock(in_ch=chs[-1], skip_ch=chs[-2], out_ch=512)\n        self.decoder3 = DecoderBlock(in_ch=512, skip_ch=chs[-3], out_ch=256)\n        self.decoder2 = DecoderBlock(in_ch=256, skip_ch=chs[-4], out_ch=128)\n        self.decoder1 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.head = nn.Conv2d(64, n_classes, kernel_size=1)\n    def forward(self, x):\n        feats = self.encoder(x)  # [f1,f2,f3,f4]\n        f1,f2,f3,f4 = feats\n        x = self.decoder4(f4, f3)\n        x = self.decoder3(x, f2)\n        x = self.decoder2(x, f1)\n        x = self.decoder1(x)\n        out = self.head(x)  # raw logits (no sigmoid)\n        return out\n\n# instantiate\nmodel = SwinUNetR(n_classes=1, swin_name='swin_base_patch4_window7_224', pretrained=True)\nmodel = model.to(device)\nprint(\"Model ready. Param count:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.014668Z","iopub.status.idle":"2025-12-10T16:58:46.014918Z","shell.execute_reply.started":"2025-12-10T16:58:46.014816Z","shell.execute_reply":"2025-12-10T16:58:46.014827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BCEWithLogits + Dice (use logits directly)\nbce_logits = nn.BCEWithLogitsLoss()\n\ndef dice_loss_from_logits(logits, target, smooth=1e-6):\n    pred = torch.sigmoid(logits)\n    pred = pred.contiguous()\n    target = target.contiguous()\n    intersection = (pred * target).sum(dim=(2,3))\n    denom = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n    loss = 1 - ((2. * intersection + smooth) / (denom + smooth))\n    return loss.mean()\n\ndef compute_iou_from_logits(logits, target, thr=0.5):\n    pred = torch.sigmoid(logits)\n    pred_bin = (pred > thr).float()\n    inter = (pred_bin * target).sum(dim=(2,3))\n    union = (pred_bin + target).clamp(0,1).sum(dim=(2,3))\n    iou = (inter + 1e-6) / (union + 1e-6)\n    return iou.mean().item()\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.015814Z","iopub.status.idle":"2025-12-10T16:58:46.016022Z","shell.execute_reply.started":"2025-12-10T16:58:46.015917Z","shell.execute_reply":"2025-12-10T16:58:46.015926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 12   # adjust as compute budget allows\nbest_val_iou = 0.0\nos.makedirs(\"/kaggle/working/seg_models\", exist_ok=True)\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss_sum = 0.0\n    train_iou_sum = 0.0\n    nb = 0\n    for imgs, masks in train_loader_seg:\n        imgs = imgs.to(device).float()\n        masks = masks.to(device).float()\n        preds = model(imgs)  # logits [B,1,H,W]\n        # ensure same size\n        if preds.shape != masks.shape:\n            preds = F.interpolate(preds, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n        loss_bce = bce_logits(preds, masks)\n        loss_dice = dice_loss_from_logits(preds, masks)\n        loss = loss_bce + loss_dice\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss_sum += loss.item()\n        train_iou_sum += compute_iou_from_logits(preds.detach().cpu(), masks.detach().cpu())\n        nb += 1\n    train_loss = train_loss_sum / max(nb,1)\n    train_iou = train_iou_sum / max(nb,1)\n\n    # validation\n    model.eval()\n    val_loss_sum = 0.0\n    val_iou_sum = 0.0\n    nb = 0\n    with torch.no_grad():\n        for imgs, masks in val_loader_seg:\n            imgs = imgs.to(device).float()\n            masks = masks.to(device).float()\n            preds = model(imgs)\n            if preds.shape != masks.shape:\n                preds = F.interpolate(preds, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n            loss_bce = bce_logits(preds, masks)\n            loss_dice = dice_loss_from_logits(preds, masks)\n            loss = loss_bce + loss_dice\n            val_loss_sum += loss.item()\n            val_iou_sum += compute_iou_from_logits(preds.cpu(), masks.cpu())\n            nb += 1\n    val_loss = val_loss_sum / max(nb,1)\n    val_iou = val_iou_sum / max(nb,1)\n    scheduler.step(val_loss)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} TrainLoss:{train_loss:.4f} TrainIoU:{train_iou:.4f} ValLoss:{val_loss:.4f} ValIoU:{val_iou:.4f}\")\n\n    # save best\n    if val_iou > best_val_iou:\n        best_val_iou = val_iou\n        torch.save(model.state_dict(), f\"/kaggle/working/seg_models/swinunetr_best.pth\")\n        print(\"Saved best model at epoch\", epoch+1, \"ValIoU:\", best_val_iou)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.016868Z","iopub.status.idle":"2025-12-10T16:58:46.017093Z","shell.execute_reply.started":"2025-12-10T16:58:46.016986Z","shell.execute_reply":"2025-12-10T16:58:46.016997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show some validation images with predicted overlay and pseudo GT\nmodel.eval()\nwith torch.no_grad():\n    imgs, masks = next(iter(val_loader_seg))\n    imgs = imgs.to(device).float()\n    preds = model(imgs)\n    preds = torch.sigmoid(preds).cpu().numpy()\n    imgs = imgs.cpu().numpy()\n    masks = masks.cpu().numpy()\n    n_show = min(6, imgs.shape[0])\n    plt.figure(figsize=(12,4*n_show//3 + 2))\n    for i in range(n_show):\n        img = np.transpose(imgs[i], (1,2,0))\n        # unnormalize (we used imagenet norm)\n        mean = np.array([0.485,0.456,0.406]); std = np.array([0.229,0.224,0.225])\n        img = (img * std) + mean\n        img = np.clip(img, 0, 1)\n        pred_mask = preds[i,0]\n        gt_mask = masks[i,0]\n        plt.subplot(n_show,3,3*i+1); plt.imshow(img); plt.axis('off'); plt.title('Image')\n        plt.subplot(n_show,3,3*i+2); plt.imshow(img); plt.imshow(pred_mask, alpha=0.45); plt.axis('off'); plt.title('Pred overlay')\n        plt.subplot(n_show,3,3*i+3); plt.imshow(gt_mask, cmap='gray'); plt.axis('off'); plt.title('Pseudo GT')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.018070Z","iopub.status.idle":"2025-12-10T16:58:46.018372Z","shell.execute_reply.started":"2025-12-10T16:58:46.018241Z","shell.execute_reply":"2025-12-10T16:58:46.018256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save final model (already saved best)\nMODEL_PATH = \"/kaggle/working/seg_models/swinunetr_best.pth\"\nprint(\"Best model path:\", MODEL_PATH)\n# Inference helper: load and predict on a single image\ndef load_model(path=MODEL_PATH):\n    m = SwinUNetR(n_classes=1, swin_name='swin_base_patch4_window7_224', pretrained=False)\n    m.load_state_dict(torch.load(path, map_location='cpu'))\n    m.to(device).eval()\n    return m\n\ndef predict_image(model, img_path, out_size=(384,384), thr=0.5):\n    img = np.array(Image.open(img_path).convert(\"RGB\"))\n    img_res = sk_resize(img, out_size, preserve_range=True).astype(np.uint8)\n    img_norm = (img_res.astype(np.float32)/255.0 - np.array([0.485,0.456,0.406])) / np.array([0.229,0.224,0.225])\n    inp = torch.tensor(np.transpose(img_norm, (2,0,1))[None]).float().to(device)\n    with torch.no_grad():\n        logits = model(inp)\n        prob = torch.sigmoid(logits)[0,0].cpu().numpy()\n    mask = (prob > thr).astype(np.uint8)\n    return img_res, prob, mask\n\n# Example usage:\n# model_loaded = load_model()\n# img_res, prob_map, mask_pred = predict_image(model_loaded, seg_val.iloc[0].image)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T16:58:46.019567Z","iopub.status.idle":"2025-12-10T16:58:46.019805Z","shell.execute_reply.started":"2025-12-10T16:58:46.019689Z","shell.execute_reply":"2025-12-10T16:58:46.019702Z"}},"outputs":[],"execution_count":null}]}