{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:21.594342Z","iopub.execute_input":"2025-12-10T05:12:21.594703Z","iopub.status.idle":"2025-12-10T05:12:22.169659Z","shell.execute_reply.started":"2025-12-10T05:12:21.594673Z","shell.execute_reply":"2025-12-10T05:12:22.169041Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# run only if timm missing\n!pip install timm --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:22.171069Z","iopub.execute_input":"2025-12-10T05:12:22.171334Z","iopub.status.idle":"2025-12-10T05:12:25.442749Z","shell.execute_reply.started":"2025-12-10T05:12:22.171310Z","shell.execute_reply":"2025-12-10T05:12:25.441838Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Cell 1\nimport os\nfrom pathlib import Path\nimport random\nimport math\nimport numpy as np\nfrom PIL import Image\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport timm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc, precision_recall_curve\nfrom sklearn.manifold import TSNE\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# CONFIG - edit these\nDATA_DIR = \"/kaggle/input/breakhis-400x\"   # <-- change if necessary\nOUT_DIR = Path(\"/kaggle/working/output\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nIMG_SIZE = 224\nBATCH_SIZE = 8           # reduce if OOM\nEPOCHS = 12\nLR = 1e-4\nRANDOM_SEED = 42\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Multi-scale + adaptive selection\nSCALES = [1.0, 0.5]      # scales relative to IMG_SIZE; reduce if OOM\nTOP_K = 16               # select top-K tokens per image\nGRU_HIDDEN = 512\nGRU_LAYERS = 1\nBIDIRECTIONAL = False\n\n# Mix/Cut params\nMIXPROB = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Progressive freezing schedule (epochs)\nFREEZE_STAGE_1 = 2   # freeze backbone initially\nFREEZE_STAGE_2 = 5   # partially unfreeze later\n\nSEED = RANDOM_SEED\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nprint(\"Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.443768Z","iopub.execute_input":"2025-12-10T05:12:25.443985Z","iopub.status.idle":"2025-12-10T05:12:25.455752Z","shell.execute_reply.started":"2025-12-10T05:12:25.443963Z","shell.execute_reply":"2025-12-10T05:12:25.455132Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Cell 2\nVALID_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'}\n\ndef safe_load_image(path, size=IMG_SIZE):\n    try:\n        img = Image.open(path).convert(\"RGB\")\n        img = img.resize((size, size), Image.BILINEAR)\n        arr = np.array(img)\n        if arr.dtype != np.uint8:\n            arr = arr.astype(np.uint8)\n        if arr.ndim != 3 or arr.shape[2] != 3:\n            arr = np.zeros((size, size, 3), dtype=np.uint8)\n        return arr\n    except Exception:\n        return np.zeros((size, size, 3), dtype=np.uint8)\n\ndata_root = Path(DATA_DIR)\nassert data_root.exists(), f\"{DATA_DIR} not found\"\n\nclasses = [d.name for d in sorted(data_root.iterdir()) if d.is_dir()]\nprint(\"Detected classes:\", classes)\n\nfilepaths = []\nlabels = []\nfor i, cls in enumerate(classes):\n    folder = data_root / cls\n    for p in sorted(folder.rglob(\"*\")):\n        if p.is_file() and p.suffix.lower() in VALID_EXTS:\n            filepaths.append(str(p))\n            labels.append(i)\nprint(\"Total samples:\", len(filepaths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.456531Z","iopub.execute_input":"2025-12-10T05:12:25.456900Z","iopub.status.idle":"2025-12-10T05:12:25.571253Z","shell.execute_reply.started":"2025-12-10T05:12:25.456876Z","shell.execute_reply":"2025-12-10T05:12:25.570590Z"}},"outputs":[{"name":"stdout","text":"Detected classes: ['test', 'train', 'valid']\nTotal samples: 1693\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Cell 3\ntrainval_paths, test_paths, trainval_labels, test_labels = train_test_split(\n    filepaths, labels, test_size=0.2, random_state=SEED, stratify=labels\n)\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    trainval_paths, trainval_labels, test_size=0.125, random_state=SEED, stratify=trainval_labels\n)  # ~10% val overall\n\nprint(\"train:\", len(train_paths), \"val:\", len(val_paths), \"test:\", len(test_paths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.573101Z","iopub.execute_input":"2025-12-10T05:12:25.573326Z","iopub.status.idle":"2025-12-10T05:12:25.584183Z","shell.execute_reply.started":"2025-12-10T05:12:25.573308Z","shell.execute_reply":"2025-12-10T05:12:25.583461Z"}},"outputs":[{"name":"stdout","text":"train: 1184 val: 170 test: 339\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Cell 4\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\neval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\nclass ImageFileDataset(Dataset):\n    def __init__(self, paths, labels, transform=None, img_size=IMG_SIZE):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.img_size = img_size\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        arr = safe_load_image(p, size=self.img_size)\n        img = Image.fromarray(arr)\n        if self.transform:\n            img = self.transform(img)\n        label = int(self.labels[idx])\n        return img, label\n\ntrain_ds = ImageFileDataset(train_paths, train_labels, transform=train_transform)\nval_ds = ImageFileDataset(val_paths, val_labels, transform=eval_transform)\ntest_ds = ImageFileDataset(test_paths, test_labels, transform=eval_transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.584862Z","iopub.execute_input":"2025-12-10T05:12:25.585084Z","iopub.status.idle":"2025-12-10T05:12:25.596885Z","shell.execute_reply.started":"2025-12-10T05:12:25.585061Z","shell.execute_reply":"2025-12-10T05:12:25.596183Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Cell 5\ndef rand_bbox(W, H, lam):\n    cut_rat = math.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef mixup_data(x, y, alpha=MIXUP_ALPHA):\n    if alpha <= 0:\n        return x, y, None, 1.0\n    lam = np.random.beta(alpha, alpha)\n    index = torch.randperm(x.size(0)).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef cutmix_data(x, y, alpha=CUTMIX_ALPHA):\n    if alpha <= 0:\n        return x, y, None, 1.0\n    lam = np.random.beta(alpha, alpha)\n    index = torch.randperm(x.size(0)).to(x.device)\n    B, C, H, W = x.size()\n    bbx1, bby1, bbx2, bby2 = rand_bbox(W, H, lam)\n    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n    y_a, y_b = y, y[index]\n    return x, y_a, y_b, lam\n\ndef apply_mix_augment(images, labels):\n    r = random.random()\n    if r < MIXPROB/2:\n        return mixup_data(images, labels, alpha=MIXUP_ALPHA)\n    elif r < MIXPROB:\n        return cutmix_data(images, labels, alpha=CUTMIX_ALPHA)\n    else:\n        return images, labels, None, 1.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.597606Z","iopub.execute_input":"2025-12-10T05:12:25.597833Z","iopub.status.idle":"2025-12-10T05:12:25.616208Z","shell.execute_reply.started":"2025-12-10T05:12:25.597779Z","shell.execute_reply":"2025-12-10T05:12:25.615457Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Cell 6\nclass BalancedFocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, reduction='mean'):\n        super().__init__()\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits, targets, class_weights=None):\n        # logits: [B, C]; targets: [B]\n        logp = F.log_softmax(logits, dim=1)\n        p = torch.exp(logp)\n        # cross entropy per-sample\n        if class_weights is not None:\n            ce = F.nll_loss(logp, targets, reduction='none', weight=class_weights)\n        else:\n            ce = F.nll_loss(logp, targets, reduction='none')\n        pt = p.gather(1, targets.unsqueeze(1)).squeeze(1)\n        focal = (1 - pt) ** self.gamma\n        loss = focal * ce\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.616978Z","iopub.execute_input":"2025-12-10T05:12:25.617198Z","iopub.status.idle":"2025-12-10T05:12:25.635388Z","shell.execute_reply.started":"2025-12-10T05:12:25.617183Z","shell.execute_reply":"2025-12-10T05:12:25.634703Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Cell 7 (FIXED)\nclass MultiScaleHybridModel(nn.Module):\n    def __init__(self, swin_name='swin_tiny_patch4_window7_224', pretrained=True,\n                 scales=SCALES, top_k=TOP_K, gru_hidden=GRU_HIDDEN, gru_layers=GRU_LAYERS,\n                 bidirectional=BIDIRECTIONAL, num_classes=None, device=DEVICE):\n        super().__init__()\n        self.scales = scales\n        self.top_k = top_k\n        self.device = device\n        self.gru_hidden = gru_hidden\n        self.num_directions = 2 if bidirectional else 1\n\n        # Backbone (feature map)\n        self.backbone_feats = timm.create_model(\n            swin_name, pretrained=pretrained, features_only=True, out_indices=[-1]\n        ).to(device)\n\n        # Backbone (global pooled)\n        self.backbone_pool = timm.create_model(\n            swin_name, pretrained=pretrained, num_classes=0, global_pool=\"avg\"\n        ).to(device)\n\n        # Detect feature map channels\n        dummy = torch.zeros(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n        with torch.no_grad():\n            feat_out = self.backbone_feats(dummy)\n            feat_out = feat_out[0] if isinstance(feat_out, list) else feat_out\n            self.feat_c = feat_out.shape[1]   # feature map channels\n\n            pooled_out = self.backbone_pool(dummy)\n            if pooled_out.dim() == 4:\n                pooled_out = pooled_out.mean([2, 3])\n            self.pooled_dim = pooled_out.shape[1]   # pooled feature dim\n\n        # GRU\n        self.gru = nn.GRU(\n            input_size=self.feat_c,\n            hidden_size=gru_hidden,\n            num_layers=gru_layers,\n            batch_first=True,\n            bidirectional=bidirectional\n        ).to(device)\n\n        # adapter for pooled â†’ GRU dimension\n        self.pooled_adapter = nn.Linear(self.pooled_dim, gru_hidden*self.num_directions).to(device)\n\n        # NOW fusion_in is correct\n        fusion_in = gru_hidden*self.num_directions + gru_hidden*self.num_directions\n\n        self.fusion_gate = nn.Sequential(\n            nn.Linear(fusion_in, fusion_in // 2),\n            nn.GELU(),\n            nn.Linear(fusion_in // 2, gru_hidden * self.num_directions),\n            nn.Sigmoid()\n        ).to(device)\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(gru_hidden*self.num_directions, gru_hidden),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(gru_hidden, num_classes)\n        ).to(device)\n\n    def _get_feature_map(self, imgs, scale):\n        if scale != 1.0:\n            size = int(IMG_SIZE * scale)\n            imgs = F.interpolate(imgs, size=(size, size), mode='bilinear', align_corners=False)\n        out = self.backbone_feats(imgs)\n        return out[0] if isinstance(out, list) else out\n\n    def forward(self, x):\n        B = x.size(0)\n\n        if x.size(2) != IMG_SIZE or x.size(3) != IMG_SIZE:\n            x = F.interpolate(x, size=(IMG_SIZE, IMG_SIZE), mode='bilinear')\n\n        pooled = self.backbone_pool(x)\n        if pooled.dim() == 4:\n            pooled = pooled.mean([2, 3])\n\n        pooled_proj = self.pooled_adapter(pooled)\n\n        # Multi-scale patches\n        tokens_multi = []\n        for scale in self.scales:\n            fmap = self._get_feature_map(x, scale)\n            Bf, Cf, Hf, Wf = fmap.shape\n            tokens = fmap.view(Bf, Cf, -1).permute(0, 2, 1)\n            tokens_multi.append(tokens)\n\n        tokens_all = torch.cat(tokens_multi, dim=1)\n\n        # Adaptive slice-selection\n        var = tokens_all.var(dim=2)\n        k = min(self.top_k, tokens_all.size(1))\n        _, idx = torch.topk(var, k, dim=1)\n        batch = torch.arange(B).unsqueeze(1).to(x.device)\n        selected = tokens_all[batch, idx]\n\n        gru_out, _ = self.gru(selected)\n        last = gru_out[:, -1, :]\n\n        # Hybrid Attention Fusion\n        fusion_input = torch.cat([pooled_proj, last], dim=1)\n        gate = self.fusion_gate(fusion_input)\n        fused = gate * last + (1 - gate) * pooled_proj\n\n        return self.classifier(fused)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.636963Z","iopub.execute_input":"2025-12-10T05:12:25.637208Z","iopub.status.idle":"2025-12-10T05:12:25.656722Z","shell.execute_reply.started":"2025-12-10T05:12:25.637190Z","shell.execute_reply":"2025-12-10T05:12:25.655974Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Cell 8\nnum_classes = len(classes)\nmodel = MultiScaleHybridModel(\n    swin_name='swin_tiny_patch4_window7_224',\n    pretrained=True,\n    scales=SCALES,\n    top_k=TOP_K,\n    gru_hidden=GRU_HIDDEN,\n    gru_layers=GRU_LAYERS,\n    bidirectional=BIDIRECTIONAL,\n    num_classes=num_classes,\n    device=DEVICE\n)\n\n# class weights (inverse freq)\ncnt = Counter(train_labels)\nfreq = np.array([cnt[i] for i in range(num_classes)], dtype=float)\ninv = (freq.sum() / (freq + 1e-8))\nclass_weights = torch.tensor(inv / inv.sum(), dtype=torch.float32).to(DEVICE)\n\ncriterion = BalancedFocalLoss(gamma=2.0).to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\nprint(\"Model created. Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:25.657617Z","iopub.execute_input":"2025-12-10T05:12:25.657843Z","iopub.status.idle":"2025-12-10T05:12:27.100624Z","shell.execute_reply.started":"2025-12-10T05:12:25.657820Z","shell.execute_reply":"2025-12-10T05:12:27.099995Z"}},"outputs":[{"name":"stdout","text":"Model created. Device: cuda\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Cell 9\ndef compute_loss(logits, y_info, criterion, class_weights=None):\n    # y_info: (y_a, y_b, lam) or (labels, None, 1.0)\n    y_a, y_b, lam = y_info\n    if y_b is None:\n        return criterion(logits, y_a, class_weights)\n    else:\n        la = criterion(logits, y_a, class_weights)\n        lb = criterion(logits, y_b, class_weights)\n        return lam * la + (1 - lam) * lb\n\ndef set_requires_grad(module, flag):\n    if module is None:\n        return\n    for p in module.parameters():\n        p.requires_grad = flag\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.101336Z","iopub.execute_input":"2025-12-10T05:12:27.101643Z","iopub.status.idle":"2025-12-10T05:12:27.106152Z","shell.execute_reply.started":"2025-12-10T05:12:27.101625Z","shell.execute_reply":"2025-12-10T05:12:27.105575Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Cell 10\nbest_val_f1 = -1.0\nbest_path = OUT_DIR/\"best_hybrid.pth\"\n\nhistory = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'val_f1':[]}\n\n# Stage 1 freeze backbone\nset_requires_grad(model.backbone_feats, False)\nset_requires_grad(model.backbone_pool, False)\n# pooled_adapter may be Identity currently\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    # progressive unfreeze\n    if epoch == FREEZE_STAGE_1 + 1:\n        set_requires_grad(model.backbone_pool, True)\n    if epoch == FREEZE_STAGE_2 + 1:\n        set_requires_grad(model.backbone_feats, True)\n        set_requires_grad(model.backbone_pool, True)\n        if not isinstance(model.pooled_adapter, nn.Identity):\n            set_requires_grad(model.pooled_adapter, True)\n\n    running_loss = 0.0\n    preds_all = []\n    targets_all = []\n\n    for imgs, labels in tqdm(train_loader, leave=False):\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        imgs_aug, y_a, y_b, lam = apply_mix_augment(imgs, labels)\n        # imgs_aug already device-matched\n        # ensure correct size\n        if imgs_aug.size(2) != IMG_SIZE or imgs_aug.size(3) != IMG_SIZE:\n            imgs_aug = F.interpolate(imgs_aug, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n\n        optimizer.zero_grad()\n        logits = model(imgs_aug)\n        y_info = (y_a, y_b, lam) if y_b is not None else (y_a, None, 1.0)\n        loss = compute_loss(logits, y_info, criterion, class_weights)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        preds_all.extend(torch.argmax(logits, dim=1).detach().cpu().numpy().tolist())\n        targets_all.extend(labels.detach().cpu().numpy().tolist())\n\n    train_loss = running_loss / len(train_loader)\n    train_acc = accuracy_score(targets_all, preds_all)\n\n    # validation\n    model.eval()\n    v_loss = 0.0\n    v_preds = []\n    v_targets = []\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs = imgs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            if imgs.size(2) != IMG_SIZE or imgs.size(3) != IMG_SIZE:\n                imgs = F.interpolate(imgs, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n            logits = model(imgs)\n            loss = criterion(logits, labels, class_weights)\n            v_loss += loss.item()\n            v_preds.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n            v_targets.extend(labels.cpu().numpy().tolist())\n\n    val_loss = v_loss / len(val_loader)\n    val_acc = accuracy_score(v_targets, v_preds)\n    val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(v_targets, v_preds, average='weighted', zero_division=0)\n\n    history['train_loss'].append(train_loss)\n    history['train_acc'] = history.get('train_acc', []) + [train_acc]\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['val_f1'].append(val_f1)\n\n    print(f\"Epoch {epoch}/{EPOCHS} TrainLoss {train_loss:.4f} TrainAcc {train_acc:.4f} ValLoss {val_loss:.4f} ValAcc {val_acc:.4f} ValF1 {val_f1:.4f}\")\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), best_path)\n        print(\"Saved best model:\", best_path)\n\n    scheduler.step()\n\nprint(\"Training finished. Best val f1:\", best_val_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.106874Z","iopub.execute_input":"2025-12-10T05:12:27.107098Z","iopub.status.idle":"2025-12-10T05:12:27.782392Z","shell.execute_reply.started":"2025-12-10T05:12:27.107082Z","shell.execute_reply":"2025-12-10T05:12:27.780993Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/148 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb723742b71143cba6a078ad338fedd9"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3064361622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0my_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_b\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3796827166.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtokens_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mfmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_feature_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mBf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3796827166.py\u001b[0m in \u001b[0;36m_get_feature_map\u001b[0;34m(self, imgs, scale)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/_features.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/_features.py\u001b[0m in \u001b[0;36m_collect\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfirst_or_last_module\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/layers/patch_embed.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict_img_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Input height ({H}) doesn't match model ({self.img_size[0]}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Input width ({W}) doesn't match model ({self.img_size[1]}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_img_pad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         )\n\u001b[0;32m-> 2132\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Input height (112) doesn't match model (224)."],"ename":"AssertionError","evalue":"Input height (112) doesn't match model (224).","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"# Cell 11\nmodel.load_state_dict(torch.load(best_path, map_location=DEVICE))\nmodel.eval()\n\ny_true = []\ny_pred = []\ny_proba = []\n\nwith torch.no_grad():\n    for imgs, labels in tqdm(test_loader, leave=False):\n        imgs = imgs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        if imgs.size(2) != IMG_SIZE or imgs.size(3) != IMG_SIZE:\n            imgs = F.interpolate(imgs, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n        logits = model(imgs)\n        probs = F.softmax(logits, dim=1)[:,1].cpu().numpy()  # prob class1\n        preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n        y_true.extend(labels.cpu().numpy().tolist())\n        y_pred.extend(preds)\n        y_proba.extend(probs.tolist())\n\nacc = accuracy_score(y_true, y_pred)\nprec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Test Acc:\", acc, \"F1:\", f1)\nprint(\"Confusion Matrix:\\n\", cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.783014Z","iopub.status.idle":"2025-12-10T05:12:27.783228Z","shell.execute_reply.started":"2025-12-10T05:12:27.783123Z","shell.execute_reply":"2025-12-10T05:12:27.783133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12\nepochs = list(range(1, len(history['train_loss'])+1))\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs, history['train_loss'], marker='o', label='train_loss')\nplt.plot(epochs, history['val_loss'], marker='o', label='val_loss')\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"loss_curve.png\", dpi=300); plt.show()\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs, history['train_acc'], marker='o', label='train_acc')\nplt.plot(epochs, history['val_acc'], marker='o', label='val_acc')\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"acc_curve.png\", dpi=300); plt.show()\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs, history['val_f1'], marker='o', label='val_f1')\nplt.xlabel(\"Epoch\"); plt.ylabel(\"F1\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"f1_curve.png\", dpi=300); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.784811Z","iopub.status.idle":"2025-12-10T05:12:27.785101Z","shell.execute_reply.started":"2025-12-10T05:12:27.784945Z","shell.execute_reply":"2025-12-10T05:12:27.784961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\nplt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.title(\"Confusion Matrix\")\nplt.savefig(OUT_DIR/\"confusion_matrix.png\", dpi=300); plt.show()\n\nfpr, tpr, _ = roc_curve(y_true, y_proba)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(6,5)); plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.4f}\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.grid(True)\nplt.savefig(OUT_DIR/\"roc_curve.png\", dpi=300); plt.show()\n\nprec_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba)\nplt.figure(figsize=(6,5)); plt.plot(recall_vals, prec_vals); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.grid(True)\nplt.savefig(OUT_DIR/\"pr_curve.png\", dpi=300); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.786373Z","iopub.status.idle":"2025-12-10T05:12:27.786687Z","shell.execute_reply.started":"2025-12-10T05:12:27.786547Z","shell.execute_reply":"2025-12-10T05:12:27.786558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 14\nembs = []\nlabels_list = []\nwith torch.no_grad():\n    for imgs, labels in tqdm(test_loader, leave=False):\n        imgs = imgs.to(DEVICE)\n        if imgs.size(2) != IMG_SIZE or imgs.size(3) != IMG_SIZE:\n            imgs = F.interpolate(imgs, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n        feat = model.backbone_pool(imgs)\n        if feat.dim() == 4:\n            feat = feat.mean(dim=[2,3])\n        embs.append(feat.cpu().numpy())\n        labels_list.extend(labels)\nembs = np.concatenate(embs, axis=0)\ntsne = TSNE(n_components=2, random_state=SEED)\ntsne_feats = tsne.fit_transform(embs)\n\nplt.figure(figsize=(8,6))\nplt.scatter(tsne_feats[:,0], tsne_feats[:,1], c=labels_list, cmap='coolwarm', s=6)\nplt.colorbar(ticks=range(len(classes))); plt.title(\"t-SNE of global features\")\nplt.savefig(OUT_DIR/\"tsne.png\", dpi=300); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.787991Z","iopub.status.idle":"2025-12-10T05:12:27.788264Z","shell.execute_reply.started":"2025-12-10T05:12:27.788130Z","shell.execute_reply":"2025-12-10T05:12:27.788145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 15\ndef grad_cam_simple(model, img_tensor, target_class=None):\n    model.eval()\n    activations = []\n    gradients = []\n    def forward_hook(module, inp, out):\n        activations.append(out.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    # Hook heuristic: last module of backbone_feats\n    target_module = None\n    for name, m in model.backbone_feats.named_modules():\n        target_module = m\n    if target_module is None:\n        print(\"No suitable module for hook\")\n        return None\n\n    h_f = target_module.register_forward_hook(forward_hook)\n    h_b = target_module.register_backward_hook(backward_hook)\n\n    img = img_tensor.unsqueeze(0).to(DEVICE)\n    img.requires_grad = True\n    logits = model(img)\n    pred = logits.argmax(dim=1).item()\n    if target_class is None:\n        target_class = pred\n    loss = logits[0, target_class]\n    model.zero_grad()\n    loss.backward(retain_graph=True)\n\n    if len(activations) == 0 or len(gradients) == 0:\n        h_f.remove(); h_b.remove()\n        return None\n\n    act = activations[-1][0]  # [C,H,W]\n    grad = gradients[-1][0]   # [C,H,W]\n    weights = grad.mean(dim=(1,2))  # [C]\n    cam = (weights.view(-1,1,1) * act).sum(dim=0).cpu().numpy()\n    cam = np.maximum(cam, 0)\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    h_f.remove(); h_b.remove()\n    return cam, pred\n\n# example use:\nsample_path = test_paths[0]\narr = safe_load_image(sample_path, IMG_SIZE)\npil = transforms.ToTensor()(Image.fromarray(arr))\npil = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(pil)\nres = grad_cam_simple(model, pil, target_class=None)\nif res is not None:\n    cam, pred = res\n    plt.imshow(arr); plt.imshow(cam, cmap='jet', alpha=0.4); plt.title(f\"Pred {classes[pred]}\"); plt.axis('off')\n    plt.savefig(OUT_DIR/\"gradcam_example.png\", dpi=300); plt.show()\nelse:\n    print(\"Grad-CAM failed to produce map\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.789019Z","iopub.status.idle":"2025-12-10T05:12:27.789279Z","shell.execute_reply.started":"2025-12-10T05:12:27.789134Z","shell.execute_reply":"2025-12-10T05:12:27.789146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 16\ntorch.save(model.state_dict(), OUT_DIR/\"final_model.pth\")\n\nimport pandas as pd\ndf_hist = pd.DataFrame({\n    'epoch': list(range(1, len(history['train_loss'])+1)),\n    'train_loss': history['train_loss'],\n    'val_loss': history['val_loss'],\n    'train_acc': history['train_acc'],\n    'val_acc': history['val_acc'],\n    'val_f1': history['val_f1']\n})\ndf_hist.to_csv(OUT_DIR/\"training_history.csv\", index=False)\nprint(\"Saved model and history to\", OUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.790059Z","iopub.status.idle":"2025-12-10T05:12:27.790254Z","shell.execute_reply.started":"2025-12-10T05:12:27.790161Z","shell.execute_reply":"2025-12-10T05:12:27.790170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 17\ntorch.save(model.state_dict(), OUT_DIR/\"final_model.pth\")\nimport pandas as pd\ndf_hist = pd.DataFrame({\n    'epoch': list(range(1, len(history['train_loss'])+1)),\n    'train_loss': history['train_loss'],\n    'val_loss': history['val_loss'],\n    'train_acc': history['train_acc'],\n    'val_acc': history['val_acc'],\n    'val_f1': history['val_f1']\n})\ndf_hist.to_csv(OUT_DIR/\"training_history.csv\", index=False)\nprint(\"Saved model and history to\", OUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T05:12:27.791983Z","iopub.status.idle":"2025-12-10T05:12:27.792315Z","shell.execute_reply.started":"2025-12-10T05:12:27.792148Z","shell.execute_reply":"2025-12-10T05:12:27.792161Z"}},"outputs":[],"execution_count":null}]}